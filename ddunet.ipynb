{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eSYmtK-2zqU0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eSYmtK-2zqU0",
        "outputId": "312dba0e-b891-460d-8ff1-e73ba9210073"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "import os\n",
        "\n",
        "# Define your destination path in Google Drive\n",
        "drive_output_path = '/content/drive/My Drive/NuInsSeg/ddunet'\n",
        "os.makedirs(drive_output_path, exist_ok=True)"
      ],
      "metadata": {
        "id": "zfQL0labKeRI"
      },
      "id": "zfQL0labKeRI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ddf745f2-aefc-45df-a8e7-8e838d56caa8",
      "metadata": {
        "id": "ddf745f2-aefc-45df-a8e7-8e838d56caa8",
        "outputId": "22dcd128-1297-44fb-8350-e052635699d6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'os' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-984955827.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m## output directories\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'result_save_path'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;34m'prediction_image/'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mopts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'result_save_path'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdrive_output_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'result_save_path'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model_save_path'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;34m'output_model/'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model_save_path'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdrive_output_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model_save_path'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"
          ]
        }
      ],
      "source": [
        "opts = {}\n",
        "opts['number_of_channel'] = 3\n",
        "opts['treshold'] = 0.5\n",
        "opts['epoch_num'] = 20\n",
        "opts['quick_run'] = 1\n",
        "opts['batch_size'] = 16\n",
        "opts['random_seed_num'] = 19\n",
        "opts['k_fold'] = 2\n",
        "opts['save_val_results'] = 1\n",
        "opts['init_LR'] = 0.001\n",
        "opts['LR_decay_factor'] = 0.5\n",
        "opts['LR_drop_after_nth_epoch'] = 20\n",
        "opts['crop_size'] = 512\n",
        "## output directories\n",
        "opts['result_save_path'] ='prediction_image/'\n",
        "opts['result_save_path'] = os.path.join(drive_output_path, opts['result_save_path'])\n",
        "opts['model_save_path'] ='output_model/'\n",
        "opts['model_save_path'] = os.path.join(drive_output_path, opts['model_save_path'])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if not os.path.exists(opts['model_save_path']):\n",
        "    os.makedirs(opts['model_save_path'])\n",
        "if not os.path.exists(opts['result_save_path']):\n",
        "    os.makedirs(opts['result_save_path'])\n",
        "if not os.path.exists(os.path.join(opts['result_save_path'],'validation/unet')):\n",
        "    os.makedirs(os.path.join(opts['result_save_path'],'validation/unet'))\n",
        "if not os.path.exists(os.path.join(opts['result_save_path'],'validation/watershed_unet')):\n",
        "    os.makedirs(os.path.join(opts['result_save_path'],'validation/watershed_unet'))"
      ],
      "metadata": {
        "id": "IXOvq2PHLmqu"
      },
      "id": "IXOvq2PHLmqu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d33d525e-8021-4299-ba53-d204ed312990",
      "metadata": {
        "id": "d33d525e-8021-4299-ba53-d204ed312990"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from glob import glob\n",
        "import numpy as np\n",
        "from sklearn.model_selection import KFold,StratifiedKFold\n",
        "import time\n",
        "import cv2\n",
        "import keras\n",
        "from keras.callbacks import CSVLogger, LearningRateScheduler, ModelCheckpoint\n",
        "from keras.layers import *\n",
        "from keras.models import Model, load_model\n",
        "from keras.optimizers import Adam\n",
        "from albumentations import *\n",
        "from keras import backend as K\n",
        "from skimage.feature import peak_local_max\n",
        "from scipy import ndimage as ndi\n",
        "from skimage.segmentation import watershed\n",
        "import skimage.morphology\n",
        "from skimage.io import imsave\n",
        "from skimage.morphology import remove_small_objects\n",
        "import tqdm\n",
        "from random import shuffle\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"ipateam/nuinsseg\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ],
      "metadata": {
        "id": "AXc3tc-vvIfd",
        "outputId": "d45d8f39-724f-4bd4-c7db-da35dfb007a6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "AXc3tc-vvIfd",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/ipateam/nuinsseg?dataset_version_number=5...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1.52G/1.52G [00:19<00:00, 83.8MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files: /root/.cache/kagglehub/datasets/ipateam/nuinsseg/versions/5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "32336488-74e8-497d-9553-c047f2049cf6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "32336488-74e8-497d-9553-c047f2049cf6",
        "outputId": "e840de90-3442-46f8-e58a-3097fb96441c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['human kidney',\n",
              " 'human rectum',\n",
              " 'human jejunum',\n",
              " 'mouse muscle_tibia',\n",
              " 'human peritoneum',\n",
              " 'mouse thymus',\n",
              " 'human oesophagus',\n",
              " 'human spleen',\n",
              " 'human brain',\n",
              " 'mouse liver',\n",
              " 'human lung',\n",
              " 'human epiglottis',\n",
              " 'human bladder',\n",
              " 'human tongue',\n",
              " 'human pancreas',\n",
              " 'human pylorus',\n",
              " 'human placenta',\n",
              " 'human cerebellum',\n",
              " 'mouse kidney',\n",
              " 'human melanoma',\n",
              " 'human umbilical cord',\n",
              " 'human muscle',\n",
              " 'mouse fat (white and brown)_subscapula',\n",
              " 'mouse heart',\n",
              " 'human salivory gland',\n",
              " 'mouse femur',\n",
              " 'human testis',\n",
              " 'human cardia',\n",
              " 'mouse spleen',\n",
              " 'human tonsile',\n",
              " 'human liver']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "# base_path = 'Dataset/'\n",
        "base_path = path\n",
        "organ_names = [ name for name in os.listdir(base_path) if os.path.isdir(os.path.join(base_path, name)) ]\n",
        "organ_names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ce61d929-e535-4878-b10a-80a58dbb0144",
      "metadata": {
        "id": "ce61d929-e535-4878-b10a-80a58dbb0144"
      },
      "outputs": [],
      "source": [
        "# input and outpu paths\n",
        "img_path = glob(os.path.join(base_path, '*/tissue images/', '*.png'))\n",
        "binary_mask_path = glob(os.path.join(base_path, '*/mask binary/', '*.png'))\n",
        "distance_mask_path = glob(os.path.join(base_path, '*/distance maps/', '*.png'))\n",
        "label_mask_path = glob(os.path.join(base_path, '*/label masks modify/', '*.tif'))\n",
        "vague_mask_path =  glob(os.path.join(base_path, '*/vague areas/mask binary/', '*.png'))\n",
        "\n",
        "\n",
        "img_path.sort()\n",
        "binary_mask_path.sort()\n",
        "distance_mask_path.sort()\n",
        "label_mask_path.sort()\n",
        "vague_mask_path.sort()\n",
        "\n",
        "\n",
        "# create folders to save the best models and images (if needed) for each fold\n",
        "if not os.path.exists('prediction_image/'):\n",
        "    os.makedirs('prediction_image/')\n",
        "if not os.path.exists('output_model/'):\n",
        "    os.makedirs('output_model/')\n",
        "if not os.path.exists(opts['result_save_path']+ 'validation/unet'):\n",
        "    os.makedirs(opts['result_save_path'] + 'validation/unet')\n",
        "if not os.path.exists(opts['result_save_path']+ 'validation/watershed_unet'):\n",
        "    os.makedirs(opts['result_save_path'] + 'validation/watershed_unet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4c816a97-901a-4840-a840-e072fb4bd92a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4c816a97-901a-4840-a840-e072fb4bd92a",
        "outputId": "5d193364-2357-4a7f-b0b5-469bd27dba5e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "image path: /root/.cache/kagglehub/datasets/ipateam/nuinsseg/versions/5/mouse liver/tissue images/mouse_liver_32.png\n",
            " binary mask path: /root/.cache/kagglehub/datasets/ipateam/nuinsseg/versions/5/mouse liver/mask binary/mouse_liver_32.png\n",
            " distance mask path: /root/.cache/kagglehub/datasets/ipateam/nuinsseg/versions/5/mouse liver/distance maps/mouse_liver_32.png\n",
            " label mask path: /root/.cache/kagglehub/datasets/ipateam/nuinsseg/versions/5/mouse liver/label masks modify/mouse_liver_32.tif\n",
            "\n"
          ]
        }
      ],
      "source": [
        "rand_num = np.random.randint(len(img_path))\n",
        "print('image path: {}\\n'.format(img_path[rand_num]),\n",
        "      'binary mask path: {}\\n'.format(binary_mask_path[rand_num]),\n",
        "      'distance mask path: {}\\n'.format(distance_mask_path[rand_num]),\n",
        "      'label mask path: {}\\n'.format(label_mask_path[rand_num]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6c36c333-4f6f-4d0b-82fa-000902824bd1",
      "metadata": {
        "id": "6c36c333-4f6f-4d0b-82fa-000902824bd1"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# ---------------------------\n",
        "# Dice Coefficient\n",
        "# ---------------------------\n",
        "def dice_coef(y_true, y_pred, smooth=1.0):\n",
        "    \"\"\"\n",
        "    Computes the Dice coefficient between y_true and y_pred.\n",
        "    Works with TF 2.x tensors.\n",
        "    \"\"\"\n",
        "    y_true_f = tf.reshape(y_true, [-1])\n",
        "    y_pred_f = tf.reshape(y_pred, [-1])\n",
        "\n",
        "    # Cast both to float32 to avoid type mismatch\n",
        "    y_true_f = tf.cast(y_true_f, tf.float32)\n",
        "    y_pred_f = tf.cast(y_pred_f, tf.float32)\n",
        "\n",
        "    intersection = tf.reduce_sum(y_true_f * y_pred_f)\n",
        "    return (2. * intersection + smooth) / (tf.reduce_sum(y_true_f) + tf.reduce_sum(y_pred_f) + smooth)\n",
        "\n",
        "# ---------------------------\n",
        "# Dice Loss\n",
        "# ---------------------------\n",
        "def dice_loss(y_true, y_pred):\n",
        "    return 1.0 - dice_coef(y_true, y_pred)\n",
        "\n",
        "# ---------------------------\n",
        "# Binary Crossentropy + Dice Loss\n",
        "# ---------------------------\n",
        "def bce_dice_loss(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Combination of binary crossentropy and Dice loss\n",
        "    \"\"\"\n",
        "    # Binary crossentropy (element-wise)\n",
        "    bce = tf.keras.losses.binary_crossentropy(y_true, y_pred)\n",
        "\n",
        "    # Dice coefficient\n",
        "    dice = dice_coef(y_true, y_pred)\n",
        "\n",
        "    # Weighted combination\n",
        "    return 0.5 * bce + (1.0 - dice)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a06f5ed1-e549-4bea-aa62-d21541006608",
      "metadata": {
        "id": "a06f5ed1-e549-4bea-aa62-d21541006608"
      },
      "outputs": [],
      "source": [
        "# learning rate scheduler\n",
        "def step_decay_schedule(initial_lr=1e-3, decay_factor=0.75, epochs_drop=1000):\n",
        "    '''\n",
        "    Wrapper function to create a LearningRateScheduler with step decay schedule.\n",
        "    '''\n",
        "    def schedule(epoch):\n",
        "        return initial_lr * (decay_factor ** np.floor(epoch/epochs_drop))\n",
        "\n",
        "    return LearningRateScheduler(schedule, verbose = 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1df3d693-33ea-475b-b6f3-c25eeb6735d2",
      "metadata": {
        "id": "1df3d693-33ea-475b-b6f3-c25eeb6735d2"
      },
      "outputs": [],
      "source": [
        "# U-net models\n",
        "#############################################################################################################\n",
        "def shallow_unet( IMG_CHANNELS, LearnRate):\n",
        "    inputs = Input((None, None, IMG_CHANNELS))\n",
        "    #s = Lambda(lambda x: x / 255) (inputs)\n",
        "    c1 = Conv2D(16, (3, 3), activation='relu', kernel_initializer='glorot_uniform', padding='same') (inputs)\n",
        "    c1 = Dropout(0.1) (c1)\n",
        "    c1 = Conv2D(16, (3, 3), activation='relu', kernel_initializer='glorot_uniform', padding='same') (c1)\n",
        "    p1 = MaxPooling2D((2, 2)) (c1)\n",
        "\n",
        "    c2 = Conv2D(32, (3, 3), activation='relu', kernel_initializer='glorot_uniform', padding='same') (p1)\n",
        "    c2 = Dropout(0.1) (c2)\n",
        "    c2 = Conv2D(32, (3, 3), activation='relu', kernel_initializer='glorot_uniform', padding='same') (c2)\n",
        "    p2 = MaxPooling2D((2, 2)) (c2)\n",
        "\n",
        "    c3 = Conv2D(64, (3, 3), activation='relu', kernel_initializer='glorot_uniform', padding='same') (p2)\n",
        "    c3 = Dropout(0.1) (c3)\n",
        "    c3 = Conv2D(64, (3, 3), activation='relu', kernel_initializer='glorot_uniform', padding='same') (c3)\n",
        "    p3 = MaxPooling2D((2, 2)) (c3)\n",
        "\n",
        "    c4 = Conv2D(128, (3, 3), activation='relu', kernel_initializer='glorot_uniform', padding='same') (p3)\n",
        "    c4 = Dropout(0.1) (c4)\n",
        "    c4 = Conv2D(128, (3, 3), activation='relu', kernel_initializer='glorot_uniform', padding='same') (c4)\n",
        "    p4 = MaxPooling2D(pool_size=(2, 2)) (c4)\n",
        "\n",
        "    c5 = Conv2D(256, (3, 3), activation='relu', kernel_initializer='glorot_uniform', padding='same') (p4)\n",
        "    c5 = Dropout(0.1) (c5)\n",
        "    c5 = Conv2D(256, (3, 3), activation='relu', kernel_initializer='glorot_uniform', padding='same') (c5)\n",
        "\n",
        "    u6 = Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same') (c5)\n",
        "    u6 = concatenate([u6, c4])\n",
        "    c6 = Conv2D(128, (3, 3), activation='relu', kernel_initializer='glorot_uniform', padding='same') (u6)\n",
        "    c6 = Dropout(0.1) (c6)\n",
        "    c6 = Conv2D(128, (3, 3), activation='relu', kernel_initializer='glorot_uniform', padding='same') (c6)\n",
        "\n",
        "    u7 = Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same') (c6)\n",
        "    u7 = concatenate([u7, c3])\n",
        "    c7 = Conv2D(64, (3, 3), activation='relu', kernel_initializer='glorot_uniform', padding='same') (u7)\n",
        "    c7 = Dropout(0.1) (c7)\n",
        "    c7 = Conv2D(64, (3, 3), activation='relu', kernel_initializer='glorot_uniform', padding='same') (c7)\n",
        "\n",
        "    u8 = Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same') (c7)\n",
        "    u8 = concatenate([u8, c2])\n",
        "    c8 = Conv2D(32, (3, 3), activation='relu', kernel_initializer='glorot_uniform', padding='same') (u8)\n",
        "    c8 = Dropout(0.1) (c8)\n",
        "    c8 = Conv2D(32, (3, 3), activation='relu', kernel_initializer='glorot_uniform', padding='same') (c8)\n",
        "\n",
        "    u9 = Conv2DTranspose(16, (2, 2), strides=(2, 2), padding='same') (c8)\n",
        "    u9 = concatenate([u9, c1], axis=3)\n",
        "    c9 = Conv2D(16, (3, 3), activation='relu', kernel_initializer='glorot_uniform', padding='same') (u9)\n",
        "    c9 = Dropout(0.1) (c9)\n",
        "    c9 = Conv2D(16, (3, 3), activation='relu', kernel_initializer='glorot_uniform', padding='same') (c9)\n",
        "\n",
        "    outputs = Conv2D(1, (1, 1), activation='sigmoid') (c9) # for binary\n",
        "\n",
        "    model = Model(inputs=[inputs], outputs=[outputs])\n",
        "    model.compile(optimizer = Adam(learning_rate= LearnRate), loss= bce_dice_loss , metrics=[dice_coef])\n",
        "    #model.summary()\n",
        "    return model\n",
        "#############################################################################################################\n",
        "def deep_unet(IMG_CHANNELS, LearnRate):\n",
        "    # Build U-Net model\n",
        "    inputs = Input((None, None, IMG_CHANNELS))\n",
        "    #s = Lambda(lambda x: x / 255) (inputs)\n",
        "    c1 = Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (inputs)\n",
        "    c1 = Dropout(0.1) (c1)\n",
        "    c1 = Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (c1)\n",
        "    p1 = MaxPooling2D((2, 2)) (c1)\n",
        "\n",
        "    c2 = Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (p1)\n",
        "    c2 = Dropout(0.1) (c2)\n",
        "    c2 = Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (c2)\n",
        "    p2 = MaxPooling2D((2, 2)) (c2)\n",
        "\n",
        "    c3 = Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (p2)\n",
        "    c3 = Dropout(0.1) (c3)\n",
        "    c3 = Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (c3)\n",
        "    p3 = MaxPooling2D((2, 2)) (c3)\n",
        "\n",
        "    c4 = Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (p3)\n",
        "    c4 = Dropout(0.1) (c4)\n",
        "    c4 = Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (c4)\n",
        "    p4 = MaxPooling2D(pool_size=(2, 2)) (c4)\n",
        "\n",
        "\n",
        "    c4_new = Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (p4)\n",
        "    c4_new = Dropout(0.1) (c4_new)\n",
        "    c4_new = Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (c4_new)\n",
        "    p4_new = MaxPooling2D(pool_size=(2, 2)) (c4_new)\n",
        "\n",
        "    c5 = Conv2D(512, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (p4_new)\n",
        "    c5 = Dropout(0.1) (c5)\n",
        "    c5 = Conv2D(512, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (c5)\n",
        "\n",
        "\n",
        "    u6_new = Conv2DTranspose(256, (2, 2), strides=(2, 2), padding='same') (c5)\n",
        "    u6_new = concatenate([u6_new, c4_new])\n",
        "    c6_new = Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (u6_new)\n",
        "    c6_new = Dropout(0.1) (c6_new)\n",
        "    c6_new = Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (c6_new)\n",
        "\n",
        "    u6 = Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same') (c6_new)\n",
        "    u6 = concatenate([u6, c4])\n",
        "    c6 = Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (u6)\n",
        "    c6 = Dropout(0.1) (c6)\n",
        "    c6 = Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (c6)\n",
        "\n",
        "    u7 = Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same') (c6)\n",
        "    u7 = concatenate([u7, c3])\n",
        "    c7 = Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (u7)\n",
        "    c7 = Dropout(0.1) (c7)\n",
        "    c7 = Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (c7)\n",
        "\n",
        "    u8 = Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same') (c7)\n",
        "    u8 = concatenate([u8, c2])\n",
        "    c8 = Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (u8)\n",
        "    c8 = Dropout(0.1) (c8)\n",
        "    c8 = Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (c8)\n",
        "\n",
        "    u9 = Conv2DTranspose(16, (2, 2), strides=(2, 2), padding='same') (c8)\n",
        "    u9 = concatenate([u9, c1], axis=3)\n",
        "    c9 = Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (u9)\n",
        "    c9 = Dropout(0.1) (c9)\n",
        "    c9 = Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (c9)\n",
        "\n",
        "    outputs = Conv2D(1, (1, 1), activation='sigmoid') (c9)\n",
        "\n",
        "    model_deep = Model(inputs=[inputs], outputs=[outputs])\n",
        "    model_deep.compile(optimizer = Adam(learning_rate=LearnRate), loss= bce_dice_loss , metrics=[ dice_coef])\n",
        "    #model_deeper.summary()\n",
        "    return model_deep\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "33879b9f",
      "metadata": {
        "id": "33879b9f"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import models, layers, regularizers\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "#####################################################################################\n",
        "def dice_coef(y_true, y_pred):\n",
        "    smooth = 1.\n",
        "    y_true_f = K.flatten(y_true)\n",
        "    y_pred_f = K.flatten(y_pred)\n",
        "    intersection = K.sum(y_true_f * y_pred_f)\n",
        "    return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
        "\n",
        "def dice_loss(y_true, y_pred):\n",
        "    return 1 - dice_coef(y_true, y_pred)\n",
        "#####################################################################################\n",
        "def bce_dice_loss(y_true, y_pred):\n",
        "    return 0.5 * tf.keras.losses.binary_crossentropy(y_true, y_pred) - dice_coef(y_true, y_pred)\n",
        "#####################################################################################################################\n",
        "def mse_score(y_true, y_pred):\n",
        "    return K.mean(K.square(y_pred - y_true), axis=-1)\n",
        "#####################################################################################################################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "edb1be16",
      "metadata": {
        "id": "edb1be16"
      },
      "outputs": [],
      "source": [
        "\n",
        "def dual_decoder_unet_binary(IMG_CHANNELS, LearnRate):\n",
        "    inputs = Input((None, None, IMG_CHANNELS))\n",
        "    #encoder\n",
        "    # s = Lambda(lambda x: x / 255)(inputs)\n",
        "    c1 = Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(inputs)\n",
        "    c1 = Dropout(0.1)(c1)\n",
        "    c1 = Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c1)\n",
        "    p1 = MaxPooling2D((2, 2))(c1)\n",
        "\n",
        "    c2 = Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p1)\n",
        "    c2 = Dropout(0.1)(c2)\n",
        "    c2 = Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c2)\n",
        "    p2 = MaxPooling2D((2, 2))(c2)\n",
        "\n",
        "    c3 = Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p2)\n",
        "    c3 = Dropout(0.2)(c3)\n",
        "    c3 = Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c3)\n",
        "    p3 = MaxPooling2D((2, 2))(c3)\n",
        "\n",
        "    c4 = Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p3)\n",
        "    c4 = Dropout(0.2)(c4)\n",
        "    c4 = Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c4)\n",
        "    p4 = MaxPooling2D(pool_size=(2, 2))(c4)\n",
        "\n",
        "\n",
        "    c5 = Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p4)\n",
        "    c5 = Dropout(0.1)(c5)\n",
        "    c5 = Conv2D(512, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c5)\n",
        "\n",
        "    ## decoder for dis unet\n",
        "    u6 = Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(c5)\n",
        "    u6 = concatenate([u6, c4])\n",
        "    c6 = Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u6)\n",
        "    c6 = Dropout(0.2)(c6)\n",
        "    c6 = Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c6)\n",
        "\n",
        "    u7 = Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(c6)\n",
        "    u7 = concatenate([u7, c3])\n",
        "    c7 = Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u7)\n",
        "    c7 = Dropout(0.2)(c7)\n",
        "    c7 = Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c7)\n",
        "\n",
        "    u8 = Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same')(c7)\n",
        "    u8 = concatenate([u8, c2])\n",
        "    c8 = Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u8)\n",
        "    c8 = Dropout(0.1)(c8)\n",
        "    c8 = Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c8)\n",
        "\n",
        "    u9 = Conv2DTranspose(16, (2, 2), strides=(2, 2), padding='same')(c8)\n",
        "    u9 = concatenate([u9, c1], axis=3)\n",
        "    c9 = Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u9)\n",
        "    c9 = Dropout(0.1)(c9)\n",
        "    c9 = Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c9)\n",
        "\n",
        "    outputs1 = Conv2D(1, (1, 1), activation='linear', name='output_dis')(c9)\n",
        "\n",
        "\n",
        "    ## decoder for segmentation unet\n",
        "    u6_seg = Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(c5)\n",
        "    u6_seg = concatenate([u6_seg, c4])\n",
        "    c6_seg = Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u6_seg)\n",
        "    c6_seg = Dropout(0.2)(c6_seg)\n",
        "    c6_seg = Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c6_seg)\n",
        "\n",
        "    u7_seg = Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(c6_seg)\n",
        "    u7_seg = concatenate([u7_seg, c3])\n",
        "    c7_seg = Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u7_seg)\n",
        "    c7_seg = Dropout(0.2)(c7_seg)\n",
        "    c7_seg = Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c7_seg)\n",
        "\n",
        "    u8_seg = Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same')(c7_seg)\n",
        "    u8_seg = concatenate([u8_seg, c2])\n",
        "    c8_seg = Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u8_seg)\n",
        "    c8_seg = Dropout(0.1)(c8_seg)\n",
        "    c8_seg = Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c8_seg)\n",
        "\n",
        "    u9_seg = Conv2DTranspose(16, (2, 2), strides=(2, 2), padding='same')(c8_seg)\n",
        "    u9_seg = concatenate([u9_seg, c1], axis=3)\n",
        "    c9_seg = Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u9_seg)\n",
        "    c9_seg = Dropout(0.1)(c9_seg)\n",
        "    c9_seg = Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c9_seg)\n",
        "\n",
        "    outputs2 = Conv2D(1, (1, 1), activation='sigmoid', name='output_seg')(c9_seg)\n",
        "\n",
        "    model_dual_path = models.Model(inputs=[inputs], outputs=[outputs1, outputs2])\n",
        "    model_dual_path.compile(optimizer = Adam(learning_rate=LearnRate),\n",
        "                            loss={'output_dis': 'mean_squared_error', 'output_seg': bce_dice_loss},\n",
        "                            loss_weights=  {'output_dis': 1.0, 'output_seg': 1.0},\n",
        "                            metrics={'output_seg':dice_coef, 'output_dis':mse_score})\n",
        "    # model_dual_path.summary()\n",
        "    return model_dual_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "87654577-9fc2-4f4c-a5d3-1c82a9241afc",
      "metadata": {
        "id": "87654577-9fc2-4f4c-a5d3-1c82a9241afc"
      },
      "outputs": [],
      "source": [
        "# Random Transformations for Data Augmentation\n",
        "from albumentations import RandomCrop\n",
        "def albumentation_aug(p=1.0, crop_size_row = 448, crop_size_col = 448 ):\n",
        "    return Compose([\n",
        "        # RandomCrop(crop_size_row, crop_size_col, p=1),\n",
        "        OneOf([\n",
        "            RandomCrop(crop_size_row, crop_size_col),\n",
        "            NoOp()\n",
        "        ], p=0.5),\n",
        "        CLAHE(clip_limit=4.0, tile_grid_size=(8, 8),p=0.5),\n",
        "        RandomBrightnessContrast(brightness_limit=0.15, contrast_limit=0.15, brightness_by_max=True, p=0.4),\n",
        "        HueSaturationValue(hue_shift_limit=20, sat_shift_limit=20, val_shift_limit=20, p=0.1),\n",
        "        HorizontalFlip(p=0.5),\n",
        "        VerticalFlip(p=0.5),\n",
        "        RandomRotate90(p=0.5),\n",
        "        ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.1, rotate_limit=20, interpolation=1,\n",
        "                         border_mode=4, p=0.1),\n",
        "\n",
        "    ], p=p, additional_targets={'dist_map': 'mask'})\n",
        "# last  p has the second proiroty comapred to the p inside each argument\n",
        "#(e.g. HorizontalFlip(always_apply=False, p=0.5) )\n",
        "#############################################################################################################\n",
        "# def albumentation_aug_light(p=1.0, crop_size_row = 448, crop_size_col = 448):\n",
        "#     return Compose([\n",
        "#         RandomCrop(crop_size_row, crop_size_col, always_apply=True, p=1.0),\n",
        "#         HorizontalFlip(always_apply=False, p=0.5),\n",
        "#         VerticalFlip(always_apply=False, p=0.5),\n",
        "#         RandomRotate90(always_apply=False, p=0.5),\n",
        "#         ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.1, rotate_limit=20, interpolation=1,\n",
        "#                          border_mode=4 , always_apply=False, p=0.1),\n",
        "#     ], p=p, additional_targets={'mask1': 'mask','mask2': 'mask'})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fd70a79b",
      "metadata": {
        "id": "fd70a79b"
      },
      "outputs": [],
      "source": [
        "def get_fast_aji(true, pred):\n",
        "    \"\"\"AJI version distributed by MoNuSeg, has no permutation problem but suffered from\n",
        "    over-penalisation similar to DICE2.\n",
        "    Fast computation requires instance IDs are in contiguous orderding i.e [1, 2, 3, 4]\n",
        "    not [2, 3, 6, 10]. Please call `remap_label` before hand and `by_size` flag has no\n",
        "    effect on the result.\n",
        "    \"\"\"\n",
        "    true = np.copy(true)  # ? do we need this\n",
        "    pred = np.copy(pred)\n",
        "    true_id_list = list(np.unique(true))\n",
        "    pred_id_list = list(np.unique(pred))\n",
        "    #print(len(pred_id_list))\n",
        "    if len(pred_id_list) == 1:\n",
        "        return 0\n",
        "\n",
        "    true_masks = [None,]\n",
        "    for t in true_id_list[1:]:\n",
        "        t_mask = np.array(true == t, np.uint8)\n",
        "        true_masks.append(t_mask)\n",
        "\n",
        "    pred_masks = [None,]\n",
        "    for p in pred_id_list[1:]:\n",
        "        p_mask = np.array(pred == p, np.uint8)\n",
        "        pred_masks.append(p_mask)\n",
        "\n",
        "    # prefill with value\n",
        "    pairwise_inter = np.zeros(\n",
        "        [len(true_id_list) - 1, len(pred_id_list) - 1], dtype=np.float64\n",
        "    )\n",
        "    pairwise_union = np.zeros(\n",
        "        [len(true_id_list) - 1, len(pred_id_list) - 1], dtype=np.float64\n",
        "    )\n",
        "\n",
        "    # caching pairwise\n",
        "    for true_id in true_id_list[1:]:  # 0-th is background\n",
        "        t_mask = true_masks[true_id]\n",
        "        pred_true_overlap = pred[t_mask > 0]\n",
        "        pred_true_overlap_id = np.unique(pred_true_overlap)\n",
        "        pred_true_overlap_id = list(pred_true_overlap_id)\n",
        "        for pred_id in pred_true_overlap_id:\n",
        "            if pred_id == 0:  # ignore\n",
        "                continue  # overlaping background\n",
        "            p_mask = pred_masks[pred_id]\n",
        "            total = (t_mask + p_mask).sum()\n",
        "            inter = (t_mask * p_mask).sum()\n",
        "            pairwise_inter[true_id - 1, pred_id - 1] = inter\n",
        "            pairwise_union[true_id - 1, pred_id - 1] = total - inter\n",
        "\n",
        "    pairwise_iou = pairwise_inter / (pairwise_union + 1.0e-6)\n",
        "    # pair of pred that give highest iou for each true, dont care\n",
        "    # about reusing pred instance multiple times\n",
        "    paired_pred = np.argmax(pairwise_iou, axis=1)\n",
        "    pairwise_iou = np.max(pairwise_iou, axis=1)\n",
        "    # exlude those dont have intersection\n",
        "    paired_true = np.nonzero(pairwise_iou > 0.0)[0]\n",
        "    paired_pred = paired_pred[paired_true]\n",
        "    # print(paired_true.shape, paired_pred.shape)\n",
        "    overall_inter = (pairwise_inter[paired_true, paired_pred]).sum()\n",
        "    overall_union = (pairwise_union[paired_true, paired_pred]).sum()\n",
        "\n",
        "    paired_true = list(paired_true + 1)  # index to instance ID\n",
        "    paired_pred = list(paired_pred + 1)\n",
        "    # add all unpaired GT and Prediction into the union\n",
        "    unpaired_true = np.array(\n",
        "        [idx for idx in true_id_list[1:] if idx not in paired_true]\n",
        "    )\n",
        "    unpaired_pred = np.array(\n",
        "        [idx for idx in pred_id_list[1:] if idx not in paired_pred]\n",
        "    )\n",
        "    for true_id in unpaired_true:\n",
        "        overall_union += true_masks[true_id].sum()\n",
        "    for pred_id in unpaired_pred:\n",
        "        overall_union += pred_masks[pred_id].sum()\n",
        "\n",
        "    aji_score = overall_inter / overall_union\n",
        "    #print(aji_score)\n",
        "    return aji_score\n",
        "\n",
        "from scipy.optimize import linear_sum_assignment\n",
        "\n",
        "#############################################################################################################\n",
        "def get_fast_pq(true, pred, match_iou=0.5):\n",
        "    \"\"\"`match_iou` is the IoU threshold level to determine the pairing between\n",
        "    GT instances `p` and prediction instances `g`. `p` and `g` is a pair\n",
        "    if IoU > `match_iou`. However, pair of `p` and `g` must be unique\n",
        "    (1 prediction instance to 1 GT instance mapping).\n",
        "    If `match_iou` < 0.5, Munkres assignment (solving minimum weight matching\n",
        "    in bipartite graphs) is caculated to find the maximal amount of unique pairing.\n",
        "    If `match_iou` >= 0.5, all IoU(p,g) > 0.5 pairing is proven to be unique and\n",
        "    the number of pairs is also maximal.\n",
        "\n",
        "    Fast computation requires instance IDs are in contiguous orderding\n",
        "    i.e [1, 2, 3, 4] not [2, 3, 6, 10]. Please call `remap_label` beforehand\n",
        "    and `by_size` flag has no effect on the result.\n",
        "    Returns:\n",
        "        [dq, sq, pq]: measurement statistic\n",
        "        [paired_true, paired_pred, unpaired_true, unpaired_pred]:\n",
        "                      pairing information to perform measurement\n",
        "\n",
        "    \"\"\"\n",
        "    assert match_iou >= 0.0, \"Cant' be negative\"\n",
        "\n",
        "    true = np.copy(true)\n",
        "    pred = np.copy(pred)\n",
        "    true_id_list = list(np.unique(true))\n",
        "    pred_id_list = list(np.unique(pred))\n",
        "\n",
        "    if len(pred_id_list) == 1:\n",
        "        return [0, 0, 0], [0,0, 0, 0]\n",
        "\n",
        "    true_masks = [\n",
        "        None,\n",
        "    ]\n",
        "    for t in true_id_list[1:]:\n",
        "        t_mask = np.array(true == t, np.uint8)\n",
        "        true_masks.append(t_mask)\n",
        "\n",
        "    pred_masks = [\n",
        "        None,\n",
        "    ]\n",
        "    for p in pred_id_list[1:]:\n",
        "        p_mask = np.array(pred == p, np.uint8)\n",
        "        pred_masks.append(p_mask)\n",
        "\n",
        "    # prefill with value\n",
        "    pairwise_iou = np.zeros(\n",
        "        [len(true_id_list) - 1, len(pred_id_list) - 1], dtype=np.float64\n",
        "    )\n",
        "\n",
        "    # caching pairwise iou\n",
        "    for true_id in true_id_list[1:]:  # 0-th is background\n",
        "        t_mask = true_masks[true_id]\n",
        "        pred_true_overlap = pred[t_mask > 0]\n",
        "        pred_true_overlap_id = np.unique(pred_true_overlap)\n",
        "        pred_true_overlap_id = list(pred_true_overlap_id)\n",
        "        for pred_id in pred_true_overlap_id:\n",
        "            if pred_id == 0:  # ignore\n",
        "                continue  # overlaping background\n",
        "            p_mask = pred_masks[pred_id]\n",
        "            total = (t_mask + p_mask).sum()\n",
        "            inter = (t_mask * p_mask).sum()\n",
        "            iou = inter / (total - inter)\n",
        "            pairwise_iou[true_id - 1, pred_id - 1] = iou\n",
        "    #\n",
        "    if match_iou >= 0.5:\n",
        "        paired_iou = pairwise_iou[pairwise_iou > match_iou]\n",
        "        pairwise_iou[pairwise_iou <= match_iou] = 0.0\n",
        "        paired_true, paired_pred = np.nonzero(pairwise_iou)\n",
        "        paired_iou = pairwise_iou[paired_true, paired_pred]\n",
        "        paired_true += 1  # index is instance id - 1\n",
        "        paired_pred += 1  # hence return back to original\n",
        "    else:  # * Exhaustive maximal unique pairing\n",
        "        #### Munkres pairing with scipy library\n",
        "        # the algorithm return (row indices, matched column indices)\n",
        "        # if there is multiple same cost in a row, index of first occurence\n",
        "        # is return, thus the unique pairing is ensure\n",
        "        # inverse pair to get high IoU as minimum\n",
        "        paired_true, paired_pred = linear_sum_assignment(-pairwise_iou)\n",
        "        ### extract the paired cost and remove invalid pair\n",
        "        paired_iou = pairwise_iou[paired_true, paired_pred]\n",
        "\n",
        "        # now select those above threshold level\n",
        "        # paired with iou = 0.0 i.e no intersection => FP or FN\n",
        "        paired_true = list(paired_true[paired_iou > match_iou] + 1)\n",
        "        paired_pred = list(paired_pred[paired_iou > match_iou] + 1)\n",
        "        paired_iou = paired_iou[paired_iou > match_iou]\n",
        "\n",
        "    # get the actual FP and FN\n",
        "    unpaired_true = [idx for idx in true_id_list[1:] if idx not in paired_true]\n",
        "    unpaired_pred = [idx for idx in pred_id_list[1:] if idx not in paired_pred]\n",
        "    # print(paired_iou.shape, paired_true.shape, len(unpaired_true), len(unpaired_pred))\n",
        "\n",
        "    #\n",
        "    tp = len(paired_true)\n",
        "    fp = len(unpaired_pred)\n",
        "    fn = len(unpaired_true)\n",
        "    # get the F1-score i.e DQ\n",
        "    dq = tp / (tp + 0.5 * fp + 0.5 * fn)\n",
        "    # get the SQ, no paired has 0 iou so not impact\n",
        "    sq = paired_iou.sum() / (tp + 1.0e-6)\n",
        "\n",
        "    return [dq, sq, dq * sq], [paired_true, paired_pred, unpaired_true, unpaired_pred]\n",
        "\n",
        "\n",
        "#############################################################################################################\n",
        "def get_dice_1(true, pred):\n",
        "    \"\"\"Traditional dice.\"\"\"\n",
        "    # cast to binary 1st\n",
        "    true = np.copy(true)\n",
        "    pred = np.copy(pred)\n",
        "    true[true > 0] = 1\n",
        "    pred[pred > 0] = 1\n",
        "    inter = true * pred\n",
        "    denom = true + pred\n",
        "    dice_score = 2.0 * np.sum(inter) / (np.sum(denom) + 0.0001)\n",
        "    if np.sum(inter)==0 and np.sum(denom)==0:\n",
        "        dice_score = 1 # to handel cases without any nuclei\n",
        "    #print(dice_score)\n",
        "    return dice_score\n",
        "#############################################################################################################\n",
        "def remap_label(pred, by_size=False):\n",
        "    \"\"\"Rename all instance id so that the id is contiguous i.e [0, 1, 2, 3]\n",
        "    not [0, 2, 4, 6]. The ordering of instances (which one comes first)\n",
        "    is preserved unless by_size=True, then the instances will be reordered\n",
        "    so that bigger nucler has smaller ID.\n",
        "    Args:\n",
        "        pred    : the 2d array contain instances where each instances is marked\n",
        "                  by non-zero integer\n",
        "        by_size : renaming with larger nuclei has smaller id (on-top)\n",
        "    \"\"\"\n",
        "    pred_id = list(np.unique(pred))\n",
        "    pred_id.remove(0)\n",
        "    if len(pred_id) == 0:\n",
        "        return pred  # no label\n",
        "    if by_size:\n",
        "        pred_size = []\n",
        "        for inst_id in pred_id:\n",
        "            size = (pred == inst_id).sum()\n",
        "            pred_size.append(size)\n",
        "        # sort the id by size in descending order\n",
        "        pair_list = zip(pred_id, pred_size)\n",
        "        pair_list = sorted(pair_list, key=lambda x: x[1], reverse=True)\n",
        "        pred_id, pred_size = zip(*pair_list)\n",
        "\n",
        "    new_pred = np.zeros(pred.shape, np.int32)\n",
        "    for idx, inst_id in enumerate(pred_id):\n",
        "        new_pred[pred == inst_id] = idx + 1\n",
        "    return new_pred\n",
        "\n",
        "import scipy\n",
        "\n",
        "#############################################################################################################\n",
        "def pair_coordinates(setA, setB, radius):\n",
        "    \"\"\"Use the Munkres or Kuhn-Munkres algorithm to find the most optimal\n",
        "    unique pairing (largest possible match) when pairing points in set B\n",
        "    against points in set A, using distance as cost function.\n",
        "    Args:\n",
        "        setA, setB: np.array (float32) of size Nx2 contains the of XY coordinate\n",
        "                    of N different points\n",
        "        radius: valid area around a point in setA to consider\n",
        "                a given coordinate in setB a candidate for match\n",
        "    Return:\n",
        "        pairing: pairing is an array of indices\n",
        "        where point at index pairing[0] in set A paired with point\n",
        "        in set B at index pairing[1]\n",
        "        unparedA, unpairedB: remaining poitn in set A and set B unpaired\n",
        "    \"\"\"\n",
        "    # * Euclidean distance as the cost matrix\n",
        "    pair_distance = scipy.spatial.distance.cdist(setA, setB, metric='euclidean')\n",
        "\n",
        "    # * Munkres pairing with scipy library\n",
        "    # the algorithm return (row indices, matched column indices)\n",
        "    # if there is multiple same cost in a row, index of first occurence\n",
        "    # is return, thus the unique pairing is ensured\n",
        "    indicesA, paired_indicesB = linear_sum_assignment(pair_distance)\n",
        "\n",
        "    # extract the paired cost and remove instances\n",
        "    # outside of designated radius\n",
        "    pair_cost = pair_distance[indicesA, paired_indicesB]\n",
        "\n",
        "    pairedA = indicesA[pair_cost <= radius]\n",
        "    pairedB = paired_indicesB[pair_cost <= radius]\n",
        "\n",
        "    pairing = np.concatenate([pairedA[:,None], pairedB[:,None]], axis=-1)\n",
        "    unpairedA = np.delete(np.arange(setA.shape[0]), pairedA)\n",
        "    unpairedB = np.delete(np.arange(setB.shape[0]), pairedB)\n",
        "    return pairing, unpairedA, unpairedB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "50e3a0a2",
      "metadata": {
        "id": "50e3a0a2"
      },
      "outputs": [],
      "source": [
        "# data generator related functions\n",
        "def get_id_from_file_path(file_path, indicator):\n",
        "    return file_path.split(os.path.sep)[-1].replace(indicator, '')\n",
        "#############################################################################################################\n",
        "# def chunker(seq, seq2, size):\n",
        "#     return ([seq[pos:pos + size], seq2[pos:pos + size]] for pos in range(0, len(seq), size))\n",
        "\n",
        "def chunker(seq, seq2, seq3, size):\n",
        "    return ([seq[pos:pos + size],\n",
        "             seq2[pos:pos + size],\n",
        "             seq3[pos:pos + size]] for pos in range(0, len(seq), size))\n",
        "\n",
        "\n",
        "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
        "\n",
        "#############################################################################################################\n",
        "def data_gen(list_files, list_files2,list_files3, batch_size, p , size_row, size_col, distance_unet_flag = 0,\n",
        "             augment= False, BACKBONE_model = None, use_pretrain_flag = 1):\n",
        "    crop_size_row = size_row\n",
        "    crop_size_col = size_col\n",
        "    aug = albumentation_aug(p, crop_size_row, crop_size_col)\n",
        "\n",
        "    while True:\n",
        "        for batch in chunker(list_files,list_files2,list_files3, batch_size):\n",
        "            X, seg_masks, dist_maps = [], [], []\n",
        "\n",
        "            for count in range(len(batch[0])):\n",
        "                x = cv2.imread(batch[0][count])\n",
        "                x = cv2.cvtColor(x, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
        "                x_mask = cv2.imread(batch[1][count], cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "                x_mask_temp = np.zeros((x_mask.shape[0], x_mask.shape[1]), dtype=np.float32)\n",
        "                # x_mask_temp[x_mask == 255] = 1 #This wrong\n",
        "\n",
        "                dist_map = cv2.imread(batch[2][count], cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "                dist_map_temp = np.zeros((dist_map.shape[0], dist_map.shape[1]),dtype=np.float32)\n",
        "                dist_map_temp[dist_map == 255] = 1\n",
        "\n",
        "                if distance_unet_flag == False:\n",
        "                    if augment:\n",
        "                        augmented = aug(image= x, mask= x_mask_temp,dist_map= dist_map_temp)\n",
        "                        x = augmented['image']\n",
        "                        if use_pretrain_flag == 1:\n",
        "                            x = preprocess_input(x)\n",
        "                        x_mask_temp = augmented['mask']\n",
        "                        dist_map_temp = augmented['dist_map']\n",
        "                        x = x/255\n",
        "                    else:\n",
        "                        x = x/255\n",
        "                    X.append(x)\n",
        "                    seg_masks.append(x_mask_temp)\n",
        "                    dist_maps.append(dist_map_temp)\n",
        "                else:\n",
        "                    if augment:\n",
        "                        augmented = aug(image=x, mask=x_mask,dist_map= dist_map)\n",
        "                        x = augmented['image']\n",
        "                        if use_pretrain_flag == 1:\n",
        "                            x = preprocess_input(x)\n",
        "                        x_mask = augmented['mask']\n",
        "                        dist_map = augmented['dist_map']\n",
        "                        x = x/255\n",
        "                    else:\n",
        "                        x = x/255\n",
        "\n",
        "                    X.append(x)\n",
        "                    x_mask = (x_mask - np.min(x_mask))/ (np.max(x_mask) - np.min(x_mask) + 0.0000001)\n",
        "                    seg_masks.append(x_mask)\n",
        "                    dist_map = (dist_map - np.min(dist_map)) / (np.max(dist_map) - np.min(dist_map) + 0.0000001)\n",
        "                    dist_maps.append(dist_map)\n",
        "\n",
        "                del x_mask\n",
        "                del x_mask_temp\n",
        "                del x\n",
        "            # Y = np.expand_dims(np.array(Y), axis=3)\n",
        "            # Y = np.array(Y)\n",
        "            yield np.array(X), {\n",
        "                'output_dis': np.expand_dims(np.array(dist_maps, dtype=np.float32), axis=3),\n",
        "                'output_seg': np.expand_dims(np.array(seg_masks, dtype=np.float32), axis=3)\n",
        "            }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "084e1b0c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "084e1b0c",
        "outputId": "872363af-53f2-493f-dc40-0e108f622a4f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/albumentations/core/validation.py:114: UserWarning: ShiftScaleRotate is a special case of Affine transform. Please use Affine transform instead.\n",
            "  original_init(self, **validated_kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 1/20\n",
            "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 956ms/step - loss: 0.0952 - output_dis_loss: 0.0049 - output_dis_mse_score: 0.0049 - output_seg_dice_coef: 0.2172 - output_seg_loss: 0.0903"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/callbacks/model_checkpoint.py:302: UserWarning: Can save best model only with val_dice_coef available.\n",
            "  if self._should_save_model(epoch, batch, logs, filepath):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 1s/step - loss: 0.0942 - output_dis_loss: 0.0048 - output_dis_mse_score: 0.0048 - output_seg_dice_coef: 0.2171 - output_seg_loss: 0.0894 - val_loss: -0.0562 - val_output_dis_loss: 1.3101e-04 - val_output_dis_mse_score: 1.3101e-04 - val_output_seg_dice_coef: 0.2624 - val_output_seg_loss: -0.0563 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 2: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 2/20\n",
            "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 908ms/step - loss: 0.0061 - output_dis_loss: 1.9327e-04 - output_dis_mse_score: 1.8872e-04 - output_seg_dice_coef: 0.2075 - output_seg_loss: 0.0070 - val_loss: -0.2722 - val_output_dis_loss: 1.7511e-05 - val_output_dis_mse_score: 1.7511e-05 - val_output_seg_dice_coef: 0.4680 - val_output_seg_loss: -0.2722 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 3: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 3/20\n",
            "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 982ms/step - loss: 0.0074 - output_dis_loss: 1.8947e-05 - output_dis_mse_score: 1.9750e-05 - output_seg_dice_coef: 0.2137 - output_seg_loss: 0.0103 - val_loss: -0.1842 - val_output_dis_loss: 1.5949e-05 - val_output_dis_mse_score: 1.5949e-05 - val_output_seg_dice_coef: 0.4030 - val_output_seg_loss: -0.1842 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 4: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 4/20\n",
            "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 850ms/step - loss: -0.0292 - output_dis_loss: 1.3594e-05 - output_dis_mse_score: 1.3492e-05 - output_seg_dice_coef: 0.2383 - output_seg_loss: -0.0278 - val_loss: -0.2808 - val_output_dis_loss: 1.5965e-05 - val_output_dis_mse_score: 1.5965e-05 - val_output_seg_dice_coef: 0.4779 - val_output_seg_loss: -0.2808 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 5: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 5/20\n",
            "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 897ms/step - loss: -0.0192 - output_dis_loss: 1.2974e-05 - output_dis_mse_score: 1.3421e-05 - output_seg_dice_coef: 0.2313 - output_seg_loss: -0.0160 - val_loss: -0.3046 - val_output_dis_loss: 1.5419e-05 - val_output_dis_mse_score: 1.5419e-05 - val_output_seg_dice_coef: 0.4955 - val_output_seg_loss: -0.3046 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 6: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 6/20\n",
            "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 891ms/step - loss: -0.0288 - output_dis_loss: 1.1397e-05 - output_dis_mse_score: 1.1367e-05 - output_seg_dice_coef: 0.2446 - output_seg_loss: -0.0279 - val_loss: -0.2797 - val_output_dis_loss: 1.5019e-05 - val_output_dis_mse_score: 1.5019e-05 - val_output_seg_dice_coef: 0.4783 - val_output_seg_loss: -0.2797 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 7: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 7/20\n",
            "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 885ms/step - loss: -0.0560 - output_dis_loss: 1.2031e-05 - output_dis_mse_score: 1.1562e-05 - output_seg_dice_coef: 0.2634 - output_seg_loss: -0.0541 - val_loss: -0.3098 - val_output_dis_loss: 1.4760e-05 - val_output_dis_mse_score: 1.4760e-05 - val_output_seg_dice_coef: 0.5359 - val_output_seg_loss: -0.3098 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 8: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 8/20\n",
            "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 901ms/step - loss: -0.0517 - output_dis_loss: 9.4217e-06 - output_dis_mse_score: 9.5739e-06 - output_seg_dice_coef: 0.2725 - output_seg_loss: -0.0541 - val_loss: -0.2733 - val_output_dis_loss: 1.5453e-05 - val_output_dis_mse_score: 1.5453e-05 - val_output_seg_dice_coef: 0.5266 - val_output_seg_loss: -0.2733 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 9: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 9/20\n",
            "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 887ms/step - loss: -0.0298 - output_dis_loss: 9.0867e-06 - output_dis_mse_score: 9.1524e-06 - output_seg_dice_coef: 0.2570 - output_seg_loss: -0.0274 - val_loss: -0.3443 - val_output_dis_loss: 1.4697e-05 - val_output_dis_mse_score: 1.4697e-05 - val_output_seg_dice_coef: 0.5341 - val_output_seg_loss: -0.3443 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 10: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 10/20\n",
            "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 893ms/step - loss: -0.0465 - output_dis_loss: 8.2664e-06 - output_dis_mse_score: 8.3056e-06 - output_seg_dice_coef: 0.2665 - output_seg_loss: -0.0542 - val_loss: -0.3409 - val_output_dis_loss: 1.4252e-05 - val_output_dis_mse_score: 1.4252e-05 - val_output_seg_dice_coef: 0.5580 - val_output_seg_loss: -0.3410 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 11: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 11/20\n",
            "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 902ms/step - loss: -0.0659 - output_dis_loss: 7.7554e-06 - output_dis_mse_score: 7.7969e-06 - output_seg_dice_coef: 0.2822 - output_seg_loss: -0.0677 - val_loss: -0.3768 - val_output_dis_loss: 1.4228e-05 - val_output_dis_mse_score: 1.4228e-05 - val_output_seg_dice_coef: 0.5774 - val_output_seg_loss: -0.3769 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 12: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 12/20\n",
            "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 901ms/step - loss: -0.0615 - output_dis_loss: 7.6650e-06 - output_dis_mse_score: 7.7278e-06 - output_seg_dice_coef: 0.2773 - output_seg_loss: -0.0637 - val_loss: -0.4113 - val_output_dis_loss: 1.4329e-05 - val_output_dis_mse_score: 1.4329e-05 - val_output_seg_dice_coef: 0.6007 - val_output_seg_loss: -0.4113 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 13: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 13/20\n",
            "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 916ms/step - loss: -0.0587 - output_dis_loss: 8.0273e-06 - output_dis_mse_score: 8.1050e-06 - output_seg_dice_coef: 0.2653 - output_seg_loss: -0.0564 - val_loss: -0.4267 - val_output_dis_loss: 1.4144e-05 - val_output_dis_mse_score: 1.4144e-05 - val_output_seg_dice_coef: 0.6115 - val_output_seg_loss: -0.4267 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 14: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 14/20\n",
            "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 886ms/step - loss: -0.0641 - output_dis_loss: 7.7156e-06 - output_dis_mse_score: 7.7832e-06 - output_seg_dice_coef: 0.2774 - output_seg_loss: -0.0615 - val_loss: -0.4726 - val_output_dis_loss: 1.4118e-05 - val_output_dis_mse_score: 1.4118e-05 - val_output_seg_dice_coef: 0.6201 - val_output_seg_loss: -0.4726 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 15: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 15/20\n",
            "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 871ms/step - loss: -0.0911 - output_dis_loss: 7.9121e-06 - output_dis_mse_score: 7.9771e-06 - output_seg_dice_coef: 0.2906 - output_seg_loss: -0.0887 - val_loss: -0.4701 - val_output_dis_loss: 1.4105e-05 - val_output_dis_mse_score: 1.4105e-05 - val_output_seg_dice_coef: 0.6333 - val_output_seg_loss: -0.4701 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 16: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 16/20\n",
            "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 872ms/step - loss: -0.1100 - output_dis_loss: 7.7438e-06 - output_dis_mse_score: 7.8002e-06 - output_seg_dice_coef: 0.3122 - output_seg_loss: -0.1071 - val_loss: -0.4495 - val_output_dis_loss: 1.4102e-05 - val_output_dis_mse_score: 1.4102e-05 - val_output_seg_dice_coef: 0.6393 - val_output_seg_loss: -0.4496 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 17: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 17/20\n",
            "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 871ms/step - loss: -0.0906 - output_dis_loss: 7.4323e-06 - output_dis_mse_score: 7.4809e-06 - output_seg_dice_coef: 0.2915 - output_seg_loss: -0.0898 - val_loss: -0.5041 - val_output_dis_loss: 1.4098e-05 - val_output_dis_mse_score: 1.4098e-05 - val_output_seg_dice_coef: 0.6625 - val_output_seg_loss: -0.5041 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 18: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 18/20\n",
            "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 881ms/step - loss: -0.1260 - output_dis_loss: 7.5612e-06 - output_dis_mse_score: 7.6062e-06 - output_seg_dice_coef: 0.3216 - output_seg_loss: -0.1245 - val_loss: -0.5342 - val_output_dis_loss: 1.4096e-05 - val_output_dis_mse_score: 1.4096e-05 - val_output_seg_dice_coef: 0.6924 - val_output_seg_loss: -0.5342 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 19: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 19/20\n",
            "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 872ms/step - loss: -0.0868 - output_dis_loss: 7.2767e-06 - output_dis_mse_score: 7.3165e-06 - output_seg_dice_coef: 0.2928 - output_seg_loss: -0.0860 - val_loss: -0.5089 - val_output_dis_loss: 1.4087e-05 - val_output_dis_mse_score: 1.4087e-05 - val_output_seg_dice_coef: 0.6609 - val_output_seg_loss: -0.5089 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 20: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 20/20\n",
            "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 872ms/step - loss: -0.0828 - output_dis_loss: 7.5179e-06 - output_dis_mse_score: 7.5541e-06 - output_seg_dice_coef: 0.2926 - output_seg_loss: -0.0872 - val_loss: -0.5413 - val_output_dis_loss: 1.4098e-05 - val_output_dis_mse_score: 1.4098e-05 - val_output_seg_dice_coef: 0.6942 - val_output_seg_loss: -0.5413 - learning_rate: 0.0010\n"
          ]
        }
      ],
      "source": [
        "kf = KFold(n_splits= opts['k_fold'],random_state= opts['random_seed_num'],shuffle=True)\n",
        "kf.get_n_splits(img_path)\n",
        "\n",
        "start_time = time.time()\n",
        "dice_unet = np.zeros([opts['k_fold'],len(img_path)//opts['k_fold']])\n",
        "AJI_unet = np.zeros([opts['k_fold'],len(img_path)//opts['k_fold']])\n",
        "PQ_unet = np.zeros([opts['k_fold'],len(img_path)//opts['k_fold']])\n",
        "\n",
        "\n",
        "dice_unet_watershed = np.zeros([opts['k_fold'],len(img_path)//opts['k_fold']])\n",
        "AJI_unet_watershed = np.zeros([opts['k_fold'],len(img_path)//opts['k_fold']])\n",
        "PQ_unet_watershed = np.zeros([opts['k_fold'],len(img_path)//opts['k_fold']])\n",
        "\n",
        "dice_unet_watershed_without_vague = np.zeros([opts['k_fold'],len(img_path)//opts['k_fold']])\n",
        "AJI_unet_watershed_without_vague = np.zeros([opts['k_fold'],len(img_path)//opts['k_fold']])\n",
        "PQ_unet_watershed_without_vague = np.zeros([opts['k_fold'],len(img_path)//opts['k_fold']])\n",
        "\n",
        "dice_mean = []\n",
        "aji_mean = []\n",
        "pq_mean = []\n",
        "\n",
        "dice_watershed_mean = []\n",
        "aji_watershed_mean = []\n",
        "pq_watershed_mean = []\n",
        "\n",
        "dice_watershed_wovague_mean = []\n",
        "aji_watershed_wovague_mean = []\n",
        "pq_watershed_wovague_mean = []\n",
        "\n",
        "current_fold = 1\n",
        "\n",
        "for idx, [train_index,  test_index] in enumerate(kf.split(img_path)):\n",
        "    shuffle(train_index)\n",
        "    shuffle(test_index)\n",
        "\n",
        "    train_img   = [img_path[name] for name in train_index]\n",
        "    train_mask  = [binary_mask_path[name] for name in train_index]\n",
        "    train_dis   = [distance_mask_path[name] for name in train_index]\n",
        "    train_label = [label_mask_path[name] for name in train_index]\n",
        "\n",
        "    test_img   = [img_path[name] for name in test_index]\n",
        "    test_mask  = [binary_mask_path[name] for name in test_index]\n",
        "    test_dis   = [distance_mask_path[name] for name in test_index]\n",
        "    test_label = [label_mask_path[name] for name in test_index]\n",
        "    test_vague = [vague_mask_path[name] for name in test_index]\n",
        "\n",
        "    #creating validation set\n",
        "    validation_set_img = []\n",
        "    validation_set_label = []\n",
        "    # validation_DIS = []\n",
        "    validation_set_vague = []\n",
        "    for counter in range(len(test_img)):\n",
        "        val_img = cv2.imread(test_img[counter])\n",
        "        val_img = cv2.cvtColor(val_img, cv2.COLOR_BGR2RGB)\n",
        "        val_img = val_img/255\n",
        "        val_label = cv2.imread(test_label[counter], -1) # cv2.IMREAD_UNCHANGED:\n",
        "        #It specifies to load an image as such including alpha channel.\n",
        "        #Alternatively, we can pass integer value -1 for this flag.\n",
        "        val_vague = cv2.imread(test_vague[counter], -1)\n",
        "        val_DIS = cv2.imread(test_dis[counter], -1)\n",
        "        # validation_DIS.append(val_DIS)\n",
        "        validation_set_img.append(val_img)\n",
        "        validation_set_label.append(val_label)\n",
        "        validation_set_vague.append(val_vague)\n",
        "\n",
        "    validation_set_img = np.array(validation_set_img)\n",
        "    validation_set_label = np.array(validation_set_label)\n",
        "    validation_set_vague = np.array(validation_set_vague)\n",
        "    # validation_DIS = np.array(validation_DIS)\n",
        "\n",
        "    model_path = opts['model_save_path'] + 'unet_{}.weights.h5'.format(current_fold)\n",
        "    logger = CSVLogger(opts['model_save_path']+ 'unet_{}.log'.format(current_fold))\n",
        "    LR_drop = step_decay_schedule(initial_lr= opts['init_LR'],\n",
        "                              decay_factor = opts['LR_decay_factor'],\n",
        "                              epochs_drop = opts['LR_drop_after_nth_epoch'])\n",
        "    # model_raw = shallow_unet(opts['number_of_channel'], opts['init_LR'])\n",
        "    model_raw = dual_decoder_unet_binary(opts['number_of_channel'], opts['init_LR'])\n",
        "    checkpoint = ModelCheckpoint(model_path, monitor='val_dice_coef', verbose=1,\n",
        "                             save_best_only=True, mode='max', save_weights_only = True)\n",
        "    history = model_raw.fit(data_gen(train_img,\n",
        "                                                 train_mask,train_dis,\n",
        "                                                 opts['batch_size'],\n",
        "                                                 1,\n",
        "                                                 opts['crop_size'], opts['crop_size'],\n",
        "                                                 distance_unet_flag=0,\n",
        "                                                 augment=True,\n",
        "                                                 BACKBONE_model= '',\n",
        "                                                 use_pretrain_flag= False),\n",
        "                                      validation_data=data_gen(test_img,\n",
        "                                                               test_mask, test_dis,\n",
        "                                                               opts['batch_size'],\n",
        "                                                               1,\n",
        "                                                               opts['crop_size'], opts['crop_size'],\n",
        "                                                               distance_unet_flag=0,\n",
        "                                                               augment= False,\n",
        "                                                               BACKBONE_model= '',\n",
        "                                                               use_pretrain_flag= False),\n",
        "                                      validation_steps=1,\n",
        "                                      epochs=opts['epoch_num'], verbose=1,\n",
        "                                      callbacks=[checkpoint, logger, LR_drop],\n",
        "                                      steps_per_epoch=(len(train_img) // opts['batch_size']) // opts['quick_run'])\n",
        "\n",
        "    model_raw.load_weights(opts['model_save_path'] + 'unet_{}.weights.h5'.format(current_fold))\n",
        "\n",
        "    ## predication on validation set\n",
        "    pred_val = model_raw.predict(validation_set_img, verbose=1, batch_size=1)\n",
        "    pred_val_seg = pred_val[0]\n",
        "    pred_val_t = (pred_val_seg > opts['treshold']).astype(np.uint8)\n",
        "    pred_dis = pred_val[1]\n",
        "\n",
        "    output_watershed_tot_fold = []\n",
        "    output_watershed_tot_fold_wo_vague = []\n",
        "    validation_set_label_tot_fold_wo_vague = []\n",
        "    for val_len in tqdm.tqdm(range(len(pred_val_seg))):\n",
        "\n",
        "        # Post Processing\n",
        "        image = np.squeeze(pred_val_seg[val_len]).astype(np.uint8)\n",
        "\n",
        "        distance_map = np.squeeze(pred_dis[val_len])\n",
        "\n",
        "\n",
        "        # NOTE: The code for obtaining average and gaussian filter generated using ChatGPT. Reed to reverify.\n",
        "        # Average Nuclei Sizes\n",
        "        num_labels, labels, stats, _ = cv2.connectedComponentsWithStats(image, connectivity=8)\n",
        "        sizes = stats[1:, cv2.CC_STAT_AREA]\n",
        "\n",
        "        # we identify the local maxima from the filtered predicted distance maps and use them as seed points for a marker-controlled watershed algorithm (36) to produce the labeled segmented masks.\n",
        "        # In base watershed applied directly to the image\n",
        "        # We first calculate the average nucleus size from the semantic segmentation results (binary mask head), and then apply a Gaussian smoothing filter to the distance map predictions with the kernel size of the file derived from the average nucleus size.\n",
        "\n",
        "        avg_size = np.mean(sizes)\n",
        "        sigma = max(1, int(avg_size ** 0.5))  # Gaussian sigma ~ sqrt(area)\n",
        "        smoothed_dis = gaussian_filter(distance_map, sigma=sigma) #need to write it,\n",
        "\n",
        "        # Get coordinates of local maxima\n",
        "        coords = peak_local_max(\n",
        "            image,\n",
        "            footprint=np.ones((15, 15)),\n",
        "            exclude_border=False,\n",
        "            labels=None\n",
        "        )\n",
        "\n",
        "        # # Create a boolean mask from the coordinates\n",
        "        # local_maxi = np.zeros_like(image, dtype=bool)\n",
        "        # local_maxi[tuple(coords.T)] = True\n",
        "\n",
        "        # # Label the local maxima to use as markers\n",
        "        # marker = ndi.label(local_maxi)[0]\n",
        "        # output_watershed = watershed(-np.squeeze(pred_val_seg[val_len]), marker,mask = np.squeeze(pred_val_t[[val_len]]))\n",
        "        # output_watershed[np.squeeze(pred_val_t[[val_len]])==0] = 0\n",
        "        # output_watershed = remove_small_objects(output_watershed, min_size=50, connectivity=2)#remove small objects\n",
        "\n",
        "       marker = ndi.label(local_maxi)[0]\n",
        "       output_watershed = watershed(-smoothed_dis, markers, mask=image)\n",
        "       output_watershed = remove_small_objects(output_watershed, min_size=50, connectivity=2)#remove small objects\n",
        "\n",
        "\n",
        "        # without post processing\n",
        "        output_raw_0 = np.squeeze(pred_val_t[val_len])\n",
        "        output_raw = skimage.morphology.label(output_raw_0)\n",
        "        output_raw = remove_small_objects(output_raw, min_size=50, connectivity=2) #remove small objects\n",
        "\n",
        "\n",
        "        output_watershed = remap_label(output_watershed)\n",
        "        validation_set_label[val_len] = remap_label(validation_set_label[val_len])\n",
        "        output_raw = remap_label(output_raw)\n",
        "\n",
        "        test_name = get_id_from_file_path(test_img[val_len],'.png' )\n",
        "\n",
        "        imsave(opts['result_save_path']+'validation/watershed_unet/{}.png'.format(test_name),\n",
        "               output_watershed.astype(np.uint16))\n",
        "        imsave(opts['result_save_path']+'validation/unet/{}.png'.format(test_name),output_raw.astype(np.uint16))\n",
        "\n",
        "\n",
        "\n",
        "        dice_unet[current_fold-1, val_len]= get_dice_1(validation_set_label[val_len], output_raw)\n",
        "        AJI_unet[current_fold-1, val_len] = get_fast_aji(validation_set_label[val_len], output_raw)\n",
        "        PQ_unet[current_fold-1, val_len] = get_fast_pq(validation_set_label[val_len], output_raw)[0][2]\n",
        "\n",
        "\n",
        "        dice_unet_watershed[current_fold-1, val_len]= get_dice_1(validation_set_label[val_len],output_watershed)\n",
        "        AJI_unet_watershed[current_fold-1, val_len] = get_fast_aji(validation_set_label[val_len], output_watershed)\n",
        "        PQ_unet_watershed[current_fold-1, val_len]  = get_fast_pq(validation_set_label[val_len], output_watershed)[0][2]\n",
        "        ###################################################################\n",
        "\n",
        "        #\n",
        "        output_watershed_wo_vague = np.copy(output_watershed)\n",
        "        output_watershed_wo_vague[validation_set_vague[val_len] == 255] = 0\n",
        "        output_watershed_wo_vague = remove_small_objects(output_watershed_wo_vague, min_size=50, connectivity=2)\n",
        "\n",
        "        validation_set_label_wo_vague = np.copy(validation_set_label[val_len])\n",
        "        validation_set_label_wo_vague[validation_set_vague[val_len] == 255] = 0\n",
        "        validation_set_label_wo_vague = remove_small_objects(validation_set_label_wo_vague, min_size=50, connectivity=2)\n",
        "\n",
        "        output_watershed_wo_vague = remap_label(output_watershed_wo_vague)\n",
        "        validation_set_label_wo_vague = remap_label(validation_set_label_wo_vague)\n",
        "\n",
        "\n",
        "\n",
        "        dice_unet_watershed_without_vague[current_fold-1, val_len]= get_dice_1(\n",
        "            validation_set_label_wo_vague,output_watershed_wo_vague)\n",
        "        AJI_unet_watershed_without_vague[current_fold-1, val_len] = get_fast_aji(\n",
        "            validation_set_label_wo_vague,output_watershed_wo_vague)\n",
        "        PQ_unet_watershed_without_vague[current_fold-1, val_len]  = get_fast_pq(\n",
        "            validation_set_label_wo_vague,output_watershed_wo_vague)[0][2]\n",
        "\n",
        "        output_watershed_tot_fold.append(output_watershed)\n",
        "        output_watershed_tot_fold_wo_vague.append(output_watershed_wo_vague)\n",
        "        validation_set_label_tot_fold_wo_vague.append(validation_set_label_wo_vague)\n",
        "\n",
        "    print('==========')\n",
        "    print('average dice pure Unet for fold{}: {:.2f}'.format(current_fold, np.mean(dice_unet[current_fold-1, :]*100)))\n",
        "    print('average AJI pure Unet for fold{}: {:.2f}'.format(current_fold, np.mean(AJI_unet[current_fold-1, :]*100)))\n",
        "    print('average PQ pure Unet for fold{}: {:.2f}'.format(current_fold, np.mean(PQ_unet[current_fold-1, :]*100)))\n",
        "    dice_mean.append(np.mean(dice_unet[current_fold-1, :]*100))\n",
        "    aji_mean.append(np.mean(AJI_unet[current_fold-1, :]*100))\n",
        "    pq_mean.append(np.mean(PQ_unet[current_fold-1, :]*100))\n",
        "    print('==========')\n",
        "\n",
        "    print('==========')\n",
        "    print('average Dice Unet watershed for fold{}: {:.2f}'.format(current_fold,\n",
        "                                                                   np.mean(dice_unet_watershed[current_fold-1, :]*100)))\n",
        "    print('average AJI Unet watershed for fold{}: {:.2f}'.format(current_fold,\n",
        "                                                                  np.mean(AJI_unet_watershed[current_fold-1, :]*100)))\n",
        "    print('average PQ Unet watershed for fold{}: {:.2f}'.format(current_fold,\n",
        "                                                                 np.mean(PQ_unet_watershed[current_fold-1, :]*100)))\n",
        "    dice_watershed_mean.append(np.mean(dice_unet_watershed[current_fold-1, :]*100))\n",
        "    aji_watershed_mean.append(np.mean(AJI_unet_watershed[current_fold-1, :]*100))\n",
        "    pq_watershed_mean.append(np.mean(PQ_unet_watershed[current_fold-1, :]*100))\n",
        "    print('==========')\n",
        "\n",
        "    print('average Dice Unet watershed wo vague for fold{}: {:.2f}'.format(current_fold,\n",
        "                                                                   np.mean(dice_unet_watershed_without_vague[current_fold-1, :]*100)))\n",
        "    print('average AJI Unet watershed wo vague for fold{}: {:.2f}'.format(current_fold,\n",
        "                                                                  np.mean(AJI_unet_watershed_without_vague[current_fold-1, :]*100)))\n",
        "    print('average PQ Unet watershed wo vague for fold{}: {:.2f}'.format(current_fold,\n",
        "                                                                 np.mean(PQ_unet_watershed_without_vague[current_fold-1, :]*100)))\n",
        "    dice_watershed_wovague_mean.append(np.mean(dice_unet_watershed_without_vague[current_fold-1, :]*100))\n",
        "    aji_watershed_wovague_mean.append(np.mean(AJI_unet_watershed_without_vague[current_fold-1, :]*100))\n",
        "    pq_watershed_wovague_mean.append(np.mean(PQ_unet_watershed_without_vague[current_fold-1, :]*100))\n",
        "    print('==========')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    current_fold = current_fold + 1\n",
        "#     if idx ==0:\n",
        "#         break\n",
        "\n",
        "fold_names = ['fold1', 'fold2','fold3','fold4','fold5']\n",
        "df_dice = pd.DataFrame({'fold num':fold_names, 'dice unet':dice_mean,'dice unet watershed':dice_watershed_mean,\n",
        "                   'dice unet whatershed wo vague':dice_watershed_wovague_mean})\n",
        "\n",
        "df_aji = pd.DataFrame({'fold num':fold_names, 'AJI unet':aji_mean,'AJI unet watershed':aji_watershed_mean,\n",
        "                   'AJI unet whatershed wo vague':aji_watershed_wovague_mean})\n",
        "\n",
        "df_pq = pd.DataFrame({'fold num':fold_names, 'PQ unet':pq_mean,'PQ unet watershed':pq_watershed_mean,\n",
        "                   'PQ unet whatershed wo vague':pq_watershed_wovague_mean})\n",
        "\n",
        "df_dice.to_csv('dice.csv', index=False)\n",
        "df_aji.to_csv('aji.csv', index=False)\n",
        "df_pq.to_csv('pq.csv', index=False)\n",
        "\n",
        "print(df_dice.head())\n",
        "print('============================================================')\n",
        "print(df_aji.head())\n",
        "print('============================================================')\n",
        "print(df_pq.head())\n",
        "print('============================================================')\n",
        "\n",
        "\n",
        "finish_time = time.time()\n",
        "print('==========')\n",
        "print('total training time (all 5 folds): {:.2f} minutes'.format((finish_time- start_time)/60))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "958db772",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280
        },
        "id": "958db772",
        "outputId": "02fa9bfc-f61a-4514-88e2-b237aaf0da62"
      },
      "outputs": [
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAABJ4AAAGACAYAAADs96imAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAALAxJREFUeJzt3Xm01XW9+P/X3mdknlFwAAQcgETvMU1UoBInwszKq6iBSpmm/mylLUuXApLmEGoDmssr3ZUNLrS0q5mGWgn2LUvIq10VDRyQGJR5Pud8fn/Q2bE5BzjgeZ/D8His5dLz+Xz2/rz3xrXfnOf+DLksy7IAAAAAgCaWb+kBAAAAALBnEp4AAAAASEJ4AgAAACAJ4QkAAACAJIQnAAAAAJIQngAAAABIQngCAAAAIAnhCQAAAIAkhCcAAAAAkhCe2OPlcrkYP358Sw9jm8aOHRtt27Zt6WEA0AzmzZsXuVwufvSjHxWWjR8/PnK5XJPt43e/+13kcrn43e9+12TPCQCN0Zy/2/Tu3TvGjh3bLPti5wlPRETE3Llz47LLLouDDz44WrduHa1bt44BAwbEV77ylXjppZdaenhJDR8+PHK53Hb/+bDxas2aNTF+/Hi/BAB7hLpQsmTJkgbXDxo0KIYPH174uS625HK5ePjhh3f4+bbl+eefj/Hjx8eyZcsatf3YsWOLPt/bt28fgwcPju985zuxfv36Hd5/S5oyZUpRwALYFe3Oc0ZTufXWWyOXy8WsWbOKlmdZFp06dYpcLhdz584tWrdu3bqoqKiI0aNH79C+brrppnjkkUc+7JChyZS29ABoeY899lj853/+Z5SWlsa5554bgwcPjnw+H6+++mr84he/iLvvvjvmzp0bvXr1aumhJnHttdfGuHHjCj+/8MIL8d3vfje++c1vxmGHHVZYfvjhh3+o/axZsyYmTJgQEVE0sQLsbSZOnBhnnnlmkx3h8/zzz8eECRNi7Nix0bFjx0Y9pqKiIu67776IiFi2bFk8/PDDcdVVV8ULL7wQP//5z5tkXDviuuuui2uuuWaHHzdlypTo2rVrvW97hw4dGmvXro3y8vImGiFAy9gV5oymcPzxx0dExIwZM+LII48sLH/llVdi2bJlUVpaGjNnzow+ffoU1r3wwguxYcOGwmMb66abborPfe5zccYZZzTJ2OHDEp72cm+++WacffbZ0atXr3j66aejR48eRetvueWWmDJlSuTz2z44bvXq1dGmTZuUQ01mxIgRRT9XVlbGd7/73RgxYsQ2A9Hu/JoBWsoRRxwRs2fPjl/+8pdx5plnttg4SktL47zzziv8fOmll8YxxxwTDz74YEyePDl69uxZ7zFZlsW6deuiVatWScZTWtp0fy3L5/NRWVnZZM8H0BJ2lTljS7lcLqZOnbpDp3gdddRRUVlZGTNmzIjLL7+8sHzmzJnRpUuXOOqoo2LGjBlFc9OMGTMiInY4PKWwbt26KC8v3+7vhdAQ/9fs5W699dZYvXp1TJ06tV50itj0F+ErrrgiDjjggMKyunN233zzzTjttNOiXbt2ce6550bEphjzta99LQ444ICoqKiIQw45JG6//fbIsqzw+IaubVFny1Pa6g6jfeONNwrfSnTo0CEuuOCCWLNmTdFj169fH1/96lejW7du0a5duzj99NPj3Xff/ZDvUPE4/v73v8fo0aOjU6dOhQlg+PDhDQaqsWPHRu/evQuvuVu3bhERMWHChK2evjd//vw444wzom3bttGtW7e46qqroqampkleA8Cu4Oyzz46DDz44Jk6cWDQ3bM2f/vSnOOWUU6JDhw7RunXrGDZsWMycObOwfvz48XH11VdHRESfPn0Kn6/z5s3boXHl8/nCZ3ndY3v37h2f+tSn4sknn4yjjjoqWrVqFT/84Q8jYtNRUldeeWVhvuvXr1/ccsstUVtbW/S8y5Yti7Fjx0aHDh2iY8eOMWbMmAZP79jaNZ4eeOCBOProo6N169bRqVOnGDp0aDz11FOF8b3yyivx+9//vvC6617D1q7xNG3atKiqqopWrVpF165d47zzzov58+cXbVM3z5uTgJa2q84ZO6O8vDw++tGPFo0nYlN4OvbYY+O4445rcF3Hjh1j0KBBERFx++23x5AhQ6JLly7RqlWrqKqqioceeqjoMblcLlavXh3//d//XXh9mwey+fPnx4UXXhj77LNPVFRUxMCBA+P+++8veo66OeTnP/95XHfddbHffvtF69atY8WKFbFx48aYMGFC9O/fPyorK6NLly5x/PHHx29/+9t6r7kx80htbW3ceeedMXDgwKisrIx99tknLr744li6dGnRdlmWxaRJk2L//feP1q1bx8c//vF45ZVXGvfm0+Ic8bSXe+yxx6Jfv35xzDHH7NDjqqur4+STT47jjz8+br/99mjdunVkWRann356PPvss3HRRRfFEUccEU8++WRcffXVMX/+/Ljjjjt2epxnnXVW9OnTJ26++eZ48cUX47777ovu3bvHLbfcUthm3Lhx8cADD8To0aNjyJAh8cwzz8TIkSN3ep8N+fznPx/9+/ePm266qVGTX51u3brF3XffHZdcckl85jOfKXxjs/npezU1NXHyySfHMcccE7fffntMnz49vvOd70Tfvn3jkksuadLXAdBSSkpK4rrrrosvfOEL2/0G+5lnnolTTz01qqqq4oYbboh8Ph9Tp06NT3ziE/Hcc8/F0UcfHWeeeWa8/vrr8bOf/SzuuOOO6Nq1a0REIfbviDfffDMiIrp06VJY9tprr8U555wTF198cXzxi1+MQw45JNasWRPDhg2L+fPnx8UXXxwHHnhgPP/88/GNb3wjFixYEHfeeWdEbPpL8qc//emYMWNGfPnLX47DDjssfvnLX8aYMWMaNZ4JEybE+PHjY8iQITFx4sQoLy+PP/3pT/HMM8/ESSedFHfeeWdcfvnl0bZt27j22msjImKfffbZ6vP96Ec/igsuuCA++tGPxs033xwLFy6Mu+66K2bOnBmzZs0qOuXEnATsCnblOWNnHH/88fHcc8/FvHnzCl9Qz5w5M8aNGxdHH3103HDDDbFs2bLo2LFjZFkWzz//fBx77LGFo4zuuuuuOP300+Pcc8+NDRs2xM9//vP4/Oc/H4899ljh954f//jHhef70pe+FBERffv2jYiIhQsXxsc+9rHI5XJx2WWXRbdu3eKJJ56Iiy66KFasWBFXXnll0XhvvPHGKC8vj6uuuirWr18f5eXlMX78+Lj55psL+1ixYkX85S9/iRdffLHoTJLGziMXX3xxYX664oorYu7cufH9738/Zs2aFTNnzoyysrKIiLj++utj0qRJcdppp8Vpp50WL774Ypx00kmxYcOGJH9WNLGMvdby5cuziMjOOOOMeuuWLl2aLV68uPDPmjVrCuvGjBmTRUR2zTXXFD3mkUceySIimzRpUtHyz33uc1kul8veeOONLMuybO7cuVlEZFOnTq2334jIbrjhhsLPN9xwQxYR2YUXXli03Wc+85msS5cuhZ9nz56dRUR26aWXFm03evToes+5PdOmTcsiInv22WfrjeOcc86pt/2wYcOyYcOG1Vs+ZsyYrFevXoWfFy9evNWx1L2nEydOLFp+5JFHZlVVVY0eO0BzqftcXLx4cYPrBw4cWPTZWPfZf9ttt2XV1dVZ//79s8GDB2e1tbUNPl9tbW3Wv3//7OSTTy5sk2VZtmbNmqxPnz7ZiBEjCstuu+22LCKyuXPnNmrsY8aMydq0aVOY4954443spptuynK5XHb44YcXtuvVq1cWEdlvfvObosffeOONWZs2bbLXX3+9aPk111yTlZSUZG+//XaWZf+eF2+99dbCNtXV1dkJJ5xQbx6se/115syZk+Xz+ewzn/lMVlNTU7Sfzd+PLd/nOs8++2zRXLZhw4ase/fu2aBBg7K1a9cWtnvssceyiMiuv/76ovfHnAQ0pd15zmjI1n6X2Z7HH388i4jsxz/+cZZlWbZgwYIsIrLf//732cqVK7OSkpLs8ccfz7Isy15++eUsIrJvfetbRa9ncxs2bMgGDRqUfeITnyha3qZNm2zMmDH19n/RRRdlPXr0yJYsWVK0/Oyzz846dOhQeP66OeSggw6qt8/BgwdnI0eO3ObrbOw88txzz2URkf3kJz8p2u43v/lN0fJFixZl5eXl2ciRI4v+fL/5zW9mEdHga2XX4lS7vdiKFSsiIhq81eXw4cOjW7duhX9+8IMf1Ntmy288f/3rX0dJSUlcccUVRcu/9rWvRZZl8cQTT+z0WL/85S8X/XzCCSfE+++/X3gNv/71ryMi6u17y2r/YW05jqbW0Ov8xz/+kXSfAM2t7hvsv/3tb1u9687s2bNjzpw5MXr06Hj//fdjyZIlsWTJkli9enV88pOfjD/84Q/1TmvbEatXry7Mcf369YtvfvObceyxx8Yvf/nLou369OkTJ598ctGyadOmxQknnBCdOnUqjGvJkiVx4oknRk1NTfzhD3+IiE1zU2lpadF8WVJSUnRtj6155JFHora2Nq6//vp619PYmQvs/uUvf4lFixbFpZdeWnTtp5EjR8ahhx4ajz/+eL3HmJOAXUFLzxlr1qwp+qyvu5PeqlWripZteWpYQ4YMGRL5fL5w7aa6I3o++tGPRtu2bePwww8vnG5X9+/Nr++0+TUGly5dGsuXL48TTjghXnzxxe3uO8uyePjhh2PUqFGRZVnR2E8++eRYvnx5vecZM2ZMvesaduzYMV555ZWYM2fOdve5vXlk2rRp0aFDhxgxYkTReKqqqqJt27bx7LPPRkTE9OnTY8OGDXH55ZcXzYFN/bse6TjVbi/Wrl27iNj0obmlH/7wh7Fy5cpYuHBh0QXu6pSWlsb+++9ftOytt96Knj17Fp63Tt2d4d56662dHuuBBx5Y9HOnTp0iYtMHbvv27eOtt96KfD5fOIy0ziGHHLLT+2zI5neZaGqVlZX1DvPt1KlToyYxgF3RtgLJueeeGzfeeGNMnDixwbvu1P2FdlunpS1fvrwwH+yoysrK+J//+Z+I2HSHuz59+tSb1yIa/tyfM2dOvPTSS1s9NWPRokURsWne69GjR70veBozN7355puRz+djwIAB2922Merm4Ib2feihhxZ+CapjTgKa2646Z9x6662FO1Nv7vLLLy/6IqFXr17bvVZUx44dY+DAgUVx6cgjjyzEnSFDhhStKy8vj6OPPrrw+MceeywmTZoUs2fPjvXr1xeWN+YLicWLF8eyZcvi3nvvjXvvvbfBbermrzoNzYETJ06MT3/603HwwQfHoEGD4pRTTonzzz+/3h3AGzOPzJkzJ5YvXx7du3ff5njq5rD+/fsXre/WrdtO/z2A5iU87cU6dOgQPXr0iJdffrneurprPm3tw7OiomKn72iwtQ/GbV2wtKSkpMHl2Q5cZ6kpNHQno1wu1+A4dvQCrFt7jQC7orqjZtauXdvg+jVr1mzzrmp132CPHTs2Hn300Xrr676Zvu222+KII45o8DkaOmK3sUpKSuLEE0/c7nYNfe7X1tbGiBEj4utf/3qDjzn44IN3ely7CnMS0JR25znjC1/4Qr27yo0YMSKuvvrqOOmkkwrLGnvH0+OPPz7uueeeWLZsWcycOTOGDBlSWDdkyJC4//77Y+PGjTFjxoyoqqoqvC/PPfdcnH766TF06NCYMmVK9OjRI8rKymLq1Knx05/+dLv7rXuPzjvvvK0Gui3jUUOvaejQofHmm2/Go48+Gk899VTcd999cccdd8Q999wT48aNK2zXmHmktrY2unfvHj/5yU8aXN9c194iPeFpLzdy5Mi477774s9//nNRTd8ZvXr1iunTp8fKlSuLjnp69dVXC+sj/n200pZ39fkwR0T16tUramtr48033yz6Nve1117b6edsrE6dOjV46sGWr2dnTo0A2FXVfaa/9tprRXc+jdj0C8Q777xT9Bfyhpx33nkxadKkmDBhQpx++ulF6+qOYG3fvv12A1Fzf7727ds3Vq1atd1x9erVK55++ulYtWpV0S88jZmb+vbtG7W1tfH3v/99q79ERTT+tW/+5/WJT3yiaN1rr71WWA+Qwu48Zxx00EFx0EEH1Vs+YMCARn2BsaXjjz8+7r777pg+fXrMmjWrcJe9iE3hae3atfH444/HP/7xj/jsZz9bWPfwww9HZWVlPPnkk1FRUVFYPnXq1Hr7aOg11t35u6amZqfGvbnOnTvHBRdcEBdccEGsWrUqhg4dGuPHjy8KT43Rt2/fmD59ehx33HHbDHd1///MmTOn6M9i8eLFjsTdTbjG017u61//erRu3TouvPDCWLhwYb31O3JE0WmnnRY1NTXx/e9/v2j5HXfcEblcLk499dSI2DQhdO3atXANjDpTpkzZiVewSd1zf/e73y1aXndnoZT69u0br776aixevLiw7G9/+1u926G2bt06IuoHN4Dd0Sc/+ckoLy+Pu+++u951M+69996orq4ufDZvTd032LNnz45f/epXReuqqqqib9++cfvttzd4Svjmn7lt2rSJiOb7fD3rrLPij3/8Yzz55JP11i1btiyqq6sjYtO8WF1dHXfffXdhfU1NTXzve9/b7j7OOOOMyOfzMXHixHrv7+Zzc5s2bRr1uo866qjo3r173HPPPUWnZzzxxBPxf//3f01+F1iAze3Nc8aW6o6emjx5cmzcuLHoiKfevXtHjx494tZbby3aNmLT68/lckVnVcybN6/B6141NDeUlJTEZz/72Xj44YcbPONl8/doW95///2in9u2bRv9+vUrmlsa66yzzoqampq48cYb662rrq4uvIYTTzwxysrK4nvf+17RHNgcv+vRNBzxtJfr379//PSnP41zzjknDjnkkDj33HNj8ODBkWVZzJ07N376059GPp9v8LoXWxo1alR8/OMfj2uvvTbmzZsXgwcPjqeeeioeffTRuPLKK4uuvzRu3Lj49re/HePGjYujjjoq/vCHP8Trr7++06/jiCOOiHPOOSemTJkSy5cvjyFDhsTTTz8db7zxxk4/Z2NdeOGFMXny5Dj55JPjoosuikWLFsU999wTAwcOLFz8PGLToaoDBgyIBx98MA4++ODo3LlzDBo0KAYNGpR8jABNrXv37nH99dfHddddF0OHDo3TTz89WrduHc8//3z87Gc/i5NOOilGjRq13eepu27H7Nmzi5bn8/m477774tRTT42BAwfGBRdcEPvtt1/Mnz8/nn322Wjfvn3hGk1VVVUREXHttdfG2WefHWVlZTFq1KjCLxdN7eqrr45f/epX8alPfSrGjh0bVVVVsXr16vjf//3feOihh2LevHnRtWvXGDVqVBx33HFxzTXXxLx582LAgAHxi1/8IpYvX77dffTr1y+uvfbauPHGG+OEE06IM888MyoqKuKFF16Inj17xs0331x47XfffXdMmjQp+vXrF927d693RFNERFlZWdxyyy1xwQUXxLBhw+Kcc86JhQsXxl133RW9e/eOr371q03+PgHU2ZvnjC0deOCBccABB8Qf//jH6N27d/Ts2bNo/ZAhQ+Lhhx+OXC4Xxx13XGH5yJEjY/LkyXHKKafE6NGjY9GiRfGDH/wg+vXrFy+99FLRc1RVVcX06dNj8uTJ0bNnz+jTp08cc8wx8e1vfzueffbZOOaYY+KLX/xiDBgwID744IN48cUXY/r06fHBBx9sd/wDBgyI4cOHR1VVVXTu3Dn+8pe/xEMPPRSXXXbZDr8Xw4YNi4svvjhuvvnmmD17dpx00klRVlYWc+bMiWnTpsVdd90Vn/vc56Jbt25x1VVXxc033xyf+tSn4rTTTotZs2bFE088EV27dt3h/dICWux+euxS3njjjeySSy7J+vXrl1VWVmatWrXKDj300OzLX/5yNnv27KJt625D3ZCVK1dmX/3qV7OePXtmZWVlWf/+/bPbbrut6LaXWbbpVqAXXXRR1qFDh6xdu3bZWWedlS1atCiLiOyGG24obLe1W69OnTq13m1Q165dm11xxRVZly5dsjZt2mSjRo3K3nnnnXrPuT3Tpk0rugX1tsZR54EHHsgOOuigrLy8PDviiCOyJ598MhszZkzWq1evou2ef/75rKqqKisvLy8a19be0y1vrw2wq3nggQeyj33sY1mbNm2yioqK7NBDD80mTJiQrVu3rmi7zW+NvaW6z/SGPmdnzZqVnXnmmVmXLl2yioqKrFevXtlZZ52VPf3000Xb3Xjjjdl+++2X5fP57d4me1vz2OZ69eq11VtGr1y5MvvGN76R9evXLysvL8+6du2aDRkyJLv99tuzDRs2FLZ7//33s/PPPz9r37591qFDh+z888/PZs2aVe9W3Fv7vL///vuzI488MquoqMg6deqUDRs2LPvtb39bWP/Pf/4zGzlyZNauXbssIgq3I6+7Ffbmc1mWZdmDDz5YeL7OnTtn5557bvbuu+826v0xJwEf1u44ZzRky8/wHXXOOedkEZGNHj263rrJkydnEZEddthh9db913/9V9a/f//Cezd16tQGP5tfffXVbOjQoVmrVq2yiMjGjBlTWLdw4cLsK1/5SnbAAQdkZWVl2b777pt98pOfzO69997CNnVzyLRp0+qNYdKkSdnRRx+ddezYsfA747e+9a2iuW9H55F77703q6qqylq1apW1a9cu+8hHPpJ9/etfz957773CNjU1NdmECROyHj16ZK1atcqGDx+evfzyy1mvXr2KXh+7plyWNfPVmQEAAADYK7jGEwAAAABJCE8AAAAAJCE8AQAAAJCE8AQAAABAEsITAAAAAEkITwAAAAAkITwBAAAAkERpYzcckf98ynEA7HV+WzutpYewSzHPADQt80x95hqAptWYucYRTwAAAAAkITwBAAAAkITwBAAAAEASwhMAAAAASQhPAAAAACQhPAEAAACQhPAEAAAAQBLCEwAAAABJCE8AAAAAJCE8AQAAAJCE8AQAAABAEsITAAAAAEkITwAAAAAkUdrSA9hrtW0TUZKPqKmJWLWmpUcDAAAA0OSEp5bQsX3kD+hZCE+17yyIWLaipUcFAAAA0KSEp+ZUUR7Rrk3k99s3IpfbtKykJPK99ova0pKIlasj1m9o2TECAAAANBHhqbmUl0Wuz/6Rq6ysvy6Xi/z+PSJbuy5i5arI3lvU/OMDAAAAaGLCUzPJ9Tmg4ei0+TatKiNaVUaUlkb2wTLXfgIAAAB2a+5q11zqTq1rzKadO0bugB4JBwMAAACQnvDUTLJ33tuh7XMVFZE7sGdE3h8RAAAAsHtSNZrLuvWRLd+xO9flOneMKC9LMx4AAACAxISn5lJTG9m8+ZG5ax0AAACwlxCemlOWRfbewsiWr2zc5stXRGzcmHhQAAAAAGm4q11zW74ysnw+cu3abPP6Tdmy5ZG99V5EljXj4AAAAACajvDUEpYuj9ra2oiSksiVlUZu326FVdnG6sj+uThi+QrRCQAAANitCU8t5V+n22URkS1ZutmKLKKmtkWGBAAAANCUhKddQU1NS48AAAAAoMm5uDgAAAAASQhPAAAAACQhPAEAAACQhPAEAAAAQBLCEwAAAABJCE8AAAAAJCE8AQAAAJCE8AQAAABAEsITAAAAAEkITwAAAAAkITwBAAAAkITwBAAAAEASwhMAAAAASQhPAAAAACRR2tID4MPJSvJR07Vt4eeS91dFrrq2BUcEAAAAsInwtBur7tkxNvbuGht7dy0sa/Ob/42SD1a34KgAAAAANhGedkO1bcqjpmu7WHvMQRGlJS09HAAAAIAGCU+7kSwXsWFAz6jev3PUdGlbb33pgmWRW72+BUYGAAAAUJ/wtJvYuF+nWHf0QZFVlkbkcvXW51avj1Yz5kRuY00LjA4AAACgPuFpF5X9qy1llWWxYeB+sbFX18gqtv7HVf7GQtEJAAAA2KUIT7uYms5tYmOfbrHxwM6bFuRzkVWUbeMBtVHxt3ei/PV/Ns8AAQAAABpJeNrFrD22X9R2aNWobUsXLIvSeUuifO6SxKMCAAAA2HHC0y4mv2Jto8JT6XvLotVM13QCAAAAdl3C0y6m4m/vRPX+nSPqXz88IiJy6zZGxSvzo3TeEtEJAAAA2KUJT7uY/Mq10faRF2PdfxwYWeuKonWl73wQZfOWRH7dxhYaHQAAAEDjCU+7mFwWkVu7IVrPfKOlhwIAAADwoeRbegAAAAAA7JmEJwAAAACSEJ4AAAAASEJ4AgAAACAJ4QkAAACAJIQnAAAAAJIQngAAAABIQngCAAAAIAnhCQAAAIAkhCcAAAAAkhCeAAAAAEhCeAIAAAAgCeEJAAAAgCSEJwAAAACSEJ4AAAAASEJ4AgAAACAJ4QkAAACAJIQnAAAAAJIQngAAAABIQngCAAAAIAnhCQAAAIAkhCcAAAAAkhCeAAAAAEhCeAIAAAAgCeEJAAAAgCSEJwAAAACSEJ4AAAAASEJ4AgAAACAJ4QkAAACAJIQnAAAAAJIQngAAAABIQngCAAAAIAnhCQAAAIAkhCcAAAAAkhCeAAAAAEhCeAIAAAAgCeEJAAAAgCSEJwAAAACSEJ4AAAAASEJ4AgAAACAJ4QkAAACAJIQnAAAAAJIQngAAAABIQngCAAAAIAnhCQAAAIAkhCcAAAAAkhCeAAAAAEhCeAIAAAAgCeEJAAAAgCSEJwAAAACSEJ4AAAAASEJ4AgAAACAJ4QkAAACAJIQnAAAAAJIQngAAAABIQngCAAAAIAnhCQAAAIAkhCcAAAAAkhCeAAAAAEhCeAIAAAAgCeEJAAAAgCSEJwAAAACSEJ4AAAAASEJ4AgAAACAJ4QkAAACAJIQnAAAAAJIQngAAAABIQngCAAAAIAnhCQAAAIAkhCcAAAAAkhCeAAAAAEhCeAIAAAAgCeEJAAAAgCSEJwAAAACSEJ4AAAAASEJ4AgAAACAJ4QkAAACAJIQnAAAAAJIQngAAAABIQngCAAAAIAnhCQAAAIAkhCcAAAAAkhCeAAAAAEhCeAIAAAAgCeEJAAAAgCSEJwAAAACSEJ4AAAAASEJ4AgAAACAJ4QkAAACAJEpbegBNaf0B7WJj11ZFy1q/vCTyG2tbaEQAAAAAe689Jjxl+VysPnKfWD24e9HyDT3aRqen5kauOmuhkQEAAADsnfaYU+3WH9AuVh/erd7y1VX7xOoj9mmBEQEAAADs3faY8JRVlETkcg2syUXW0GIAAAAAktojwlOWj1h6Wt+WHgYAAAAAm9kjwlNkEblqFxAHAAAA2JXsEeEpl0V0+vWbLT0MAAAAADazR4SniH8d8VTT8FFPWVnedZ4AAAAAmtkeE57K31kZbf62qMF1y4cfGLVtypp5RAAAAAB7tz0mPOWyiLYv/DNKVqyvvzLvcCcAAACA5rbHhKeIiPJFa6Lbz/4vSt9f29JDAQAAANjr7VHhKSKibNGa6Drt1egwfV7k11VHfl11tP3zgsivrW7poQEAAADsVUpbegAplC1eG6VL1ka7Py/YtKA2i1zWsmMCAAAA2NvskeEpYtM1n6JGbQIAAABoKXvcqXYAAAAA7BqEJwAAAACSEJ4AAAAASEJ4AgAAACAJ4QkAAACAJIQnAAAAAJIQngAAAABIQngCAAAAIAnhCQAAAIAkhCcAAAAAkhCeAAAAAEhCeAIAAAAgCeEJAAAAgCSEJwAAAACSEJ4AAAAASEJ4AgAAACAJ4QkAAACAJIQnAAAAAJIQngAAAABIQngCAAAAIAnhCQAAAIAkhCcAAAAAkhCeAAAAAEhCeAIAAAAgCeEJAAAAgCSEJwAAAACSEJ4AAAAASEJ4AgAAACAJ4QkAAACAJIQnAAAAAJIQngAAAABIQngCAAAAIAnhCQAAAIAkhCcAAAAAkhCeAAAAAEhCeAIAAAAgCeEJAAAAgCSEJwAAAACSEJ4AAAAASEJ4AgAAACAJ4QkAAACAJIQnAAAAAJIQngAAAABIQngCAAAAIAnhCQAAAIAkhCcAAAAAkhCeAAAAAEhCeAIAAAAgCeEJAAAAgCSEJwAAAACSEJ4AAAAASEJ4AgAAACAJ4QkAAACAJIQnAAAAAJIQngAAAABIQngCAAAAIAnhCQAAAIAkhCcAAAAAkhCeAAAAAEhCeAIAAAAgCeEJAAAAgCSEJwAAAACSEJ4AAAAASEJ4AgAAACAJ4QkAAACAJIQnAAAAAJIQngAAAABIQngCAAAAIAnhCQAAAIAkhCcAAAAAkhCeAAAAAEhCeAIAAAAgCeEJAAAAgCSEJwAAAACSEJ4AAAAASEJ4AgAAACAJ4QkAAACAJIQnAAAAAJIQngAAAABIQngCAAAAIAnhCQAAAIAkhCcAAAAAkhCeAAAAAEhCeAIAAAAgCeEJAAAAgCSEJwAAAACSEJ4AAAAASEJ4AgAAACAJ4QkAAACAJIQnAAAAAJIQngAAAABIQngCAAAAIAnhCQAAAIAkhCcAAAAAkiht6QE0mVa5yPUoiYiI/MjWkTu6YrsPqX1iTWT/b31ERGRvV0dUJx0hAAAAwF5llwlPG/atiJVDukTZP9dF++c/aNRjcsdVRO6Qsk3/vW9p5E9qtUP7LPlS+4gvRUQWUfuL1ZGtro1YVBu1j63Z0eEDAAAAsIUWCU8bu5XHgq/0jdqKf5/pl1XkI2qz2O87b2z7wa1yUfKldpGrqohc53xE2yY4WzAXkf9sm03/vT6LqM6i9jdrP/zzAgAAAOzFmjU8ZfmIpaftGyuP6RwbDig+Oqnsn+uix5R/RPmCdVt9fO64isgPbxX5ETt2ZNMOqchFtHPpKwAAAIAPq1nCU5aL2NijMpae1D1WDO0akc8V1uXX1UTZovWx791ztx2dPl4ZpV/r0DRHOG1PbvubAAAAALBtycPThh6VsWJI51h2yj6RlW0RjWqz6PqTd6LDc+9v/Qn2K4n8Ka2j5PNtIiqbqQhlzbMbAAAAgD1ZsvBUW5mPjftUxIJLDoqN+1bWW59fVxNdH3w32s/YRnQqiSid2ClyfctSDbO+9VnEitrm2x8AAADAHipJeMpKIhadf2CsPK5Lwxs05kiniMgNqYzc/s14Gaosoua+lVH7pAuLAwAAAHxYTVp1slzExn0rY+kp+8TKIZ0b3KZRRzrFZtd0qmjGCy59UBO1T4tOAAAAAE2hycLThh6VsXJI51h66j6RlTZ8AfA2f10a7V5YGu3+39JtPlfuhMoovaZj80anRTVRPX5pxAdOswMAAABoCk0SnmrLcvHeFX1jY4+Gr+VUunh97HP/W1G2cH2UrKnZ5nPlPlIWpV9vxiOd1mdR86OVUfu7dRELtj02AAAAABqvScJTrjaicu7qovBUtmBdtPvzB1G+YN32j3DKZ3HYhauipCKL3P6lke+xIVbVVMRba7dyjaimUndNp2mr0+4HAAAAYC/UNOGpJovuP347Oj3+z8Ky/LraKHt/w3YemEWvU9fGRy5dGe0Pqo5cPqtbEXPXdk0anrI/rouae1dG9nZ1sn0AAAAA7M2a7BpP+bW1UTF/XaO3b9erOvqcviYGjFsZJRVZ0br56zvGiysObKqhFVtUEzVPrInaB1dHrMm2vz0AAAAAO6VJ72rXWK17VMewH7wfHfptLFq+dklJLP6gbfx1xf5R0zMX0fA1yndKNr86ojqi5qZlkb22cfsPAAAAAOBDadbwlCvJ4rCxq6L3qLX1otPKt0vjuSs7x9K/l0Xkl0bJ1zpEfmTrD7fDLKL2odWRral1hBMAAABAM2vW8DT4/1sRA8at2uxaThHVa/Mxe3L7ePfZylj97r+GUxtR8/0VUfvI6ii5qmPk9iuJaNu4w58KRzZNXh6xsjayt6oj3KwOAAAAoNk1a3hqvW9NUXR695lW8faTrWLuo60iIle88dossjnVUX3xksgdXxH54a0if2KrBp83+9P6qH19Y0SWRe3PVkesdWQTAAAAQEtr1vA0+4728ff72xV+XrOgJDYs3/6RTNmM9VHz1w1R++CqKLmqY0RlLmq+szxide2m9YtrIlaITQAAAAC7kmYNT2sWlMaaBTv54M2OgAIAAABg19eE940DAAAAgH8TngAAAABIQngCAAAAIAnhCQAAAIAkhCcAAAAAkhCeAAAAAEhCeAIAAAAgCeEJAAAAgCSEJwAAAACSEJ4AAAAASEJ4AgAAACAJ4QkAAACAJIQnAAAAAJIQngAAAABIQngCAAAAIAnhCQAAAIAkhCcAAAAAkhCeAAAAAEhCeAIAAAAgCeEJAAAAgCSEJwAAAACSEJ4AAAAASEJ4AgAAACAJ4QkAAACAJIQnAAAAAJIQngAAAABIQngCAAAAIAnhCQAAAIAkhCcAAAAAkhCeAAAAAEhCeAIAAAAgCeEJAAAAgCSEJwAAAACSEJ4AAAAASEJ4AgAAACAJ4QkAAACAJIQnAAAAAJIQngAAAABIQngCAAAAIAnhCQAAAIAkhCcAAAAAkhCeAAAAAEhCeAIAAAAgCeEJAAAAgCSEJwAAAACSEJ4AAAAASEJ4AgAAACAJ4QkAAACAJIQnAAAAAJIQngAAAABIQngCAAAAIAnhCQAAAIAkhCcAAAAAkhCeAAAAAEhCeAIAAAAgiUaHpxM//0F0339DyrEAAAAAsAcpbeyGV9/1dsx5qXV8sPDfD3n0/q7x19+3i4hcirEBAAAAsBtrdHiKiOh/+JqinwcPWRUTx/WOv/6+fZMOCgAAAIDd34e6xlNlm9oYNfb9KCnLmmo8AAAAAOwhPvTFxY85cUWMu/a9KCkVnwAAAAD4tw8dnvIlWZxx0ZL4z8sWNsV4AAAAANhDfOjwFLEpPh170orYx13vAAAAAPiXJglPEREHH7EmrpnyVrTrWN1UTwkAAADAbqzJwlNExICq1XH5t99tyqcEAAAAYDfVpOEpchGDjlkd/zFsZUS42DgAAADA3qxpw1NEdNlnY1x/39z4j2GrmvqpAQAAANiNNHl4ioho1aY2rrrzbUc+AQAAAOzFkoSnCEc+AQAAAOztkoWniH8f+VQ1bEWUV9am3BUAAAAAu5ik4Sli05FPE/57blx919updwUAAADALiR5eIqIKCvPYuDRq+MjH3PaHQAAAMDeIpdlmat/AwAAANDkmuWIJwAAAAD2PsITAAAAAEkITwAAAAAkITwBAAAAkITwBAAAAEASwhMAAAAASQhPAAAAACQhPAEAAACQhPAEAAAAQBL/P+6cKf9LZxRqAAAAAElFTkSuQmCC",
            "text/plain": [
              "<Figure size 1500x500 with 3 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Pick an index of the validation image you want to visualize\n",
        "idx = 0\n",
        "\n",
        "# Ground truth mask\n",
        "gt_mask = validation_set_label[idx]\n",
        "\n",
        "# Predicted mask (raw UNet)\n",
        "pred_mask = output_raw  # make sure it's the corresponding output for idx\n",
        "\n",
        "# Predicted mask (watershed)\n",
        "pred_watershed = output_watershed  # make sure it's the corresponding output for idx\n",
        "\n",
        "# Display side by side\n",
        "fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n",
        "\n",
        "axs[0].imshow(gt_mask)\n",
        "axs[0].set_title('Ground Truth')\n",
        "axs[0].axis('off')\n",
        "\n",
        "axs[1].imshow(pred_mask)\n",
        "axs[1].set_title('UNet Prediction')\n",
        "axs[1].axis('off')\n",
        "\n",
        "axs[2].imshow(pred_watershed)\n",
        "axs[2].set_title('UNet + Watershed')\n",
        "axs[2].axis('off')\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6Wh8sj5c4NA1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Wh8sj5c4NA1",
        "outputId": "23811eba-f2d1-4151-f0b9-64b9ff1d0172"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "All files from 'output_model/' saved to Google Drive at /content/drive/My Drive/NuInsSeg/first_run_models\n"
          ]
        }
      ],
      "source": [
        "import shutil\n",
        "import os\n",
        "\n",
        "# Define your destination path in Google Drive\n",
        "drive_output_path = '/content/drive/My Drive/NuInsSeg/first_run_models'\n",
        "\n",
        "# Create the folder if it doesn't exist\n",
        "os.makedirs(drive_output_path, exist_ok=True)\n",
        "\n",
        "# Copy all files from output_model/ to Google Drive\n",
        "for filename in os.listdir('output_model/'):\n",
        "    src_path = os.path.join('output_model/', filename)\n",
        "    dst_path = os.path.join(drive_output_path, filename)\n",
        "    shutil.copy(src_path, dst_path)\n",
        "\n",
        "print(f\"All files from 'output_model/' saved to Google Drive at {drive_output_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "h1ha75xi57JX",
      "metadata": {
        "id": "h1ha75xi57JX"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}