{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nishalahmedpk/NuInsSeg/blob/main/shallow_unet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JfnZSShYqrKZ",
        "outputId": "18b83a03-2edb-4025-b373-e7011849e3a2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/ipateam/nuinsseg?dataset_version_number=5...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1.52G/1.52G [00:08<00:00, 181MB/s] "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data source import complete.\n"
          ]
        }
      ],
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "import kagglehub\n",
        "ipateam_nuinsseg_path = kagglehub.dataset_download('ipateam/nuinsseg')\n",
        "\n",
        "print('Data source import complete.')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dtiJOJGeq0fT",
        "outputId": "9ce966cc-fa16-4c78-d7d4-8267255a9627"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GIwlqUz0sIr1"
      },
      "outputs": [],
      "source": [
        "import shutil\n",
        "import os\n",
        "\n",
        "# Define your destination path in Google Drive\n",
        "drive_output_path = '/content/drive/My Drive/NuInsSeg/shallow_unet'\n",
        "os.makedirs(drive_output_path, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IueJWyC2qrKi"
      },
      "outputs": [],
      "source": [
        "# set all hyper parameters\n",
        "opts = {}\n",
        "opts['number_of_channel'] = 3\n",
        "opts['treshold'] = 0.5\n",
        "opts['epoch_num'] = 100\n",
        "opts['quick_run'] = 1\n",
        "opts['batch_size'] = 16\n",
        "opts['random_seed_num'] = 19\n",
        "opts['k_fold'] = 2\n",
        "opts['save_val_results'] = 1\n",
        "opts['init_LR'] = 0.001\n",
        "opts['LR_decay_factor'] = 0.5\n",
        "opts['LR_drop_after_nth_epoch'] = 20\n",
        "opts['crop_size'] = 512\n",
        "## output directories\n",
        "opts['result_save_path'] ='prediction_image/'\n",
        "opts['result_save_path'] = os.path.join(drive_output_path, opts['result_save_path'])\n",
        "opts['model_save_path'] ='output_model/'\n",
        "opts['model_save_path'] = os.path.join(drive_output_path, opts['model_save_path'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N6yh7-4C_8lz",
        "outputId": "679d5e7b-2b6f-4a97-cee7-f99e1f1023ed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: keras in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Collecting opencv-python\n",
            "  Using cached opencv_python-4.12.0.88-cp37-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (19 kB)\n",
            "Collecting tensorflow\n",
            "  Using cached tensorflow-2.20.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.5 kB)\n",
            "Collecting albumentations\n",
            "  Downloading albumentations-2.0.8-py3-none-any.whl.metadata (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.1/43.1 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: absl-py in /usr/local/lib/python3.12/dist-packages (from keras) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from keras) (2.0.2)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from keras) (14.1.0)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.12/dist-packages (from keras) (0.1.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.12/dist-packages (from keras) (3.14.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.12/dist-packages (from keras) (0.17.0)\n",
            "Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.12/dist-packages (from keras) (0.5.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from keras) (25.0)\n",
            "Collecting astunparse>=1.6.0 (from tensorflow)\n",
            "  Downloading astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
            "Collecting flatbuffers>=24.3.25 (from tensorflow)\n",
            "  Downloading flatbuffers-25.9.23-py2.py3-none-any.whl.metadata (875 bytes)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.6.0)\n",
            "Collecting google_pasta>=0.1.1 (from tensorflow)\n",
            "  Downloading google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
            "Collecting libclang>=13.0.0 (from tensorflow)\n",
            "  Downloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl.metadata (5.2 kB)\n",
            "Requirement already satisfied: opt_einsum>=2.3.2 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: protobuf>=5.28.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (6.32.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.32.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: typing_extensions>=3.6.6 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (4.15.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.17.3)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.75.1)\n",
            "Collecting tensorboard~=2.20.0 (from tensorflow)\n",
            "  Downloading tensorboard-2.20.0-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: scipy>=1.10.0 in /usr/local/lib/python3.12/dist-packages (from albumentations) (1.16.2)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.12/dist-packages (from albumentations) (6.0.3)\n",
            "Requirement already satisfied: pydantic>=2.9.2 in /usr/local/lib/python3.12/dist-packages (from albumentations) (2.11.9)\n",
            "Collecting albucore==0.0.24 (from albumentations)\n",
            "  Downloading albucore-0.0.24-py3-none-any.whl.metadata (5.3 kB)\n",
            "Collecting opencv-python-headless>=4.9.0.80 (from albumentations)\n",
            "  Downloading opencv_python_headless-4.12.0.88-cp37-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (19 kB)\n",
            "Collecting stringzilla>=3.10.4 (from albucore==0.0.24->albumentations)\n",
            "  Downloading stringzilla-4.2.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_28_x86_64.whl.metadata (105 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.1/105.1 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting simsimd>=5.9.2 (from albucore==0.0.24->albumentations)\n",
            "  Downloading simsimd-6.5.3-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (70 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.4/70.4 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting wheel<1.0,>=0.23.0 (from astunparse>=1.6.0->tensorflow)\n",
            "  Downloading wheel-0.45.1-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.9.2->albumentations) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.9.2->albumentations) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.9.2->albumentations) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.8.3)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/lib/python3/dist-packages (from tensorboard~=2.20.0->tensorflow) (3.3.6)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.20.0->tensorflow) (11.3.0)\n",
            "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard~=2.20.0->tensorflow)\n",
            "  Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl.metadata (1.1 kB)\n",
            "Collecting werkzeug>=1.0.1 (from tensorboard~=2.20.0->tensorflow)\n",
            "  Downloading werkzeug-3.1.3-py3-none-any.whl.metadata (3.7 kB)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->keras) (0.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.20.0->tensorflow) (3.0.3)\n",
            "Downloading opencv_python-4.12.0.88-cp37-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (67.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 MB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorflow-2.20.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (620.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m620.7/620.7 MB\u001b[0m \u001b[31m852.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading albumentations-2.0.8-py3-none-any.whl (369 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m369.4/369.4 kB\u001b[0m \u001b[31m34.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading albucore-0.0.24-py3-none-any.whl (15 kB)\n",
            "Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
            "Downloading flatbuffers-25.9.23-py2.py3-none-any.whl (30 kB)\n",
            "Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl (24.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.5/24.5 MB\u001b[0m \u001b[31m111.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opencv_python_headless-4.12.0.88-cp37-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (54.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.0/54.0 MB\u001b[0m \u001b[31m27.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard-2.20.0-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m151.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading simsimd-6.5.3-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m71.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading stringzilla-4.2.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_28_x86_64.whl (535 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m535.2/535.2 kB\u001b[0m \u001b[31m46.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m146.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading werkzeug-3.1.3-py3-none-any.whl (224 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading wheel-0.45.1-py3-none-any.whl (72 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: simsimd, libclang, flatbuffers, wheel, werkzeug, tensorboard-data-server, stringzilla, opencv-python-headless, opencv-python, google_pasta, tensorboard, astunparse, albucore, albumentations, tensorflow\n",
            "Successfully installed albucore-0.0.24 albumentations-2.0.8 astunparse-1.6.3 flatbuffers-25.9.23 google_pasta-0.2.0 libclang-18.1.1 opencv-python-4.12.0.88 opencv-python-headless-4.12.0.88 simsimd-6.5.3 stringzilla-4.2.0 tensorboard-2.20.0 tensorboard-data-server-0.7.2 tensorflow-2.20.0 werkzeug-3.1.3 wheel-0.45.1\n"
          ]
        }
      ],
      "source": [
        "!pip install keras opencv-python tensorflow albumentations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zMrLWMLYqrKn",
        "outputId": "f08e37ba-841b-4b07-e809-98926d84210e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jax/_src/cloud_tpu_init.py:82: UserWarning: Transparent hugepages are not enabled. TPU runtime startup and shutdown time should be significantly improved on TPU v5e and newer. If not already set, you may need to enable transparent hugepages in your VM image (sudo sh -c \"echo always > /sys/kernel/mm/transparent_hugepage/enabled\")\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "#import libs\n",
        "import os\n",
        "from glob import glob\n",
        "import numpy as np\n",
        "from sklearn.model_selection import KFold,StratifiedKFold\n",
        "import time\n",
        "import cv2\n",
        "import keras\n",
        "from keras.callbacks import CSVLogger, LearningRateScheduler, ModelCheckpoint\n",
        "from keras.layers import *\n",
        "from keras.models import Model, load_model\n",
        "from keras.optimizers import Adam # Changed from adam_v2\n",
        "from albumentations import *\n",
        "from keras import backend as K\n",
        "from skimage.feature import peak_local_max\n",
        "from scipy import ndimage as ndi\n",
        "from skimage.segmentation import watershed\n",
        "import skimage.morphology\n",
        "from skimage.io import imsave\n",
        "from skimage.morphology import remove_small_objects\n",
        "import tqdm\n",
        "from random import shuffle\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KBI7dngoqrKq"
      },
      "outputs": [],
      "source": [
        "## disabeling warning msg\n",
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "\n",
        "# 0 = all messages are logged (default behavior)\n",
        "# 1 = INFO messages are not printed\n",
        "# 2 = INFO and WARNING messages are not printed\n",
        "# 3 = INFO, WARNING, and ERROR messages are not printed\n",
        "import warnings\n",
        "warnings.simplefilter('ignore')\n",
        "import sys\n",
        "sys.stdout.flush() # resolving tqdm problem"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9MtsFi8vqrKs",
        "outputId": "53d9258f-9bca-4c14-8531-31dffed9932f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['human spleen',\n",
              " 'human melanoma',\n",
              " 'human cardia',\n",
              " 'mouse liver',\n",
              " 'human jejunum',\n",
              " 'human salivory gland',\n",
              " 'human tongue',\n",
              " 'human lung',\n",
              " 'human pylorus',\n",
              " 'mouse femur',\n",
              " 'human muscle',\n",
              " 'human bladder',\n",
              " 'human placenta',\n",
              " 'human umbilical cord',\n",
              " 'human epiglottis',\n",
              " 'human kidney',\n",
              " 'mouse spleen',\n",
              " 'human rectum',\n",
              " 'human brain',\n",
              " 'human tonsile',\n",
              " 'mouse kidney',\n",
              " 'human testis',\n",
              " 'human cerebellum',\n",
              " 'mouse muscle_tibia',\n",
              " 'human liver',\n",
              " 'mouse fat (white and brown)_subscapula',\n",
              " 'human pancreas',\n",
              " 'mouse heart',\n",
              " 'human peritoneum',\n",
              " 'human oesophagus',\n",
              " 'mouse thymus']"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "base_path = ipateam_nuinsseg_path\n",
        "organ_names = [ name for name in os.listdir(base_path) if os.path.isdir(os.path.join(base_path, name)) ]\n",
        "organ_names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mu_Th_-YqrKt"
      },
      "outputs": [],
      "source": [
        "# input and outpu paths\n",
        "img_path = glob(os.path.join(base_path, '*/tissue images/', '*.png'))\n",
        "binary_mask_path = glob(os.path.join(base_path, '*/mask binary/', '*.png'))\n",
        "distance_mask_path = glob(os.path.join(base_path, '*/distance maps/', '*.png'))\n",
        "label_mask_path = glob(os.path.join(base_path, '*/label masks modify/', '*.tif'))\n",
        "vague_mask_path =  glob(os.path.join(base_path, '*/vague areas/mask binary/', '*.png'))\n",
        "\n",
        "\n",
        "\n",
        "img_path.sort()\n",
        "binary_mask_path.sort()\n",
        "distance_mask_path.sort()\n",
        "label_mask_path.sort()\n",
        "vague_mask_path.sort()\n",
        "\n",
        "if not os.path.exists(opts['model_save_path']):\n",
        "    os.makedirs(opts['model_save_path'])\n",
        "if not os.path.exists(opts['result_save_path']):\n",
        "    os.makedirs(opts['result_save_path'])\n",
        "if not os.path.exists(os.path.join(opts['result_save_path'],'validation/unet')):\n",
        "    os.makedirs(os.path.join(opts['result_save_path'],'validation/unet'))\n",
        "if not os.path.exists(os.path.join(opts['result_save_path'],'validation/watershed_unet')):\n",
        "    os.makedirs(os.path.join(opts['result_save_path'],'validation/watershed_unet'))\n",
        "\n",
        "# create folders to save the best models and images (if needed) for each fold\n",
        "if not os.path.exists('prediction_image/'):\n",
        "    os.makedirs('prediction_image/')\n",
        "if not os.path.exists('output_model/'):\n",
        "    os.makedirs('output_model/')\n",
        "if not os.path.exists(opts['result_save_path']+ 'validation/unet'):\n",
        "    os.makedirs(opts['result_save_path'] + 'validation/unet')\n",
        "if not os.path.exists(opts['result_save_path']+ 'validation/watershed_unet'):\n",
        "    os.makedirs(opts['result_save_path'] + 'validation/watershed_unet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-hGgVQxfqrKv",
        "outputId": "4350f283-054d-45a9-cbba-2b717adc7714"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "image path: /root/.cache/kagglehub/datasets/ipateam/nuinsseg/versions/5/mouse fat (white and brown)_subscapula/tissue images/mouse_subscapula_08.png\n",
            " binary mask path: /root/.cache/kagglehub/datasets/ipateam/nuinsseg/versions/5/mouse fat (white and brown)_subscapula/mask binary/mouse_subscapula_08.png\n",
            " distance mask path: /root/.cache/kagglehub/datasets/ipateam/nuinsseg/versions/5/mouse fat (white and brown)_subscapula/distance maps/mouse_subscapula_08.png\n",
            " label mask path: /root/.cache/kagglehub/datasets/ipateam/nuinsseg/versions/5/mouse fat (white and brown)_subscapula/label masks modify/mouse_subscapula_08.tif\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#random check\n",
        "rand_num = np.random.randint(len(img_path))\n",
        "print('image path: {}\\n'.format(img_path[rand_num]),\n",
        "      'binary mask path: {}\\n'.format(binary_mask_path[rand_num]),\n",
        "      'distance mask path: {}\\n'.format(distance_mask_path[rand_num]),\n",
        "      'label mask path: {}\\n'.format(label_mask_path[rand_num]))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZiKltIkEqrKy"
      },
      "outputs": [],
      "source": [
        "# define loss function\n",
        "def dice_coef(y_true, y_pred):\n",
        "    smooth = 1.\n",
        "    y_true_f = keras.layers.Flatten()(y_true)\n",
        "    y_pred_f = keras.layers.Flatten()(y_pred)\n",
        "    intersection = keras.ops.sum(y_true_f * y_pred_f)\n",
        "    return (2. * intersection + smooth) / (keras.ops.sum(y_true_f) + keras.ops.sum(y_pred_f) + smooth)\n",
        "\n",
        "def dice_loss(y_true, y_pred):\n",
        "    return 1 - dice_coef(y_true, y_pred)\n",
        "#####################################################################################\n",
        "# Combination of Dice and binary cross entophy loss function that is used in this baseline segmentation\n",
        "def bce_dice_loss(y_true, y_pred):\n",
        "    return 0.5 * keras.losses.binary_crossentropy(y_true, y_pred) - dice_coef(y_true, y_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O7R4Ui-FqrK1"
      },
      "outputs": [],
      "source": [
        "# learning rate scheduler\n",
        "def step_decay_schedule(initial_lr=1e-3, decay_factor=0.75, epochs_drop=1000):\n",
        "    '''\n",
        "    Wrapper function to create a LearningRateScheduler with step decay schedule.\n",
        "    '''\n",
        "    def schedule(epoch):\n",
        "        return initial_lr * (decay_factor ** np.floor(epoch/epochs_drop))\n",
        "\n",
        "    return LearningRateScheduler(schedule, verbose = 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VXNhIpaaqrK4"
      },
      "outputs": [],
      "source": [
        "# U-net models\n",
        "#############################################################################################################\n",
        "def shallow_unet( IMG_CHANNELS, LearnRate):\n",
        "    inputs = Input((None, None, IMG_CHANNELS))\n",
        "    #s = Lambda(lambda x: x / 255) (inputs)\n",
        "    c1 = Conv2D(16, (3, 3), activation='relu', kernel_initializer='glorot_uniform', padding='same') (inputs)\n",
        "    c1 = Dropout(0.1) (c1)\n",
        "    c1 = Conv2D(16, (3, 3), activation='relu', kernel_initializer='glorot_uniform', padding='same') (c1)\n",
        "    p1 = MaxPooling2D((2, 2)) (c1)\n",
        "\n",
        "    c2 = Conv2D(32, (3, 3), activation='relu', kernel_initializer='glorot_uniform', padding='same') (p1)\n",
        "    c2 = Dropout(0.1) (c2)\n",
        "    c2 = Conv2D(32, (3, 3), activation='relu', kernel_initializer='glorot_uniform', padding='same') (c2)\n",
        "    p2 = MaxPooling2D((2, 2)) (c2)\n",
        "\n",
        "    c3 = Conv2D(64, (3, 3), activation='relu', kernel_initializer='glorot_uniform', padding='same') (p2)\n",
        "    c3 = Dropout(0.1) (c3)\n",
        "    c3 = Conv2D(64, (3, 3), activation='relu', kernel_initializer='glorot_uniform', padding='same') (c3)\n",
        "    p3 = MaxPooling2D((2, 2)) (c3)\n",
        "\n",
        "    c4 = Conv2D(128, (3, 3), activation='relu', kernel_initializer='glorot_uniform', padding='same') (p3)\n",
        "    c4 = Dropout(0.1) (c4)\n",
        "    c4 = Conv2D(128, (3, 3), activation='relu', kernel_initializer='glorot_uniform', padding='same') (c4)\n",
        "    p4 = MaxPooling2D(pool_size=(2, 2)) (c4)\n",
        "\n",
        "    c5 = Conv2D(256, (3, 3), activation='relu', kernel_initializer='glorot_uniform', padding='same') (p4)\n",
        "    c5 = Dropout(0.1) (c5)\n",
        "    c5 = Conv2D(256, (3, 3), activation='relu', kernel_initializer='glorot_uniform', padding='same') (c5)\n",
        "\n",
        "    u6 = Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same') (c5)\n",
        "    u6 = concatenate([u6, c4])\n",
        "    c6 = Conv2D(128, (3, 3), activation='relu', kernel_initializer='glorot_uniform', padding='same') (u6)\n",
        "    c6 = Dropout(0.1) (c6)\n",
        "    c6 = Conv2D(128, (3, 3), activation='relu', kernel_initializer='glorot_uniform', padding='same') (c6)\n",
        "\n",
        "    u7 = Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same') (c6)\n",
        "    u7 = concatenate([u7, c3])\n",
        "    c7 = Conv2D(64, (3, 3), activation='relu', kernel_initializer='glorot_uniform', padding='same') (u7)\n",
        "    c7 = Dropout(0.1) (c7)\n",
        "    c7 = Conv2D(64, (3, 3), activation='relu', kernel_initializer='glorot_uniform', padding='same') (c7)\n",
        "\n",
        "    u8 = Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same') (c7)\n",
        "    u8 = concatenate([u8, c2])\n",
        "    c8 = Conv2D(32, (3, 3), activation='relu', kernel_initializer='glorot_uniform', padding='same') (u8)\n",
        "    c8 = Dropout(0.1) (c8)\n",
        "    c8 = Conv2D(32, (3, 3), activation='relu', kernel_initializer='glorot_uniform', padding='same') (c8)\n",
        "\n",
        "    u9 = Conv2DTranspose(16, (2, 2), strides=(2, 2), padding='same') (c8)\n",
        "    u9 = concatenate([u9, c1], axis=3)\n",
        "    c9 = Conv2D(16, (3, 3), activation='relu', kernel_initializer='glorot_uniform', padding='same') (u9)\n",
        "    c9 = Dropout(0.1) (c9)\n",
        "    c9 = Conv2D(16, (3, 3), activation='relu', kernel_initializer='glorot_uniform', padding='same') (c9)\n",
        "\n",
        "    outputs = Conv2D(1, (1, 1), activation='sigmoid') (c9) # for binary\n",
        "\n",
        "    model = Model(inputs=[inputs], outputs=[outputs])\n",
        "    model.compile(optimizer = Adam(learning_rate= LearnRate), loss= bce_dice_loss , metrics=[dice_coef])\n",
        "    #model.summary()\n",
        "    return model\n",
        "#############################################################################################################\n",
        "def deep_unet(IMG_CHANNELS, LearnRate):\n",
        "    # Build U-Net model\n",
        "    inputs = Input((None, None, IMG_CHANNELS))\n",
        "    #s = Lambda(lambda x: x / 255) (inputs)\n",
        "    c1 = Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (inputs)\n",
        "    c1 = Dropout(0.1) (c1)\n",
        "    c1 = Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (c1)\n",
        "    p1 = MaxPooling2D((2, 2)) (c1)\n",
        "\n",
        "    c2 = Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (p1)\n",
        "    c2 = Dropout(0.1) (c2)\n",
        "    c2 = Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (c2)\n",
        "    p2 = MaxPooling2D((2, 2)) (c2)\n",
        "\n",
        "    c3 = Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (p2)\n",
        "    c3 = Dropout(0.1) (c3)\n",
        "    c3 = Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (c3)\n",
        "    p3 = MaxPooling2D((2, 2)) (c3)\n",
        "\n",
        "    c4 = Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (p3)\n",
        "    c4 = Dropout(0.1) (c4)\n",
        "    c4 = Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (c4)\n",
        "    p4 = MaxPooling2D(pool_size=(2, 2)) (c4)\n",
        "\n",
        "\n",
        "    c4_new = Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (p4)\n",
        "    c4_new = Dropout(0.1) (c4_new)\n",
        "    c4_new = Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (c4_new)\n",
        "    p4_new = MaxPooling2D(pool_size=(2, 2)) (c4_new)\n",
        "\n",
        "    c5 = Conv2D(512, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (p4_new)\n",
        "    c5 = Dropout(0.1) (c5)\n",
        "    c5 = Conv2D(512, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (c5)\n",
        "\n",
        "\n",
        "    u6_new = Conv2DTranspose(256, (2, 2), strides=(2, 2), padding='same') (c5)\n",
        "    u6_new = concatenate([u6_new, c4_new])\n",
        "    c6_new = Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (u6_new)\n",
        "    c6_new = Dropout(0.1) (c6_new)\n",
        "    c6_new = Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (c6_new)\n",
        "\n",
        "    u6 = Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same') (c6_new)\n",
        "    u6 = concatenate([u6, c4])\n",
        "    c6 = Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (u6)\n",
        "    c6 = Dropout(0.1) (c6)\n",
        "    c6 = Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (c6)\n",
        "\n",
        "    u7 = Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same') (c6)\n",
        "    u7 = concatenate([u7, c3])\n",
        "    c7 = Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (u7)\n",
        "    c7 = Dropout(0.1) (c7)\n",
        "    c7 = Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (c7)\n",
        "\n",
        "    u8 = Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same') (c7)\n",
        "    u8 = concatenate([u8, c2])\n",
        "    c8 = Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (u8)\n",
        "    c8 = Dropout(0.1) (c8)\n",
        "    c8 = Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (c8)\n",
        "\n",
        "    u9 = Conv2DTranspose(16, (2, 2), strides=(2, 2), padding='same') (c8)\n",
        "    u9 = concatenate([u9, c1], axis=3)\n",
        "    c9 = Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (u9)\n",
        "    c9 = Dropout(0.1) (c9)\n",
        "    c9 = Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (c9)\n",
        "\n",
        "    outputs = Conv2D(1, (1, 1), activation='sigmoid') (c9)\n",
        "\n",
        "    model_deep = Model(inputs=[inputs], outputs=[outputs])\n",
        "    model_deep.compile(optimizer = Adam(learning_rate=LearnRate), loss= bce_dice_loss , metrics=[ dice_coef])\n",
        "    #model_deeper.summary()\n",
        "    return model_deep"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XcAIUR1xqrK7"
      },
      "outputs": [],
      "source": [
        "# augmentation function\n",
        "def albumentation_aug(p=1.0, crop_size_row = 448, crop_size_col = 448 ):\n",
        "    return Compose([\n",
        "        RandomCrop(crop_size_row, crop_size_col, always_apply=True, p=1),\n",
        "        CLAHE(clip_limit=4.0, tile_grid_size=(8, 8), always_apply=False, p=0.5),\n",
        "        RandomBrightnessContrast(brightness_limit=0.15, contrast_limit=0.15, brightness_by_max=True, p=0.4),\n",
        "        HueSaturationValue(hue_shift_limit=20, sat_shift_limit=20, val_shift_limit=20, p=0.1),\n",
        "        HorizontalFlip(always_apply=False, p=0.5),\n",
        "        VerticalFlip(always_apply=False, p=0.5),\n",
        "        RandomRotate90(p=0.5),\n",
        "        ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.1, rotate_limit=20, interpolation=1,\n",
        "                         border_mode=4, always_apply=False, p=0.1),\n",
        "\n",
        "    ], p=p)\n",
        "# last  p has the second proiroty comapred to the p inside each argument\n",
        "#(e.g. HorizontalFlip(always_apply=False, p=0.5) )\n",
        "#############################################################################################################\n",
        "# def albumentation_aug_light(p=1.0, crop_size_row = 448, crop_size_col = 448):\n",
        "#     return Compose([\n",
        "#         RandomCrop(crop_size_row, crop_size_col, always_apply=True, p=1.0),\n",
        "#         HorizontalFlip(always_apply=False, p=0.5),\n",
        "#         VerticalFlip(always_apply=False, p=0.5),\n",
        "#         RandomRotate90(always_apply=False, p=0.5),\n",
        "#         ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.1, rotate_limit=20, interpolation=1,\n",
        "#                          border_mode=4 , always_apply=False, p=0.1),\n",
        "#     ], p=p, additional_targets={'mask1': 'mask','mask2': 'mask'})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ychvC6HQqrK9"
      },
      "outputs": [],
      "source": [
        "# evaluation index (from: https://github.com/vqdang/hover_net/blob/master/src/metrics/stats_utils.py)\n",
        "#############################################################################################################\n",
        "\n",
        "def get_fast_aji(true, pred):\n",
        "    \"\"\"AJI version distributed by MoNuSeg, has no permutation problem but suffered from\n",
        "    over-penalisation similar to DICE2.\n",
        "    Fast computation requires instance IDs are in contiguous orderding i.e [1, 2, 3, 4]\n",
        "    not [2, 3, 6, 10]. Please call `remap_label` before hand and `by_size` flag has no\n",
        "    effect on the result.\n",
        "    \"\"\"\n",
        "    true = np.copy(true)  # ? do we need this\n",
        "    pred = np.copy(pred)\n",
        "    true_id_list = list(np.unique(true))\n",
        "    pred_id_list = list(np.unique(pred))\n",
        "    #print(len(pred_id_list))\n",
        "    if len(pred_id_list) == 1:\n",
        "        return 0\n",
        "\n",
        "    true_masks = [None,]\n",
        "    for t in true_id_list[1:]:\n",
        "        t_mask = np.array(true == t, np.uint8)\n",
        "        true_masks.append(t_mask)\n",
        "\n",
        "    pred_masks = [None,]\n",
        "    for p in pred_id_list[1:]:\n",
        "        p_mask = np.array(pred == p, np.uint8)\n",
        "        pred_masks.append(p_mask)\n",
        "\n",
        "    # prefill with value\n",
        "    pairwise_inter = np.zeros(\n",
        "        [len(true_id_list) - 1, len(pred_id_list) - 1], dtype=np.float64\n",
        "    )\n",
        "    pairwise_union = np.zeros(\n",
        "        [len(true_id_list) - 1, len(pred_id_list) - 1], dtype=np.float64\n",
        "    )\n",
        "\n",
        "    # caching pairwise\n",
        "    for true_id in true_id_list[1:]:  # 0-th is background\n",
        "        t_mask = true_masks[true_id]\n",
        "        pred_true_overlap = pred[t_mask > 0]\n",
        "        pred_true_overlap_id = np.unique(pred_true_overlap)\n",
        "        pred_true_overlap_id = list(pred_true_overlap_id)\n",
        "        for pred_id in pred_true_overlap_id:\n",
        "            if pred_id == 0:  # ignore\n",
        "                continue  # overlaping background\n",
        "            p_mask = pred_masks[pred_id]\n",
        "            total = (t_mask + p_mask).sum()\n",
        "            inter = (t_mask * p_mask).sum()\n",
        "            pairwise_inter[true_id - 1, pred_id - 1] = inter\n",
        "            pairwise_union[true_id - 1, pred_id - 1] = total - inter\n",
        "\n",
        "    pairwise_iou = pairwise_inter / (pairwise_union + 1.0e-6)\n",
        "    # pair of pred that give highest iou for each true, dont care\n",
        "    # about reusing pred instance multiple times\n",
        "    paired_pred = np.argmax(pairwise_iou, axis=1)\n",
        "    pairwise_iou = np.max(pairwise_iou, axis=1)\n",
        "    # exlude those dont have intersection\n",
        "    paired_true = np.nonzero(pairwise_iou > 0.0)[0]\n",
        "    paired_pred = paired_pred[paired_true]\n",
        "    # print(paired_true.shape, paired_pred.shape)\n",
        "    overall_inter = (pairwise_inter[paired_true, paired_pred]).sum()\n",
        "    overall_union = (pairwise_union[paired_true, paired_pred]).sum()\n",
        "\n",
        "    paired_true = list(paired_true + 1)  # index to instance ID\n",
        "    paired_pred = list(paired_pred + 1)\n",
        "    # add all unpaired GT and Prediction into the union\n",
        "    unpaired_true = np.array(\n",
        "        [idx for idx in true_id_list[1:] if idx not in paired_true]\n",
        "    )\n",
        "    unpaired_pred = np.array(\n",
        "        [idx for idx in pred_id_list[1:] if idx not in paired_pred]\n",
        "    )\n",
        "    for true_id in unpaired_true:\n",
        "        overall_union += true_masks[true_id].sum()\n",
        "    for pred_id in unpaired_pred:\n",
        "        overall_union += pred_masks[pred_id].sum()\n",
        "\n",
        "    aji_score = overall_inter / overall_union\n",
        "    #print(aji_score)\n",
        "    return aji_score\n",
        "\n",
        "#############################################################################################################\n",
        "def get_fast_pq(true, pred, match_iou=0.5):\n",
        "    \"\"\"`match_iou` is the IoU threshold level to determine the pairing between\n",
        "    GT instances `p` and prediction instances `g`. `p` and `g` is a pair\n",
        "    if IoU > `match_iou`. However, pair of `p` and `g` must be unique\n",
        "    (1 prediction instance to 1 GT instance mapping).\n",
        "    If `match_iou` < 0.5, Munkres assignment (solving minimum weight matching\n",
        "    in bipartite graphs) is caculated to find the maximal amount of unique pairing.\n",
        "    If `match_iou` >= 0.5, all IoU(p,g) > 0.5 pairing is proven to be unique and\n",
        "    the number of pairs is also maximal.\n",
        "\n",
        "    Fast computation requires instance IDs are in contiguous orderding\n",
        "    i.e [1, 2, 3, 4] not [2, 3, 6, 10]. Please call `remap_label` beforehand\n",
        "    and `by_size` flag has no effect on the result.\n",
        "    Returns:\n",
        "        [dq, sq, pq]: measurement statistic\n",
        "        [paired_true, paired_pred, unpaired_true, unpaired_pred]:\n",
        "                      pairing information to perform measurement\n",
        "\n",
        "    \"\"\"\n",
        "    assert match_iou >= 0.0, \"Cant' be negative\"\n",
        "\n",
        "    true = np.copy(true)\n",
        "    pred = np.copy(pred)\n",
        "    true_id_list = list(np.unique(true))\n",
        "    pred_id_list = list(np.unique(pred))\n",
        "\n",
        "    if len(pred_id_list) == 1:\n",
        "        return [0, 0, 0], [0,0, 0, 0]\n",
        "\n",
        "    true_masks = [\n",
        "        None,\n",
        "    ]\n",
        "    for t in true_id_list[1:]:\n",
        "        t_mask = np.array(true == t, np.uint8)\n",
        "        true_masks.append(t_mask)\n",
        "\n",
        "    pred_masks = [\n",
        "        None,\n",
        "    ]\n",
        "    for p in pred_id_list[1:]:\n",
        "        p_mask = np.array(pred == p, np.uint8)\n",
        "        pred_masks.append(p_mask)\n",
        "\n",
        "    # prefill with value\n",
        "    pairwise_iou = np.zeros(\n",
        "        [len(true_id_list) - 1, len(pred_id_list) - 1], dtype=np.float64\n",
        "    )\n",
        "\n",
        "    # caching pairwise iou\n",
        "    for true_id in true_id_list[1:]:  # 0-th is background\n",
        "        t_mask = true_masks[true_id]\n",
        "        pred_true_overlap = pred[t_mask > 0]\n",
        "        pred_true_overlap_id = np.unique(pred_true_overlap)\n",
        "        pred_true_overlap_id = list(pred_true_overlap_id)\n",
        "        for pred_id in pred_true_overlap_id:\n",
        "            if pred_id == 0:  # ignore\n",
        "                continue  # overlaping background\n",
        "            p_mask = pred_masks[pred_id]\n",
        "            total = (t_mask + p_mask).sum()\n",
        "            inter = (t_mask * p_mask).sum()\n",
        "            iou = inter / (total - inter)\n",
        "            pairwise_iou[true_id - 1, pred_id - 1] = iou\n",
        "    #\n",
        "    if match_iou >= 0.5:\n",
        "        paired_iou = pairwise_iou[pairwise_iou > match_iou]\n",
        "        pairwise_iou[pairwise_iou <= match_iou] = 0.0\n",
        "        paired_true, paired_pred = np.nonzero(pairwise_iou)\n",
        "        paired_iou = pairwise_iou[paired_true, paired_pred]\n",
        "        paired_true += 1  # index is instance id - 1\n",
        "        paired_pred += 1  # hence return back to original\n",
        "    else:  # * Exhaustive maximal unique pairing\n",
        "        #### Munkres pairing with scipy library\n",
        "        # the algorithm return (row indices, matched column indices)\n",
        "        # if there is multiple same cost in a row, index of first occurence\n",
        "        # is return, thus the unique pairing is ensure\n",
        "        # inverse pair to get high IoU as minimum\n",
        "        paired_true, paired_pred = linear_sum_assignment(-pairwise_iou)\n",
        "        ### extract the paired cost and remove invalid pair\n",
        "        paired_iou = pairwise_iou[paired_true, paired_pred]\n",
        "\n",
        "        # now select those above threshold level\n",
        "        # paired with iou = 0.0 i.e no intersection => FP or FN\n",
        "        paired_true = list(paired_true[paired_iou > match_iou] + 1)\n",
        "        paired_pred = list(paired_pred[paired_iou > match_iou] + 1)\n",
        "        paired_iou = paired_iou[paired_iou > match_iou]\n",
        "\n",
        "    # get the actual FP and FN\n",
        "    unpaired_true = [idx for idx in true_id_list[1:] if idx not in paired_true]\n",
        "    unpaired_pred = [idx for idx in pred_id_list[1:] if idx not in paired_pred]\n",
        "    # print(paired_iou.shape, paired_true.shape, len(unpaired_true), len(unpaired_pred))\n",
        "\n",
        "    #\n",
        "    tp = len(paired_true)\n",
        "    fp = len(unpaired_pred)\n",
        "    fn = len(unpaired_true)\n",
        "    # get the F1-score i.e DQ\n",
        "    dq = tp / (tp + 0.5 * fp + 0.5 * fn)\n",
        "    # get the SQ, no paired has 0 iou so not impact\n",
        "    sq = paired_iou.sum() / (tp + 1.0e-6)\n",
        "\n",
        "    return [dq, sq, dq * sq], [paired_true, paired_pred, unpaired_true, unpaired_pred]\n",
        "\n",
        "\n",
        "#############################################################################################################\n",
        "def get_dice_1(true, pred):\n",
        "    \"\"\"Traditional dice.\"\"\"\n",
        "    # cast to binary 1st\n",
        "    true = np.copy(true)\n",
        "    pred = np.copy(pred)\n",
        "    true[true > 0] = 1\n",
        "    pred[pred > 0] = 1\n",
        "    inter = true * pred\n",
        "    denom = true + pred\n",
        "    dice_score = 2.0 * np.sum(inter) / (np.sum(denom) + 0.0001)\n",
        "    if np.sum(inter)==0 and np.sum(denom)==0:\n",
        "        dice_score = 1 # to handel cases without any nuclei\n",
        "    #print(dice_score)\n",
        "    return dice_score\n",
        "#############################################################################################################\n",
        "def remap_label(pred, by_size=False):\n",
        "    \"\"\"Rename all instance id so that the id is contiguous i.e [0, 1, 2, 3]\n",
        "    not [0, 2, 4, 6]. The ordering of instances (which one comes first)\n",
        "    is preserved unless by_size=True, then the instances will be reordered\n",
        "    so that bigger nucler has smaller ID.\n",
        "    Args:\n",
        "        pred    : the 2d array contain instances where each instances is marked\n",
        "                  by non-zero integer\n",
        "        by_size : renaming with larger nuclei has smaller id (on-top)\n",
        "    \"\"\"\n",
        "    pred_id = list(np.unique(pred))\n",
        "    pred_id.remove(0)\n",
        "    if len(pred_id) == 0:\n",
        "        return pred  # no label\n",
        "    if by_size:\n",
        "        pred_size = []\n",
        "        for inst_id in pred_id:\n",
        "            size = (pred == inst_id).sum()\n",
        "            pred_size.append(size)\n",
        "        # sort the id by size in descending order\n",
        "        pair_list = zip(pred_id, pred_size)\n",
        "        pair_list = sorted(pair_list, key=lambda x: x[1], reverse=True)\n",
        "        pred_id, pred_size = zip(*pair_list)\n",
        "\n",
        "    new_pred = np.zeros(pred.shape, np.int32)\n",
        "    for idx, inst_id in enumerate(pred_id):\n",
        "        new_pred[pred == inst_id] = idx + 1\n",
        "    return new_pred\n",
        "\n",
        "\n",
        "#############################################################################################################\n",
        "def pair_coordinates(setA, setB, radius):\n",
        "    \"\"\"Use the Munkres or Kuhn-Munkres algorithm to find the most optimal\n",
        "    unique pairing (largest possible match) when pairing points in set B\n",
        "    against points in set A, using distance as cost function.\n",
        "    Args:\n",
        "        setA, setB: np.array (float32) of size Nx2 contains the of XY coordinate\n",
        "                    of N different points\n",
        "        radius: valid area around a point in setA to consider\n",
        "                a given coordinate in setB a candidate for match\n",
        "    Return:\n",
        "        pairing: pairing is an array of indices\n",
        "        where point at index pairing[0] in set A paired with point\n",
        "        in set B at index pairing[1]\n",
        "        unparedA, unpairedB: remaining poitn in set A and set B unpaired\n",
        "    \"\"\"\n",
        "    # * Euclidean distance as the cost matrix\n",
        "    pair_distance = scipy.spatial.distance.cdist(setA, setB, metric='euclidean')\n",
        "\n",
        "    # * Munkres pairing with scipy library\n",
        "    # the algorithm return (row indices, matched column indices)\n",
        "    # if there is multiple same cost in a row, index of first occurence\n",
        "    # is return, thus the unique pairing is ensured\n",
        "    indicesA, paired_indicesB = linear_sum_assignment(pair_distance)\n",
        "\n",
        "    # extract the paired cost and remove instances\n",
        "    # outside of designated radius\n",
        "    pair_cost = pair_distance[indicesA, paired_indicesB]\n",
        "\n",
        "    pairedA = indicesA[pair_cost <= radius]\n",
        "    pairedB = paired_indicesB[pair_cost <= radius]\n",
        "\n",
        "    pairing = np.concatenate([pairedA[:,None], pairedB[:,None]], axis=-1)\n",
        "    unpairedA = np.delete(np.arange(setA.shape[0]), pairedA)\n",
        "    unpairedB = np.delete(np.arange(setB.shape[0]), pairedB)\n",
        "    return pairing, unpairedA, unpairedB\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wLxYh-GtqrLB"
      },
      "outputs": [],
      "source": [
        "# data generator related functions\n",
        "def get_id_from_file_path(file_path, indicator):\n",
        "    return file_path.split(os.path.sep)[-1].replace(indicator, '')\n",
        "#############################################################################################################\n",
        "def chunker(seq, seq2, size):\n",
        "    return ([seq[pos:pos + size], seq2[pos:pos + size]] for pos in range(0, len(seq), size))\n",
        "#############################################################################################################\n",
        "def data_gen(list_files, list_files2, batch_size, p , size_row, size_col, distance_unet_flag = 0,\n",
        "             augment= False, BACKBONE_model = None, use_pretrain_flag = 1):\n",
        "    crop_size_row = size_row\n",
        "    crop_size_col = size_col\n",
        "    aug = albumentation_aug(p, crop_size_row, crop_size_col)\n",
        "\n",
        "    while True:\n",
        "        for batch in chunker(list_files,list_files2, batch_size):\n",
        "            X = []\n",
        "            Y = []\n",
        "\n",
        "            for count in range(len(batch[0])):\n",
        "                x = cv2.imread(batch[0][count])\n",
        "                x = cv2.cvtColor(x, cv2.COLOR_BGR2RGB)\n",
        "                x_mask = cv2.imread(batch[1][count], cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "                x_mask_temp = np.zeros((x_mask.shape[0], x_mask.shape[1]))\n",
        "                x_mask_temp[x_mask == 255] = 1\n",
        "\n",
        "\n",
        "                if distance_unet_flag == False:\n",
        "                    if augment:\n",
        "                        augmented = aug(image= x, mask= x_mask_temp)\n",
        "                        x = augmented['image']\n",
        "                        if use_pretrain_flag == 1:\n",
        "                            x = preprocess_input(x)\n",
        "                        x_mask_temp = augmented['mask']\n",
        "                        x = x/255\n",
        "                    else:\n",
        "                        x = x/255\n",
        "                    X.append(x)\n",
        "                    Y.append(x_mask_temp)\n",
        "                else:\n",
        "                    if augment:\n",
        "                        augmented = aug(image=x, mask=x_mask)\n",
        "                        x = augmented['image']\n",
        "                        if use_pretrain_flag == 1:\n",
        "                            x = preprocess_input(x)\n",
        "                        x_mask = augmented['mask']\n",
        "                        x = x/255\n",
        "                    else:\n",
        "                        x = x/255\n",
        "\n",
        "                    X.append(x)\n",
        "                    x_mask = (x_mask - np.min(x_mask))/ (np.max(x_mask) - np.min(x_mask) + 0.0000001)\n",
        "                    Y.append(x_mask)\n",
        "\n",
        "                del x_mask\n",
        "                del x_mask_temp\n",
        "                del x\n",
        "            Y = np.expand_dims(np.array(Y), axis=3)\n",
        "            Y = np.array(Y)\n",
        "            yield np.array(X), np.array(Y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "K9FrDyDxqrLD",
        "outputId": "3a968b76-b922-48b2-b4dc-b5634909a2f0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 1: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 1/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15s/step - dice_coef: 0.2254 - loss: 0.0447 \n",
            "Epoch 1: val_dice_coef improved from -inf to 0.23015, saving model to /content/drive/My Drive/NuInsSeg/shallow_unet/output_model/unet_1.weights.h5\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m320s\u001b[0m 16s/step - dice_coef: 0.2258 - loss: 0.0424 - val_dice_coef: 0.2302 - val_loss: -0.0621 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 2: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 2/100\n",
            "\u001b[1m 7/20\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m3:21\u001b[0m 15s/step - dice_coef: 0.2517 - loss: -0.0783"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4176931.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     76\u001b[0m     checkpoint = ModelCheckpoint(model_path, monitor='val_dice_coef', verbose=1,\n\u001b[1;32m     77\u001b[0m                              save_best_only=True, mode='max', save_weights_only = True)\n\u001b[0;32m---> 78\u001b[0;31m     history = model_raw.fit(data_gen(train_img,\n\u001b[0m\u001b[1;32m     79\u001b[0m                                                  \u001b[0mtrain_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m                                                  \u001b[0mopts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'batch_size'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    375\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mepoch_iterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 377\u001b[0;31m                     \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    378\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mfunction\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m    218\u001b[0m                 \u001b[0miterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDistributedIterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m             ):\n\u001b[0;32m--> 220\u001b[0;31m                 \u001b[0mopt_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmulti_step_on_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    221\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mopt_outputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    831\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 833\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    834\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    876\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    877\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 878\u001b[0;31m       results = tracing_compilation.call_function(\n\u001b[0m\u001b[1;32m    879\u001b[0m           \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    880\u001b[0m       )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m   \u001b[0mbound_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m   \u001b[0mflat_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbound_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m   return function._call_flat(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    140\u001b[0m       \u001b[0mflat_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m   )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1320\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1321\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1324\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;34m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     \u001b[0mflat_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_recording\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bound_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m             outputs = self._bound_context.call_function(\n\u001b[0m\u001b[1;32m    252\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1686\u001b[0m     \u001b[0mcancellation_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcancellation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1687\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcancellation_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1688\u001b[0;31m       outputs = execute.execute(\n\u001b[0m\u001b[1;32m   1689\u001b[0m           \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1690\u001b[0m           \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# main training loop (for all k fold cross-validation)\n",
        "kf = KFold(n_splits= opts['k_fold'],random_state= opts['random_seed_num'],shuffle=True)\n",
        "kf.get_n_splits(img_path)\n",
        "\n",
        "start_time = time.time()\n",
        "dice_unet = np.zeros([opts['k_fold'],len(img_path)//opts['k_fold']])\n",
        "AJI_unet = np.zeros([opts['k_fold'],len(img_path)//opts['k_fold']])\n",
        "PQ_unet = np.zeros([opts['k_fold'],len(img_path)//opts['k_fold']])\n",
        "\n",
        "\n",
        "dice_unet_watershed = np.zeros([opts['k_fold'],len(img_path)//opts['k_fold']])\n",
        "AJI_unet_watershed = np.zeros([opts['k_fold'],len(img_path)//opts['k_fold']])\n",
        "PQ_unet_watershed = np.zeros([opts['k_fold'],len(img_path)//opts['k_fold']])\n",
        "\n",
        "dice_unet_watershed_without_vague = np.zeros([opts['k_fold'],len(img_path)//opts['k_fold']])\n",
        "AJI_unet_watershed_without_vague = np.zeros([opts['k_fold'],len(img_path)//opts['k_fold']])\n",
        "PQ_unet_watershed_without_vague = np.zeros([opts['k_fold'],len(img_path)//opts['k_fold']])\n",
        "\n",
        "dice_mean = []\n",
        "aji_mean = []\n",
        "pq_mean = []\n",
        "\n",
        "dice_watershed_mean = []\n",
        "aji_watershed_mean = []\n",
        "pq_watershed_mean = []\n",
        "\n",
        "dice_watershed_wovague_mean = []\n",
        "aji_watershed_wovague_mean = []\n",
        "pq_watershed_wovague_mean = []\n",
        "\n",
        "current_fold = 1\n",
        "\n",
        "for idx, [train_index,  test_index] in enumerate(kf.split(img_path)):\n",
        "    shuffle(train_index)\n",
        "    shuffle(test_index)\n",
        "\n",
        "    train_img   = [img_path[name] for name in train_index]\n",
        "    train_mask  = [binary_mask_path[name] for name in train_index]\n",
        "    train_dis   = [distance_mask_path[name] for name in train_index]\n",
        "    train_label = [label_mask_path[name] for name in train_index]\n",
        "\n",
        "    test_img   = [img_path[name] for name in test_index]\n",
        "    test_mask  = [binary_mask_path[name] for name in test_index]\n",
        "    test_dis   = [distance_mask_path[name] for name in test_index]\n",
        "    test_label = [label_mask_path[name] for name in test_index]\n",
        "    test_vague = [vague_mask_path[name] for name in test_index]\n",
        "\n",
        "    #creating validation set\n",
        "    validation_set_img = []\n",
        "    validation_set_label = []\n",
        "    #validation_DIS = []\n",
        "    validation_set_vague = []\n",
        "    for counter in range(len(test_img)):\n",
        "        val_img = cv2.imread(test_img[counter])\n",
        "        val_img = cv2.cvtColor(val_img, cv2.COLOR_BGR2RGB)\n",
        "        val_img = val_img/255\n",
        "        val_label = cv2.imread(test_label[counter], -1) # cv2.IMREAD_UNCHANGED:\n",
        "        #It specifies to load an image as such including alpha channel.\n",
        "        #Alternatively, we can pass integer value -1 for this flag.\n",
        "        val_vague = cv2.imread(test_vague[counter], -1)\n",
        "\n",
        "        validation_set_img.append(val_img)\n",
        "        validation_set_label.append(val_label)\n",
        "        validation_set_vague.append(val_vague)\n",
        "\n",
        "    validation_set_img = np.array(validation_set_img)\n",
        "    validation_set_label = np.array(validation_set_label)\n",
        "    validation_set_vague = np.array(validation_set_vague)\n",
        "\n",
        "    model_path = opts['model_save_path'] + 'unet_{}.weights.h5'.format(current_fold)\n",
        "    logger = CSVLogger(opts['model_save_path']+ 'unet_{}.log'.format(current_fold))\n",
        "    LR_drop = step_decay_schedule(initial_lr= opts['init_LR'],\n",
        "                              decay_factor = opts['LR_decay_factor'],\n",
        "                              epochs_drop = opts['LR_drop_after_nth_epoch'])\n",
        "    model_raw = shallow_unet(opts['number_of_channel'], opts['init_LR'])\n",
        "    checkpoint = ModelCheckpoint(model_path, monitor='val_dice_coef', verbose=1,\n",
        "                             save_best_only=True, mode='max', save_weights_only = True)\n",
        "    history = model_raw.fit(data_gen(train_img,\n",
        "                                                 train_mask,\n",
        "                                                 opts['batch_size'],\n",
        "                                                 1,\n",
        "                                                 opts['crop_size'], opts['crop_size'],\n",
        "                                                 distance_unet_flag=0,\n",
        "                                                 augment=True,\n",
        "                                                 BACKBONE_model= '',\n",
        "                                                 use_pretrain_flag= False),\n",
        "                                      validation_data=data_gen(test_img,\n",
        "                                                               test_mask,\n",
        "                                                               opts['batch_size'],\n",
        "                                                               1,\n",
        "                                                               opts['crop_size'], opts['crop_size'],\n",
        "                                                               distance_unet_flag=0,\n",
        "                                                               augment= False,\n",
        "                                                               BACKBONE_model= '',\n",
        "                                                               use_pretrain_flag= False),\n",
        "                                      validation_steps=1,\n",
        "                                      epochs=opts['epoch_num'], verbose=1,\n",
        "                                      callbacks=[checkpoint, logger, LR_drop],\n",
        "                                      steps_per_epoch=(len(train_img) // opts['batch_size']) // opts['quick_run'])\n",
        "\n",
        "    model_raw.load_weights(opts['model_save_path'] + 'unet_{}.weights.h5'.format(current_fold))\n",
        "\n",
        "    ## predication on validation set\n",
        "    pred_val = model_raw.predict(validation_set_img, verbose=1, batch_size=1)\n",
        "    pred_val_t = (pred_val > opts['treshold']).astype(np.uint8)\n",
        "\n",
        "    output_watershed_tot_fold = []\n",
        "    output_watershed_tot_fold_wo_vague = []\n",
        "    validation_set_label_tot_fold_wo_vague = []\n",
        "    for val_len in tqdm.tqdm(range(len(pred_val))):\n",
        "        # with watershed post processing\n",
        "        local_maxi = peak_local_max(np.squeeze(pred_val[val_len]), indices=False,\n",
        "                                    exclude_border=False, footprint=np.ones((15, 15)))\n",
        "        marker = ndi.label(local_maxi)[0]\n",
        "        output_watershed = watershed(-np.squeeze(pred_val[val_len]), marker,mask = np.squeeze(pred_val_t[[val_len]]))\n",
        "        output_watershed[np.squeeze(pred_val_t[[val_len]])==0] = 0\n",
        "        output_watershed = remove_small_objects(output_watershed, min_size=50, connectivity=2)#remove small objects\n",
        "\n",
        "\n",
        "\n",
        "        # without post processing\n",
        "        output_raw_0 = np.squeeze(pred_val_t[val_len])\n",
        "        output_raw = skimage.morphology.label(output_raw_0)\n",
        "        output_raw = remove_small_objects(output_raw, min_size=50, connectivity=2) #remove small objects\n",
        "\n",
        "\n",
        "        output_watershed = remap_label(output_watershed)\n",
        "        validation_set_label[val_len] = remap_label(validation_set_label[val_len])\n",
        "        output_raw = remap_label(output_raw)\n",
        "\n",
        "        test_name = get_id_from_file_path(test_img[val_len],'.png' )\n",
        "\n",
        "        imsave(opts['result_save_path']+'validation/watershed_unet/{}.png'.format(test_name),\n",
        "               output_watershed.astype(np.uint16))\n",
        "        imsave(opts['result_save_path']+'validation/unet/{}.png'.format(test_name),output_raw.astype(np.uint16))\n",
        "\n",
        "\n",
        "\n",
        "        dice_unet[current_fold-1, val_len]= get_dice_1(validation_set_label[val_len], output_raw)\n",
        "        AJI_unet[current_fold-1, val_len] = get_fast_aji(validation_set_label[val_len], output_raw)\n",
        "        PQ_unet[current_fold-1, val_len] = get_fast_pq(validation_set_label[val_len], output_raw)[0][2]\n",
        "\n",
        "\n",
        "        dice_unet_watershed[current_fold-1, val_len]= get_dice_1(validation_set_label[val_len],output_watershed)\n",
        "        AJI_unet_watershed[current_fold-1, val_len] = get_fast_aji(validation_set_label[val_len], output_watershed)\n",
        "        PQ_unet_watershed[current_fold-1, val_len]  = get_fast_pq(validation_set_label[val_len], output_watershed)[0][2]\n",
        "        ###################################################################\n",
        "\n",
        "        #\n",
        "        output_watershed_wo_vague = np.copy(output_watershed)\n",
        "        output_watershed_wo_vague[validation_set_vague[val_len] == 255] = 0\n",
        "        output_watershed_wo_vague = remove_small_objects(output_watershed_wo_vague, min_size=50, connectivity=2)\n",
        "\n",
        "        validation_set_label_wo_vague = np.copy(validation_set_label[val_len])\n",
        "        validation_set_label_wo_vague[validation_set_vague[val_len] == 255] = 0\n",
        "        validation_set_label_wo_vague = remove_small_objects(validation_set_label_wo_vague, min_size=50, connectivity=2)\n",
        "\n",
        "        output_watershed_wo_vague = remap_label(output_watershed_wo_vague)\n",
        "        validation_set_label_wo_vague = remap_label(validation_set_label_wo_vague)\n",
        "\n",
        "\n",
        "\n",
        "        dice_unet_watershed_without_vague[current_fold-1, val_len]= get_dice_1(\n",
        "            validation_set_label_wo_vague,output_watershed_wo_vague)\n",
        "        AJI_unet_watershed_without_vague[current_fold-1, val_len] = get_fast_aji(\n",
        "            validation_set_label_wo_vague,output_watershed_wo_vague)\n",
        "        PQ_unet_watershed_without_vague[current_fold-1, val_len]  = get_fast_pq(\n",
        "            validation_set_label_wo_vague,output_watershed_wo_vague)[0][2]\n",
        "\n",
        "        output_watershed_tot_fold.append(output_watershed)\n",
        "        output_watershed_tot_fold_wo_vague.append(output_watershed_wo_vague)\n",
        "        validation_set_label_tot_fold_wo_vague.append(validation_set_label_wo_vague)\n",
        "\n",
        "    print('==========')\n",
        "    print('average dice pure Unet for fold{}: {:.2f}'.format(current_fold, np.mean(dice_unet[current_fold-1, :]*100)))\n",
        "    print('average AJI pure Unet for fold{}: {:.2f}'.format(current_fold, np.mean(AJI_unet[current_fold-1, :]*100)))\n",
        "    print('average PQ pure Unet for fold{}: {:.2f}'.format(current_fold, np.mean(PQ_unet[current_fold-1, :]*100)))\n",
        "    dice_mean.append(np.mean(dice_unet[current_fold-1, :]*100))\n",
        "    aji_mean.append(np.mean(AJI_unet[current_fold-1, :]*100))\n",
        "    pq_mean.append(np.mean(PQ_unet[current_fold-1, :]*100))\n",
        "    print('==========')\n",
        "\n",
        "    print('==========')\n",
        "    print('average Dice Unet watershed for fold{}: {:.2f}'.format(current_fold,\n",
        "                                                                   np.mean(dice_unet_watershed[current_fold-1, :]*100)))\n",
        "    print('average AJI Unet watershed for fold{}: {:.2f}'.format(current_fold,\n",
        "                                                                  np.mean(AJI_unet_watershed[current_fold-1, :]*100)))\n",
        "    print('average PQ Unet watershed for fold{}: {:.2f}'.format(current_fold,\n",
        "                                                                 np.mean(PQ_unet_watershed[current_fold-1, :]*100)))\n",
        "    dice_watershed_mean.append(np.mean(dice_unet_watershed[current_fold-1, :]*100))\n",
        "    aji_watershed_mean.append(np.mean(AJI_unet_watershed[current_fold-1, :]*100))\n",
        "    pq_watershed_mean.append(np.mean(PQ_unet_watershed[current_fold-1, :]*100))\n",
        "    print('==========')\n",
        "\n",
        "    print('average Dice Unet watershed wo vague for fold{}: {:.2f}'.format(current_fold,\n",
        "                                                                   np.mean(dice_unet_watershed_without_vague[current_fold-1, :]*100)))\n",
        "    print('average AJI Unet watershed wo vague for fold{}: {:.2f}'.format(current_fold,\n",
        "                                                                  np.mean(AJI_unet_watershed_without_vague[current_fold-1, :]*100)))\n",
        "    print('average PQ Unet watershed wo vague for fold{}: {:.2f}'.format(current_fold,\n",
        "                                                                 np.mean(PQ_unet_watershed_without_vague[current_fold-1, :]*100)))\n",
        "    dice_watershed_wovague_mean.append(np.mean(dice_unet_watershed_without_vague[current_fold-1, :]*100))\n",
        "    aji_watershed_wovague_mean.append(np.mean(AJI_unet_watershed_without_vague[current_fold-1, :]*100))\n",
        "    pq_watershed_wovague_mean.append(np.mean(PQ_unet_watershed_without_vague[current_fold-1, :]*100))\n",
        "    print('==========')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    current_fold = current_fold + 1\n",
        "#     if idx ==0:\n",
        "#         break\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ul89tGCPtc7k"
      },
      "outputs": [],
      "source": [
        "fold_names = ['fold1', 'fold2']\n",
        "df_dice = pd.DataFrame({'fold num':fold_names, 'dice unet':dice_mean,'dice unet watershed':dice_watershed_mean,\n",
        "                   'dice unet whatershed wo vague':dice_watershed_wovague_mean})\n",
        "\n",
        "df_aji = pd.DataFrame({'fold num':fold_names, 'AJI unet':aji_mean,'AJI unet watershed':aji_watershed_mean,\n",
        "                   'AJI unet whatershed wo vague':aji_watershed_wovague_mean})\n",
        "\n",
        "df_pq = pd.DataFrame({'fold num':fold_names, 'PQ unet':pq_mean,'PQ unet watershed':pq_watershed_mean,\n",
        "                   'PQ unet whatershed wo vague':pq_watershed_wovague_mean})\n",
        "\n",
        "df_dice.to_csv('/kaggle/working/dice.csv', index=False)\n",
        "df_aji.to_csv('/kaggle/working/aji.csv', index=False)\n",
        "df_pq.to_csv('/kaggle/working/pq.csv', index=False)\n",
        "\n",
        "print(df_dice.head())\n",
        "print('============================================================')\n",
        "print(df_aji.head())\n",
        "print('============================================================')\n",
        "print(df_pq.head())\n",
        "print('============================================================')\n",
        "\n",
        "\n",
        "finish_time = time.time()\n",
        "print('==========')\n",
        "print('total training time (all 5 folds): {:.2f} minutes'.format((finish_time- start_time)/60))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "628AmbTQqrLF"
      },
      "outputs": [],
      "source": [
        "#showing random images and results from the last fold\n",
        "for i in range(4):\n",
        "    rand_num  = np.random.randint(len(validation_set_img))\n",
        "    plt.figure(figsize= (10,60))\n",
        "    plt.subplot(1,6,1)\n",
        "    plt.imshow(validation_set_img[rand_num])\n",
        "    plt.title(get_id_from_file_path(test_img[rand_num], '.png'))\n",
        "\n",
        "\n",
        "    plt.subplot(1,6,2)\n",
        "    plt.imshow(validation_set_label[rand_num])\n",
        "    plt.title('GT')\n",
        "\n",
        "    plt.subplot(1,6,3)\n",
        "    plt.imshow(validation_set_vague[rand_num])\n",
        "    plt.title('GT vague')\n",
        "\n",
        "    plt.subplot(1,6,4)\n",
        "    plt.imshow(output_watershed_tot_fold[rand_num])\n",
        "    plt.title('labeled \\n prediction')\n",
        "\n",
        "    plt.subplot(1,6,5)\n",
        "    plt.imshow(output_watershed_tot_fold_wo_vague[rand_num])\n",
        "    plt.title('labeled prediction \\n (wo vague)')\n",
        "\n",
        "#     plt.subplot(1,6,6)\n",
        "#     plt.imshow(validation_set_label_tot_fold_wo_vague[rand_num])\n",
        "#     plt.title('GT wo vague')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tLaOm8W0qrLG"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "datasetId": 1911713,
          "sourceId": 5909621,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 30158,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}