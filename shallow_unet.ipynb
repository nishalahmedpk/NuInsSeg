{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ddf745f2-aefc-45df-a8e7-8e838d56caa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "opts = {}\n",
    "opts['number_of_channel'] = 3                   \n",
    "opts['treshold'] = 0.5                          \n",
    "opts['epoch_num'] = 100                   \n",
    "opts['quick_run'] = 1   \n",
    "opts['batch_size'] = 16                          \n",
    "opts['random_seed_num'] = 19   \n",
    "opts['k_fold'] = 5                             \n",
    "opts['save_val_results'] = 1         \n",
    "opts['init_LR'] = 0.001                         \n",
    "opts['LR_decay_factor'] = 0.5                   \n",
    "opts['LR_drop_after_nth_epoch'] = 20            \n",
    "opts['crop_size'] = 512   \n",
    "## output directories\n",
    "opts['result_save_path'] ='/prediction_image/'\n",
    "opts['model_save_path'] ='/output_model/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d33d525e-8021-4299-ba53-d204ed312990",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chicken\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold,StratifiedKFold\n",
    "import time  \n",
    "import cv2\n",
    "import keras\n",
    "from keras.callbacks import CSVLogger, LearningRateScheduler, ModelCheckpoint\n",
    "from keras.layers import *\n",
    "from keras.models import Model, load_model\n",
    "from keras.optimizers import Adam\n",
    "from albumentations import *\n",
    "from keras import backend as K\n",
    "from skimage.feature import peak_local_max\n",
    "from scipy import ndimage as ndi\n",
    "from skimage.segmentation import watershed\n",
    "import skimage.morphology\n",
    "from skimage.io import imsave\n",
    "from skimage.morphology import remove_small_objects\n",
    "import tqdm\n",
    "from random import shuffle \n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "print('Chicken')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32336488-74e8-497d-9553-c047f2049cf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['human bladder',\n",
       " 'human brain',\n",
       " 'human cardia',\n",
       " 'human cerebellum',\n",
       " 'human epiglottis',\n",
       " 'human jejunum',\n",
       " 'human kidney',\n",
       " 'human liver',\n",
       " 'human lung',\n",
       " 'human melanoma',\n",
       " 'human muscle',\n",
       " 'human oesophagus',\n",
       " 'human pancreas',\n",
       " 'human peritoneum',\n",
       " 'human placenta',\n",
       " 'human pylorus',\n",
       " 'human rectum',\n",
       " 'human salivory gland',\n",
       " 'human spleen',\n",
       " 'human testis',\n",
       " 'human tongue',\n",
       " 'human tonsile',\n",
       " 'human umbilical cord',\n",
       " 'mouse fat (white and brown)_subscapula',\n",
       " 'mouse femur',\n",
       " 'mouse heart',\n",
       " 'mouse kidney',\n",
       " 'mouse liver',\n",
       " 'mouse muscle_tibia',\n",
       " 'mouse spleen',\n",
       " 'mouse thymus']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_path = 'Dataset/'\n",
    "organ_names = [ name for name in os.listdir(base_path) if os.path.isdir(os.path.join(base_path, name)) ]\n",
    "organ_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce61d929-e535-4878-b10a-80a58dbb0144",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input and outpu paths\n",
    "img_path = glob('{}*{}'.format('Dataset/*/tissue images/', 'png'))\n",
    "binary_mask_path = glob('{}*{}'.format('Dataset/*/mask binary/', 'png'))\n",
    "distance_mask_path = glob('{}*{}'.format('Dataset/*/distance maps/', 'png'))\n",
    "label_mask_path = glob('{}*{}'.format('Dataset/*/label masks modify/', 'tif'))\n",
    "vague_mask_path =  glob('{}*{}'.format('Dataset/*/vague areas/mask binary/', 'png'))\n",
    "\n",
    "\n",
    "img_path.sort()\n",
    "binary_mask_path.sort()\n",
    "distance_mask_path.sort()\n",
    "label_mask_path.sort()\n",
    "vague_mask_path.sort()\n",
    "\n",
    "\n",
    "# create folders to save the best models and images (if needed) for each fold\n",
    "if not os.path.exists('prediction_image/'):\n",
    "    os.makedirs('prediction_image/')\n",
    "if not os.path.exists('output_model/'):\n",
    "    os.makedirs('output_model/')    \n",
    "if not os.path.exists(opts['result_save_path']+ 'validation/unet'):\n",
    "    os.makedirs(opts['result_save_path'] + 'validation/unet')\n",
    "if not os.path.exists(opts['result_save_path']+ 'validation/watershed_unet'):\n",
    "    os.makedirs(opts['result_save_path'] + 'validation/watershed_unet')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c816a97-901a-4840-a840-e072fb4bd92a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image path: Dataset\\human pancreas\\tissue images\\human_pancreas_41.png\n",
      " binary mask path: Dataset\\human pancreas\\mask binary\\human_pancreas_41.png\n",
      " distance mask path: Dataset\\human pancreas\\distance maps\\human_pancreas_41.png\n",
      " label mask path: Dataset\\human pancreas\\label masks modify\\human_pancreas_41.tif\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rand_num = np.random.randint(len(img_path))\n",
    "print('image path: {}\\n'.format(img_path[rand_num]),\n",
    "      'binary mask path: {}\\n'.format(binary_mask_path[rand_num]),\n",
    "      'distance mask path: {}\\n'.format(distance_mask_path[rand_num]),\n",
    "      'label mask path: {}\\n'.format(label_mask_path[rand_num]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c36c333-4f6f-4d0b-82fa-000902824bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define loss function\n",
    "def dice_coef(y_true, y_pred):\n",
    "    smooth = 1.\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
    "\n",
    "def dice_loss(y_true, y_pred):\n",
    "    return 1 - dice_coef(y_true, y_pred)\n",
    "#####################################################################################\n",
    "# Combination of Dice and binary cross entophy loss function that is used in this baseline segmentation\n",
    "def bce_dice_loss(y_true, y_pred):\n",
    "    return 0.5 * keras.losses.binary_crossentropy(y_true, y_pred) - dice_coef(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a06f5ed1-e549-4bea-aa62-d21541006608",
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning rate scheduler\n",
    "def step_decay_schedule(initial_lr=1e-3, decay_factor=0.75, epochs_drop=1000):\n",
    "    '''\n",
    "    Wrapper function to create a LearningRateScheduler with step decay schedule.\n",
    "    '''\n",
    "    def schedule(epoch):\n",
    "        return initial_lr * (decay_factor ** np.floor(epoch/epochs_drop))\n",
    "    \n",
    "    return LearningRateScheduler(schedule, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1df3d693-33ea-475b-b6f3-c25eeb6735d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# U-net models\n",
    "#############################################################################################################\n",
    "def shallow_unet( IMG_CHANNELS, LearnRate):\n",
    "    inputs = Input((None, None, IMG_CHANNELS))\n",
    "    #s = Lambda(lambda x: x / 255) (inputs)\n",
    "    c1 = Conv2D(16, (3, 3), activation='relu', kernel_initializer='glorot_uniform', padding='same') (inputs)\n",
    "    c1 = Dropout(0.1) (c1)\n",
    "    c1 = Conv2D(16, (3, 3), activation='relu', kernel_initializer='glorot_uniform', padding='same') (c1)\n",
    "    p1 = MaxPooling2D((2, 2)) (c1)\n",
    "\n",
    "    c2 = Conv2D(32, (3, 3), activation='relu', kernel_initializer='glorot_uniform', padding='same') (p1)\n",
    "    c2 = Dropout(0.1) (c2)\n",
    "    c2 = Conv2D(32, (3, 3), activation='relu', kernel_initializer='glorot_uniform', padding='same') (c2)\n",
    "    p2 = MaxPooling2D((2, 2)) (c2)\n",
    "\n",
    "    c3 = Conv2D(64, (3, 3), activation='relu', kernel_initializer='glorot_uniform', padding='same') (p2)\n",
    "    c3 = Dropout(0.1) (c3)\n",
    "    c3 = Conv2D(64, (3, 3), activation='relu', kernel_initializer='glorot_uniform', padding='same') (c3)\n",
    "    p3 = MaxPooling2D((2, 2)) (c3)\n",
    "\n",
    "    c4 = Conv2D(128, (3, 3), activation='relu', kernel_initializer='glorot_uniform', padding='same') (p3)\n",
    "    c4 = Dropout(0.1) (c4)\n",
    "    c4 = Conv2D(128, (3, 3), activation='relu', kernel_initializer='glorot_uniform', padding='same') (c4)\n",
    "    p4 = MaxPooling2D(pool_size=(2, 2)) (c4)\n",
    "\n",
    "    c5 = Conv2D(256, (3, 3), activation='relu', kernel_initializer='glorot_uniform', padding='same') (p4)\n",
    "    c5 = Dropout(0.1) (c5)\n",
    "    c5 = Conv2D(256, (3, 3), activation='relu', kernel_initializer='glorot_uniform', padding='same') (c5)\n",
    "\n",
    "    u6 = Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same') (c5)\n",
    "    u6 = concatenate([u6, c4])\n",
    "    c6 = Conv2D(128, (3, 3), activation='relu', kernel_initializer='glorot_uniform', padding='same') (u6)\n",
    "    c6 = Dropout(0.1) (c6)\n",
    "    c6 = Conv2D(128, (3, 3), activation='relu', kernel_initializer='glorot_uniform', padding='same') (c6)\n",
    "\n",
    "    u7 = Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same') (c6)\n",
    "    u7 = concatenate([u7, c3])\n",
    "    c7 = Conv2D(64, (3, 3), activation='relu', kernel_initializer='glorot_uniform', padding='same') (u7)\n",
    "    c7 = Dropout(0.1) (c7)\n",
    "    c7 = Conv2D(64, (3, 3), activation='relu', kernel_initializer='glorot_uniform', padding='same') (c7)\n",
    "\n",
    "    u8 = Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same') (c7)\n",
    "    u8 = concatenate([u8, c2])\n",
    "    c8 = Conv2D(32, (3, 3), activation='relu', kernel_initializer='glorot_uniform', padding='same') (u8)\n",
    "    c8 = Dropout(0.1) (c8)\n",
    "    c8 = Conv2D(32, (3, 3), activation='relu', kernel_initializer='glorot_uniform', padding='same') (c8)\n",
    "\n",
    "    u9 = Conv2DTranspose(16, (2, 2), strides=(2, 2), padding='same') (c8)\n",
    "    u9 = concatenate([u9, c1], axis=3)\n",
    "    c9 = Conv2D(16, (3, 3), activation='relu', kernel_initializer='glorot_uniform', padding='same') (u9)\n",
    "    c9 = Dropout(0.1) (c9)\n",
    "    c9 = Conv2D(16, (3, 3), activation='relu', kernel_initializer='glorot_uniform', padding='same') (c9)\n",
    "\n",
    "    outputs = Conv2D(1, (1, 1), activation='sigmoid') (c9) # for binary\n",
    "\n",
    "    model = Model(inputs=[inputs], outputs=[outputs])\n",
    "    model.compile(optimizer = Adam(learning_rate= LearnRate), loss= bce_dice_loss , metrics=[dice_coef]) \n",
    "    #model.summary()\n",
    "    return model\n",
    "#############################################################################################################\n",
    "def deep_unet(IMG_CHANNELS, LearnRate):\n",
    "    # Build U-Net model\n",
    "    inputs = Input((None, None, IMG_CHANNELS))\n",
    "    #s = Lambda(lambda x: x / 255) (inputs)\n",
    "    c1 = Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (inputs)\n",
    "    c1 = Dropout(0.1) (c1)\n",
    "    c1 = Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (c1)\n",
    "    p1 = MaxPooling2D((2, 2)) (c1)\n",
    "\n",
    "    c2 = Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (p1)\n",
    "    c2 = Dropout(0.1) (c2)\n",
    "    c2 = Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (c2)\n",
    "    p2 = MaxPooling2D((2, 2)) (c2)\n",
    "\n",
    "    c3 = Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (p2)\n",
    "    c3 = Dropout(0.1) (c3)\n",
    "    c3 = Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (c3)\n",
    "    p3 = MaxPooling2D((2, 2)) (c3)\n",
    "\n",
    "    c4 = Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (p3)\n",
    "    c4 = Dropout(0.1) (c4)\n",
    "    c4 = Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (c4)\n",
    "    p4 = MaxPooling2D(pool_size=(2, 2)) (c4)\n",
    "    \n",
    "    \n",
    "    c4_new = Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (p4)\n",
    "    c4_new = Dropout(0.1) (c4_new)\n",
    "    c4_new = Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (c4_new)\n",
    "    p4_new = MaxPooling2D(pool_size=(2, 2)) (c4_new)\n",
    "\n",
    "    c5 = Conv2D(512, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (p4_new)\n",
    "    c5 = Dropout(0.1) (c5)\n",
    "    c5 = Conv2D(512, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (c5)\n",
    "    \n",
    "    \n",
    "    u6_new = Conv2DTranspose(256, (2, 2), strides=(2, 2), padding='same') (c5)\n",
    "    u6_new = concatenate([u6_new, c4_new])\n",
    "    c6_new = Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (u6_new)\n",
    "    c6_new = Dropout(0.1) (c6_new)\n",
    "    c6_new = Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (c6_new)\n",
    "\n",
    "    u6 = Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same') (c6_new)\n",
    "    u6 = concatenate([u6, c4])\n",
    "    c6 = Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (u6)\n",
    "    c6 = Dropout(0.1) (c6)\n",
    "    c6 = Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (c6)\n",
    "\n",
    "    u7 = Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same') (c6)\n",
    "    u7 = concatenate([u7, c3])\n",
    "    c7 = Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (u7)\n",
    "    c7 = Dropout(0.1) (c7)\n",
    "    c7 = Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (c7)\n",
    "\n",
    "    u8 = Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same') (c7)\n",
    "    u8 = concatenate([u8, c2])\n",
    "    c8 = Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (u8)\n",
    "    c8 = Dropout(0.1) (c8)\n",
    "    c8 = Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (c8)\n",
    "\n",
    "    u9 = Conv2DTranspose(16, (2, 2), strides=(2, 2), padding='same') (c8)\n",
    "    u9 = concatenate([u9, c1], axis=3)\n",
    "    c9 = Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (u9)\n",
    "    c9 = Dropout(0.1) (c9)\n",
    "    c9 = Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (c9)\n",
    "\n",
    "    outputs = Conv2D(1, (1, 1), activation='sigmoid') (c9)\n",
    "\n",
    "    model_deep = Model(inputs=[inputs], outputs=[outputs])\n",
    "    model_deep.compile(optimizer = Adam(learning_rate=LearnRate), loss= bce_dice_loss , metrics=[ dice_coef])\n",
    "    #model_deeper.summary()\n",
    "    return model_deep\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "87654577-9fc2-4f4c-a5d3-1c82a9241afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Transformations for Data Augmentation\n",
    "def albumentation_aug(p=1.0, crop_size_row = 448, crop_size_col = 448 ):\n",
    "    return Compose([\n",
    "        RandomCrop(crop_size_row, crop_size_col, always_apply=True, p=1),\n",
    "        CLAHE(clip_limit=4.0, tile_grid_size=(8, 8), always_apply=False, p=0.5),\n",
    "        RandomBrightnessContrast(brightness_limit=0.15, contrast_limit=0.15, brightness_by_max=True, p=0.4),\n",
    "        HueSaturationValue(hue_shift_limit=20, sat_shift_limit=20, val_shift_limit=20, p=0.1),\n",
    "        HorizontalFlip(always_apply=False, p=0.5),\n",
    "        VerticalFlip(always_apply=False, p=0.5),\n",
    "        RandomRotate90(always_apply=False, p=0.5),\n",
    "        ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.1, rotate_limit=20, interpolation=1, \n",
    "                         border_mode=4, always_apply=False, p=0.1),\n",
    "\n",
    "    ], p=p)\n",
    "# last  p has the second proiroty comapred to the p inside each argument \n",
    "#(e.g. HorizontalFlip(always_apply=False, p=0.5) )\n",
    "#############################################################################################################\n",
    "# def albumentation_aug_light(p=1.0, crop_size_row = 448, crop_size_col = 448):\n",
    "#     return Compose([\n",
    "#         RandomCrop(crop_size_row, crop_size_col, always_apply=True, p=1.0),\n",
    "#         HorizontalFlip(always_apply=False, p=0.5),\n",
    "#         VerticalFlip(always_apply=False, p=0.5),\n",
    "#         RandomRotate90(always_apply=False, p=0.5),\n",
    "#         ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.1, rotate_limit=20, interpolation=1, \n",
    "#                          border_mode=4 , always_apply=False, p=0.1),\n",
    "#     ], p=p, additional_targets={'mask1': 'mask','mask2': 'mask'}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fd70a79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fast_aji(true, pred):\n",
    "    \"\"\"AJI version distributed by MoNuSeg, has no permutation problem but suffered from \n",
    "    over-penalisation similar to DICE2.\n",
    "    Fast computation requires instance IDs are in contiguous orderding i.e [1, 2, 3, 4] \n",
    "    not [2, 3, 6, 10]. Please call `remap_label` before hand and `by_size` flag has no \n",
    "    effect on the result.\n",
    "    \"\"\"\n",
    "    true = np.copy(true)  # ? do we need this\n",
    "    pred = np.copy(pred)\n",
    "    true_id_list = list(np.unique(true))\n",
    "    pred_id_list = list(np.unique(pred))\n",
    "    #print(len(pred_id_list))\n",
    "    if len(pred_id_list) == 1:\n",
    "        return 0\n",
    "\n",
    "    true_masks = [None,]\n",
    "    for t in true_id_list[1:]:\n",
    "        t_mask = np.array(true == t, np.uint8)\n",
    "        true_masks.append(t_mask)\n",
    "\n",
    "    pred_masks = [None,]\n",
    "    for p in pred_id_list[1:]:\n",
    "        p_mask = np.array(pred == p, np.uint8)\n",
    "        pred_masks.append(p_mask)\n",
    "\n",
    "    # prefill with value\n",
    "    pairwise_inter = np.zeros(\n",
    "        [len(true_id_list) - 1, len(pred_id_list) - 1], dtype=np.float64\n",
    "    )\n",
    "    pairwise_union = np.zeros(\n",
    "        [len(true_id_list) - 1, len(pred_id_list) - 1], dtype=np.float64\n",
    "    )\n",
    "\n",
    "    # caching pairwise\n",
    "    for true_id in true_id_list[1:]:  # 0-th is background\n",
    "        t_mask = true_masks[true_id]\n",
    "        pred_true_overlap = pred[t_mask > 0]\n",
    "        pred_true_overlap_id = np.unique(pred_true_overlap)\n",
    "        pred_true_overlap_id = list(pred_true_overlap_id)\n",
    "        for pred_id in pred_true_overlap_id:\n",
    "            if pred_id == 0:  # ignore\n",
    "                continue  # overlaping background\n",
    "            p_mask = pred_masks[pred_id]\n",
    "            total = (t_mask + p_mask).sum()\n",
    "            inter = (t_mask * p_mask).sum()\n",
    "            pairwise_inter[true_id - 1, pred_id - 1] = inter\n",
    "            pairwise_union[true_id - 1, pred_id - 1] = total - inter\n",
    "\n",
    "    pairwise_iou = pairwise_inter / (pairwise_union + 1.0e-6)\n",
    "    # pair of pred that give highest iou for each true, dont care\n",
    "    # about reusing pred instance multiple times\n",
    "    paired_pred = np.argmax(pairwise_iou, axis=1)\n",
    "    pairwise_iou = np.max(pairwise_iou, axis=1)\n",
    "    # exlude those dont have intersection\n",
    "    paired_true = np.nonzero(pairwise_iou > 0.0)[0]\n",
    "    paired_pred = paired_pred[paired_true]\n",
    "    # print(paired_true.shape, paired_pred.shape)\n",
    "    overall_inter = (pairwise_inter[paired_true, paired_pred]).sum()\n",
    "    overall_union = (pairwise_union[paired_true, paired_pred]).sum()\n",
    "\n",
    "    paired_true = list(paired_true + 1)  # index to instance ID\n",
    "    paired_pred = list(paired_pred + 1)\n",
    "    # add all unpaired GT and Prediction into the union\n",
    "    unpaired_true = np.array(\n",
    "        [idx for idx in true_id_list[1:] if idx not in paired_true]\n",
    "    )\n",
    "    unpaired_pred = np.array(\n",
    "        [idx for idx in pred_id_list[1:] if idx not in paired_pred]\n",
    "    )\n",
    "    for true_id in unpaired_true:\n",
    "        overall_union += true_masks[true_id].sum()\n",
    "    for pred_id in unpaired_pred:\n",
    "        overall_union += pred_masks[pred_id].sum()\n",
    "\n",
    "    aji_score = overall_inter / overall_union\n",
    "    #print(aji_score)\n",
    "    return aji_score\n",
    "\n",
    "#############################################################################################################\n",
    "def get_fast_pq(true, pred, match_iou=0.5):\n",
    "    \"\"\"`match_iou` is the IoU threshold level to determine the pairing between\n",
    "    GT instances `p` and prediction instances `g`. `p` and `g` is a pair\n",
    "    if IoU > `match_iou`. However, pair of `p` and `g` must be unique \n",
    "    (1 prediction instance to 1 GT instance mapping).\n",
    "    If `match_iou` < 0.5, Munkres assignment (solving minimum weight matching\n",
    "    in bipartite graphs) is caculated to find the maximal amount of unique pairing. \n",
    "    If `match_iou` >= 0.5, all IoU(p,g) > 0.5 pairing is proven to be unique and\n",
    "    the number of pairs is also maximal.    \n",
    "    \n",
    "    Fast computation requires instance IDs are in contiguous orderding \n",
    "    i.e [1, 2, 3, 4] not [2, 3, 6, 10]. Please call `remap_label` beforehand \n",
    "    and `by_size` flag has no effect on the result.\n",
    "    Returns:\n",
    "        [dq, sq, pq]: measurement statistic\n",
    "        [paired_true, paired_pred, unpaired_true, unpaired_pred]: \n",
    "                      pairing information to perform measurement\n",
    "                    \n",
    "    \"\"\"\n",
    "    assert match_iou >= 0.0, \"Cant' be negative\"\n",
    "\n",
    "    true = np.copy(true)\n",
    "    pred = np.copy(pred)\n",
    "    true_id_list = list(np.unique(true))\n",
    "    pred_id_list = list(np.unique(pred))\n",
    "    \n",
    "    if len(pred_id_list) == 1:\n",
    "        return [0, 0, 0], [0,0, 0, 0]\n",
    "\n",
    "    true_masks = [\n",
    "        None,\n",
    "    ]\n",
    "    for t in true_id_list[1:]:\n",
    "        t_mask = np.array(true == t, np.uint8)\n",
    "        true_masks.append(t_mask)\n",
    "\n",
    "    pred_masks = [\n",
    "        None,\n",
    "    ]\n",
    "    for p in pred_id_list[1:]:\n",
    "        p_mask = np.array(pred == p, np.uint8)\n",
    "        pred_masks.append(p_mask)\n",
    "\n",
    "    # prefill with value\n",
    "    pairwise_iou = np.zeros(\n",
    "        [len(true_id_list) - 1, len(pred_id_list) - 1], dtype=np.float64\n",
    "    )\n",
    "\n",
    "    # caching pairwise iou\n",
    "    for true_id in true_id_list[1:]:  # 0-th is background\n",
    "        t_mask = true_masks[true_id]\n",
    "        pred_true_overlap = pred[t_mask > 0]\n",
    "        pred_true_overlap_id = np.unique(pred_true_overlap)\n",
    "        pred_true_overlap_id = list(pred_true_overlap_id)\n",
    "        for pred_id in pred_true_overlap_id:\n",
    "            if pred_id == 0:  # ignore\n",
    "                continue  # overlaping background\n",
    "            p_mask = pred_masks[pred_id]\n",
    "            total = (t_mask + p_mask).sum()\n",
    "            inter = (t_mask * p_mask).sum()\n",
    "            iou = inter / (total - inter)\n",
    "            pairwise_iou[true_id - 1, pred_id - 1] = iou\n",
    "    #\n",
    "    if match_iou >= 0.5:\n",
    "        paired_iou = pairwise_iou[pairwise_iou > match_iou]\n",
    "        pairwise_iou[pairwise_iou <= match_iou] = 0.0\n",
    "        paired_true, paired_pred = np.nonzero(pairwise_iou)\n",
    "        paired_iou = pairwise_iou[paired_true, paired_pred]\n",
    "        paired_true += 1  # index is instance id - 1\n",
    "        paired_pred += 1  # hence return back to original\n",
    "    else:  # * Exhaustive maximal unique pairing\n",
    "        #### Munkres pairing with scipy library\n",
    "        # the algorithm return (row indices, matched column indices)\n",
    "        # if there is multiple same cost in a row, index of first occurence\n",
    "        # is return, thus the unique pairing is ensure\n",
    "        # inverse pair to get high IoU as minimum\n",
    "        paired_true, paired_pred = linear_sum_assignment(-pairwise_iou)\n",
    "        ### extract the paired cost and remove invalid pair\n",
    "        paired_iou = pairwise_iou[paired_true, paired_pred]\n",
    "\n",
    "        # now select those above threshold level\n",
    "        # paired with iou = 0.0 i.e no intersection => FP or FN\n",
    "        paired_true = list(paired_true[paired_iou > match_iou] + 1)\n",
    "        paired_pred = list(paired_pred[paired_iou > match_iou] + 1)\n",
    "        paired_iou = paired_iou[paired_iou > match_iou]\n",
    "\n",
    "    # get the actual FP and FN\n",
    "    unpaired_true = [idx for idx in true_id_list[1:] if idx not in paired_true]\n",
    "    unpaired_pred = [idx for idx in pred_id_list[1:] if idx not in paired_pred]\n",
    "    # print(paired_iou.shape, paired_true.shape, len(unpaired_true), len(unpaired_pred))\n",
    "\n",
    "    #\n",
    "    tp = len(paired_true)\n",
    "    fp = len(unpaired_pred)\n",
    "    fn = len(unpaired_true)\n",
    "    # get the F1-score i.e DQ\n",
    "    dq = tp / (tp + 0.5 * fp + 0.5 * fn)\n",
    "    # get the SQ, no paired has 0 iou so not impact\n",
    "    sq = paired_iou.sum() / (tp + 1.0e-6)\n",
    "\n",
    "    return [dq, sq, dq * sq], [paired_true, paired_pred, unpaired_true, unpaired_pred]\n",
    "\n",
    "\n",
    "#############################################################################################################\n",
    "def get_dice_1(true, pred):\n",
    "    \"\"\"Traditional dice.\"\"\"\n",
    "    # cast to binary 1st\n",
    "    true = np.copy(true)\n",
    "    pred = np.copy(pred)\n",
    "    true[true > 0] = 1\n",
    "    pred[pred > 0] = 1\n",
    "    inter = true * pred\n",
    "    denom = true + pred\n",
    "    dice_score = 2.0 * np.sum(inter) / (np.sum(denom) + 0.0001)\n",
    "    if np.sum(inter)==0 and np.sum(denom)==0:\n",
    "        dice_score = 1 # to handel cases without any nuclei\n",
    "    #print(dice_score)\n",
    "    return dice_score\n",
    "#############################################################################################################\n",
    "def remap_label(pred, by_size=False):\n",
    "    \"\"\"Rename all instance id so that the id is contiguous i.e [0, 1, 2, 3] \n",
    "    not [0, 2, 4, 6]. The ordering of instances (which one comes first) \n",
    "    is preserved unless by_size=True, then the instances will be reordered\n",
    "    so that bigger nucler has smaller ID.\n",
    "    Args:\n",
    "        pred    : the 2d array contain instances where each instances is marked\n",
    "                  by non-zero integer\n",
    "        by_size : renaming with larger nuclei has smaller id (on-top)\n",
    "    \"\"\"\n",
    "    pred_id = list(np.unique(pred))\n",
    "    pred_id.remove(0)\n",
    "    if len(pred_id) == 0:\n",
    "        return pred  # no label\n",
    "    if by_size:\n",
    "        pred_size = []\n",
    "        for inst_id in pred_id:\n",
    "            size = (pred == inst_id).sum()\n",
    "            pred_size.append(size)\n",
    "        # sort the id by size in descending order\n",
    "        pair_list = zip(pred_id, pred_size)\n",
    "        pair_list = sorted(pair_list, key=lambda x: x[1], reverse=True)\n",
    "        pred_id, pred_size = zip(*pair_list)\n",
    "\n",
    "    new_pred = np.zeros(pred.shape, np.int32)\n",
    "    for idx, inst_id in enumerate(pred_id):\n",
    "        new_pred[pred == inst_id] = idx + 1\n",
    "    return new_pred\n",
    "\n",
    "\n",
    "#############################################################################################################\n",
    "def pair_coordinates(setA, setB, radius):\n",
    "    \"\"\"Use the Munkres or Kuhn-Munkres algorithm to find the most optimal \n",
    "    unique pairing (largest possible match) when pairing points in set B \n",
    "    against points in set A, using distance as cost function.\n",
    "    Args:\n",
    "        setA, setB: np.array (float32) of size Nx2 contains the of XY coordinate\n",
    "                    of N different points \n",
    "        radius: valid area around a point in setA to consider \n",
    "                a given coordinate in setB a candidate for match\n",
    "    Return:\n",
    "        pairing: pairing is an array of indices\n",
    "        where point at index pairing[0] in set A paired with point\n",
    "        in set B at index pairing[1]\n",
    "        unparedA, unpairedB: remaining poitn in set A and set B unpaired\n",
    "    \"\"\"\n",
    "    # * Euclidean distance as the cost matrix\n",
    "    pair_distance = scipy.spatial.distance.cdist(setA, setB, metric='euclidean')\n",
    "\n",
    "    # * Munkres pairing with scipy library\n",
    "    # the algorithm return (row indices, matched column indices)\n",
    "    # if there is multiple same cost in a row, index of first occurence \n",
    "    # is return, thus the unique pairing is ensured\n",
    "    indicesA, paired_indicesB = linear_sum_assignment(pair_distance)\n",
    "\n",
    "    # extract the paired cost and remove instances \n",
    "    # outside of designated radius\n",
    "    pair_cost = pair_distance[indicesA, paired_indicesB]\n",
    "\n",
    "    pairedA = indicesA[pair_cost <= radius]\n",
    "    pairedB = paired_indicesB[pair_cost <= radius]\n",
    "\n",
    "    pairing = np.concatenate([pairedA[:,None], pairedB[:,None]], axis=-1)\n",
    "    unpairedA = np.delete(np.arange(setA.shape[0]), pairedA)\n",
    "    unpairedB = np.delete(np.arange(setB.shape[0]), pairedB)\n",
    "    return pairing, unpairedA, unpairedB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "50e3a0a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data generator related functions\n",
    "def get_id_from_file_path(file_path, indicator):\n",
    "    return file_path.split(os.path.sep)[-1].replace(indicator, '')\n",
    "#############################################################################################################\n",
    "def chunker(seq, seq2, size):\n",
    "    return ([seq[pos:pos + size], seq2[pos:pos + size]] for pos in range(0, len(seq), size))\n",
    "#############################################################################################################\n",
    "def data_gen(list_files, list_files2, batch_size, p , size_row, size_col, distance_unet_flag = 0,\n",
    "             augment= False, BACKBONE_model = None, use_pretrain_flag = 1):\n",
    "    crop_size_row = size_row\n",
    "    crop_size_col = size_col\n",
    "    aug = albumentation_aug(p, crop_size_row, crop_size_col)\n",
    "\n",
    "    while True:\n",
    "        for batch in chunker(list_files,list_files2, batch_size):\n",
    "            X = []\n",
    "            Y = []\n",
    "\n",
    "            for count in range(len(batch[0])):\n",
    "                x = cv2.imread(batch[0][count])\n",
    "                x = cv2.cvtColor(x, cv2.COLOR_BGR2RGB)\n",
    "                x_mask = cv2.imread(batch[1][count], cv2.IMREAD_GRAYSCALE)\n",
    "                \n",
    "                x_mask_temp = np.zeros((x_mask.shape[0], x_mask.shape[1]))\n",
    "                x_mask_temp[x_mask == 255] = 1\n",
    "                \n",
    "\n",
    "                if distance_unet_flag == False:\n",
    "                    if augment:\n",
    "                        augmented = aug(image= x, mask= x_mask_temp)\n",
    "                        x = augmented['image']\n",
    "                        if use_pretrain_flag == 1:\n",
    "                            x = preprocess_input(x)\n",
    "                        x_mask_temp = augmented['mask']\n",
    "                        x = x/255\n",
    "                    else:\n",
    "                        x = x/255    \n",
    "                    X.append(x)\n",
    "                    Y.append(x_mask_temp)\n",
    "                else:\n",
    "                    if augment:\n",
    "                        augmented = aug(image=x, mask=x_mask)\n",
    "                        x = augmented['image']\n",
    "                        if use_pretrain_flag == 1:\n",
    "                            x = preprocess_input(x)\n",
    "                        x_mask = augmented['mask']\n",
    "                        x = x/255\n",
    "                    else:\n",
    "                        x = x/255  \n",
    "                        \n",
    "                    X.append(x)\n",
    "                    x_mask = (x_mask - np.min(x_mask))/ (np.max(x_mask) - np.min(x_mask) + 0.0000001)\n",
    "                    Y.append(x_mask)\n",
    "\n",
    "                del x_mask\n",
    "                del x_mask_temp\n",
    "                del x\n",
    "            Y = np.expand_dims(np.array(Y), axis=3)\n",
    "            Y = np.array(Y)\n",
    "            yield np.array(X), np.array(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084e1b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits= opts['k_fold'],random_state= opts['random_seed_num'],shuffle=True)\n",
    "kf.get_n_splits(img_path)\n",
    "\n",
    "start_time = time.time()\n",
    "dice_unet = np.zeros([opts['k_fold'],len(img_path)//opts['k_fold']])\n",
    "AJI_unet = np.zeros([opts['k_fold'],len(img_path)//opts['k_fold']])\n",
    "PQ_unet = np.zeros([opts['k_fold'],len(img_path)//opts['k_fold']])\n",
    "\n",
    "\n",
    "dice_unet_watershed = np.zeros([opts['k_fold'],len(img_path)//opts['k_fold']])\n",
    "AJI_unet_watershed = np.zeros([opts['k_fold'],len(img_path)//opts['k_fold']])\n",
    "PQ_unet_watershed = np.zeros([opts['k_fold'],len(img_path)//opts['k_fold']])\n",
    "\n",
    "dice_unet_watershed_without_vague = np.zeros([opts['k_fold'],len(img_path)//opts['k_fold']])\n",
    "AJI_unet_watershed_without_vague = np.zeros([opts['k_fold'],len(img_path)//opts['k_fold']])\n",
    "PQ_unet_watershed_without_vague = np.zeros([opts['k_fold'],len(img_path)//opts['k_fold']])\n",
    "\n",
    "dice_mean = []\n",
    "aji_mean = []\n",
    "pq_mean = []\n",
    "                   \n",
    "dice_watershed_mean = []\n",
    "aji_watershed_mean = []\n",
    "pq_watershed_mean = []\n",
    "                   \n",
    "dice_watershed_wovague_mean = []\n",
    "aji_watershed_wovague_mean = []\n",
    "pq_watershed_wovague_mean = []\n",
    "\n",
    "current_fold = 1\n",
    "\n",
    "for idx, [train_index,  test_index] in enumerate(kf.split(img_path)):\n",
    "    shuffle(train_index)\n",
    "    shuffle(test_index)\n",
    "\n",
    "    train_img   = [img_path[name] for name in train_index]\n",
    "    train_mask  = [binary_mask_path[name] for name in train_index]\n",
    "    train_dis   = [distance_mask_path[name] for name in train_index]\n",
    "    train_label = [label_mask_path[name] for name in train_index]\n",
    "    \n",
    "    test_img   = [img_path[name] for name in test_index]\n",
    "    test_mask  = [binary_mask_path[name] for name in test_index]\n",
    "    test_dis   = [distance_mask_path[name] for name in test_index]\n",
    "    test_label = [label_mask_path[name] for name in test_index]\n",
    "    test_vague = [vague_mask_path[name] for name in test_index]\n",
    "    \n",
    "    #creating validation set\n",
    "    validation_set_img = []\n",
    "    validation_set_label = []\n",
    "    #validation_DIS = []\n",
    "    validation_set_vague = []\n",
    "    for counter in range(len(test_img)):\n",
    "        val_img = cv2.imread(test_img[counter])\n",
    "        val_img = cv2.cvtColor(val_img, cv2.COLOR_BGR2RGB)\n",
    "        val_img = val_img/255\n",
    "        val_label = cv2.imread(test_label[counter], -1) # cv2.IMREAD_UNCHANGED: \n",
    "        #It specifies to load an image as such including alpha channel. \n",
    "        #Alternatively, we can pass integer value -1 for this flag.\n",
    "        val_vague = cv2.imread(test_vague[counter], -1)\n",
    "        \n",
    "        validation_set_img.append(val_img)\n",
    "        validation_set_label.append(val_label)\n",
    "        validation_set_vague.append(val_vague)\n",
    "        \n",
    "    validation_set_img = np.array(validation_set_img)\n",
    "    validation_set_label = np.array(validation_set_label)\n",
    "    validation_set_vague = np.array(validation_set_vague)\n",
    "    \n",
    "    model_path = opts['model_save_path'] + 'unet_{}.h5'.format(current_fold)\n",
    "    logger = CSVLogger(opts['model_save_path']+ 'unet_{}.log'.format(current_fold))\n",
    "    LR_drop = step_decay_schedule(initial_lr= opts['init_LR'], \n",
    "                              decay_factor = opts['LR_decay_factor'], \n",
    "                              epochs_drop = opts['LR_drop_after_nth_epoch'])\n",
    "    model_raw = shallow_unet(opts['number_of_channel'], opts['init_LR'])\n",
    "    checkpoint = ModelCheckpoint(model_path, monitor='val_dice_coef', verbose=1,\n",
    "                             save_best_only=True, mode='max', save_weights_only = True)\n",
    "    history = model_raw.fit_generator(data_gen(train_img,\n",
    "                                                 train_mask,\n",
    "                                                 opts['batch_size'],\n",
    "                                                 1,\n",
    "                                                 opts['crop_size'], opts['crop_size'],\n",
    "                                                 distance_unet_flag=0,\n",
    "                                                 augment=True,\n",
    "                                                 BACKBONE_model= '',\n",
    "                                                 use_pretrain_flag= False),\n",
    "                                      validation_data=data_gen(test_img,\n",
    "                                                               test_mask,\n",
    "                                                               opts['batch_size'],\n",
    "                                                               1,\n",
    "                                                               opts['crop_size'], opts['crop_size'],\n",
    "                                                               distance_unet_flag=0,\n",
    "                                                               augment= False,\n",
    "                                                               BACKBONE_model= '',\n",
    "                                                               use_pretrain_flag= False),\n",
    "                                      validation_steps=1,\n",
    "                                      epochs=opts['epoch_num'], verbose=1,\n",
    "                                      callbacks=[checkpoint, logger, LR_drop],\n",
    "                                      steps_per_epoch=(len(train_img) // opts['batch_size']) // opts['quick_run'])\n",
    "    \n",
    "    model_raw.load_weights(opts['model_save_path'] + 'unet_{}.h5'.format(current_fold))\n",
    "\n",
    "    ## predication on validation set\n",
    "    pred_val = model_raw.predict(validation_set_img, verbose=1, batch_size=1)\n",
    "    pred_val_t = (pred_val > opts['treshold']).astype(np.uint8)\n",
    "\n",
    "    output_watershed_tot_fold = []\n",
    "    output_watershed_tot_fold_wo_vague = []\n",
    "    validation_set_label_tot_fold_wo_vague = []\n",
    "    for val_len in tqdm.tqdm(range(len(pred_val))):\n",
    "        # with watershed post processing\n",
    "        local_maxi = peak_local_max(np.squeeze(pred_val[val_len]), indices=False,\n",
    "                                    exclude_border=False, footprint=np.ones((15, 15)))\n",
    "        marker = ndi.label(local_maxi)[0]\n",
    "        output_watershed = watershed(-np.squeeze(pred_val[val_len]), marker,mask = np.squeeze(pred_val_t[[val_len]]))\n",
    "        output_watershed[np.squeeze(pred_val_t[[val_len]])==0] = 0\n",
    "        output_watershed = remove_small_objects(output_watershed, min_size=50, connectivity=2)#remove small objects\n",
    "        \n",
    "        \n",
    "        \n",
    "        # without post processing \n",
    "        output_raw_0 = np.squeeze(pred_val_t[val_len])\n",
    "        output_raw = skimage.morphology.label(output_raw_0)\n",
    "        output_raw = remove_small_objects(output_raw, min_size=50, connectivity=2) #remove small objects\n",
    "\n",
    "        \n",
    "        output_watershed = remap_label(output_watershed)\n",
    "        validation_set_label[val_len] = remap_label(validation_set_label[val_len])\n",
    "        output_raw = remap_label(output_raw)\n",
    "        \n",
    "        test_name = get_id_from_file_path(test_img[val_len],'.png' )\n",
    "        \n",
    "        imsave(opts['result_save_path']+'validation/watershed_unet/{}.png'.format(test_name),\n",
    "               output_watershed.astype(np.uint16))\n",
    "        imsave(opts['result_save_path']+'validation/unet/{}.png'.format(test_name),output_raw.astype(np.uint16))\n",
    "\n",
    "\n",
    "        \n",
    "        dice_unet[current_fold-1, val_len]= get_dice_1(validation_set_label[val_len], output_raw)\n",
    "        AJI_unet[current_fold-1, val_len] = get_fast_aji(validation_set_label[val_len], output_raw)\n",
    "        PQ_unet[current_fold-1, val_len] = get_fast_pq(validation_set_label[val_len], output_raw)[0][2]\n",
    "        \n",
    "        \n",
    "        dice_unet_watershed[current_fold-1, val_len]= get_dice_1(validation_set_label[val_len],output_watershed)\n",
    "        AJI_unet_watershed[current_fold-1, val_len] = get_fast_aji(validation_set_label[val_len], output_watershed)\n",
    "        PQ_unet_watershed[current_fold-1, val_len]  = get_fast_pq(validation_set_label[val_len], output_watershed)[0][2]\n",
    "        ###################################################################\n",
    "        \n",
    "        #\n",
    "        output_watershed_wo_vague = np.copy(output_watershed)\n",
    "        output_watershed_wo_vague[validation_set_vague[val_len] == 255] = 0\n",
    "        output_watershed_wo_vague = remove_small_objects(output_watershed_wo_vague, min_size=50, connectivity=2) \n",
    "        \n",
    "        validation_set_label_wo_vague = np.copy(validation_set_label[val_len])\n",
    "        validation_set_label_wo_vague[validation_set_vague[val_len] == 255] = 0\n",
    "        validation_set_label_wo_vague = remove_small_objects(validation_set_label_wo_vague, min_size=50, connectivity=2)\n",
    "        \n",
    "        output_watershed_wo_vague = remap_label(output_watershed_wo_vague)\n",
    "        validation_set_label_wo_vague = remap_label(validation_set_label_wo_vague)\n",
    "        \n",
    "        \n",
    "        \n",
    "        dice_unet_watershed_without_vague[current_fold-1, val_len]= get_dice_1(\n",
    "            validation_set_label_wo_vague,output_watershed_wo_vague)\n",
    "        AJI_unet_watershed_without_vague[current_fold-1, val_len] = get_fast_aji(\n",
    "            validation_set_label_wo_vague,output_watershed_wo_vague)\n",
    "        PQ_unet_watershed_without_vague[current_fold-1, val_len]  = get_fast_pq(\n",
    "            validation_set_label_wo_vague,output_watershed_wo_vague)[0][2]\n",
    "\n",
    "        output_watershed_tot_fold.append(output_watershed)\n",
    "        output_watershed_tot_fold_wo_vague.append(output_watershed_wo_vague)\n",
    "        validation_set_label_tot_fold_wo_vague.append(validation_set_label_wo_vague)\n",
    "    \n",
    "    print('==========')    \n",
    "    print('average dice pure Unet for fold{}: {:.2f}'.format(current_fold, np.mean(dice_unet[current_fold-1, :]*100)))\n",
    "    print('average AJI pure Unet for fold{}: {:.2f}'.format(current_fold, np.mean(AJI_unet[current_fold-1, :]*100)))\n",
    "    print('average PQ pure Unet for fold{}: {:.2f}'.format(current_fold, np.mean(PQ_unet[current_fold-1, :]*100)))\n",
    "    dice_mean.append(np.mean(dice_unet[current_fold-1, :]*100))\n",
    "    aji_mean.append(np.mean(AJI_unet[current_fold-1, :]*100))\n",
    "    pq_mean.append(np.mean(PQ_unet[current_fold-1, :]*100))\n",
    "    print('==========') \n",
    "    \n",
    "    print('==========')    \n",
    "    print('average Dice Unet watershed for fold{}: {:.2f}'.format(current_fold,\n",
    "                                                                   np.mean(dice_unet_watershed[current_fold-1, :]*100)))\n",
    "    print('average AJI Unet watershed for fold{}: {:.2f}'.format(current_fold,\n",
    "                                                                  np.mean(AJI_unet_watershed[current_fold-1, :]*100)))\n",
    "    print('average PQ Unet watershed for fold{}: {:.2f}'.format(current_fold,\n",
    "                                                                 np.mean(PQ_unet_watershed[current_fold-1, :]*100)))\n",
    "    dice_watershed_mean.append(np.mean(dice_unet_watershed[current_fold-1, :]*100))\n",
    "    aji_watershed_mean.append(np.mean(AJI_unet_watershed[current_fold-1, :]*100))\n",
    "    pq_watershed_mean.append(np.mean(PQ_unet_watershed[current_fold-1, :]*100))\n",
    "    print('==========') \n",
    "       \n",
    "    print('average Dice Unet watershed wo vague for fold{}: {:.2f}'.format(current_fold,\n",
    "                                                                   np.mean(dice_unet_watershed_without_vague[current_fold-1, :]*100)))\n",
    "    print('average AJI Unet watershed wo vague for fold{}: {:.2f}'.format(current_fold,\n",
    "                                                                  np.mean(AJI_unet_watershed_without_vague[current_fold-1, :]*100)))\n",
    "    print('average PQ Unet watershed wo vague for fold{}: {:.2f}'.format(current_fold,\n",
    "                                                                 np.mean(PQ_unet_watershed_without_vague[current_fold-1, :]*100)))\n",
    "    dice_watershed_wovague_mean.append(np.mean(dice_unet_watershed_without_vague[current_fold-1, :]*100))\n",
    "    aji_watershed_wovague_mean.append(np.mean(AJI_unet_watershed_without_vague[current_fold-1, :]*100))\n",
    "    pq_watershed_wovague_mean.append(np.mean(PQ_unet_watershed_without_vague[current_fold-1, :]*100))\n",
    "    print('==========') \n",
    "    \n",
    "     \n",
    "\n",
    "   \n",
    "    \n",
    "    \n",
    "    \n",
    "    current_fold = current_fold + 1\n",
    "#     if idx ==0:\n",
    "#         break \n",
    "\n",
    "fold_names = ['fold1', 'fold2','fold3','fold4','fold5']\n",
    "df_dice = pd.DataFrame({'fold num':fold_names, 'dice unet':dice_mean,'dice unet watershed':dice_watershed_mean,\n",
    "                   'dice unet whatershed wo vague':dice_watershed_wovague_mean})\n",
    "\n",
    "df_aji = pd.DataFrame({'fold num':fold_names, 'AJI unet':aji_mean,'AJI unet watershed':aji_watershed_mean,\n",
    "                   'AJI unet whatershed wo vague':aji_watershed_wovague_mean})\n",
    "\n",
    "df_pq = pd.DataFrame({'fold num':fold_names, 'PQ unet':pq_mean,'PQ unet watershed':pq_watershed_mean,\n",
    "                   'PQ unet whatershed wo vague':pq_watershed_wovague_mean})\n",
    "\n",
    "df_dice.to_csv('/kaggle/working/dice.csv', index=False)\n",
    "df_aji.to_csv('/kaggle/working/aji.csv', index=False)\n",
    "df_pq.to_csv('/kaggle/working/pq.csv', index=False)\n",
    "\n",
    "print(df_dice.head())\n",
    "print('============================================================')\n",
    "print(df_aji.head())\n",
    "print('============================================================')\n",
    "print(df_pq.head())\n",
    "print('============================================================')\n",
    "\n",
    "\n",
    "finish_time = time.time() \n",
    "print('==========') \n",
    "print('total training time (all 5 folds): {:.2f} minutes'.format((finish_time- start_time)/60))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb7662f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
